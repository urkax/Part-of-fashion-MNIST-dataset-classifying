split train val random seed:8245
RANDOM SEED: 8245
data catagory: train
len of img: 27000
data catagory: val
len of img: 3000
data catagory: test
len of img: 5000
using device: cuda
~fc: Conv2d1.conv.weight
~fc: Conv2d1.bn.weight
~fc: Conv2d1.bn.bias
~fc: Conv2d2.conv.weight
~fc: Conv2d2.bn.weight
~fc: Conv2d2.bn.bias
~fc: Mixed_3a.branch1x1.conv.weight
~fc: Mixed_3a.branch1x1.bn.weight
~fc: Mixed_3a.branch1x1.bn.bias
~fc: Mixed_3a.branch3x3_1.conv.weight
~fc: Mixed_3a.branch3x3_1.bn.weight
~fc: Mixed_3a.branch3x3_1.bn.bias
~fc: Mixed_3a.branch3x3_2.conv.weight
~fc: Mixed_3a.branch3x3_2.bn.weight
~fc: Mixed_3a.branch3x3_2.bn.bias
~fc: Mixed_3a.branch5x5_1.conv.weight
~fc: Mixed_3a.branch5x5_1.bn.weight
~fc: Mixed_3a.branch5x5_1.bn.bias
~fc: Mixed_3a.branch5x5_2.conv.weight
~fc: Mixed_3a.branch5x5_2.bn.weight
~fc: Mixed_3a.branch5x5_2.bn.bias
~fc: Mixed_3a.branch_pool.conv.weight
~fc: Mixed_3a.branch_pool.bn.weight
~fc: Mixed_3a.branch_pool.bn.bias
~fc: Mixed_3b.branch1x1.conv.weight
~fc: Mixed_3b.branch1x1.bn.weight
~fc: Mixed_3b.branch1x1.bn.bias
~fc: Mixed_3b.branch3x3_1.conv.weight
~fc: Mixed_3b.branch3x3_1.bn.weight
~fc: Mixed_3b.branch3x3_1.bn.bias
~fc: Mixed_3b.branch3x3_2.conv.weight
~fc: Mixed_3b.branch3x3_2.bn.weight
~fc: Mixed_3b.branch3x3_2.bn.bias
~fc: Mixed_3b.branch5x5_1.conv.weight
~fc: Mixed_3b.branch5x5_1.bn.weight
~fc: Mixed_3b.branch5x5_1.bn.bias
~fc: Mixed_3b.branch5x5_2.conv.weight
~fc: Mixed_3b.branch5x5_2.bn.weight
~fc: Mixed_3b.branch5x5_2.bn.bias
~fc: Mixed_3b.branch_pool.conv.weight
~fc: Mixed_3b.branch_pool.bn.weight
~fc: Mixed_3b.branch_pool.bn.bias
~fc: Mixed_4a.branch1x1.conv.weight
~fc: Mixed_4a.branch1x1.bn.weight
~fc: Mixed_4a.branch1x1.bn.bias
~fc: Mixed_4a.branch3x3_1.conv.weight
~fc: Mixed_4a.branch3x3_1.bn.weight
~fc: Mixed_4a.branch3x3_1.bn.bias
~fc: Mixed_4a.branch3x3_2.conv.weight
~fc: Mixed_4a.branch3x3_2.bn.weight
~fc: Mixed_4a.branch3x3_2.bn.bias
~fc: Mixed_4a.branch5x5_1.conv.weight
~fc: Mixed_4a.branch5x5_1.bn.weight
~fc: Mixed_4a.branch5x5_1.bn.bias
~fc: Mixed_4a.branch5x5_2.conv.weight
~fc: Mixed_4a.branch5x5_2.bn.weight
~fc: Mixed_4a.branch5x5_2.bn.bias
~fc: Mixed_4a.branch_pool.conv.weight
~fc: Mixed_4a.branch_pool.bn.weight
~fc: Mixed_4a.branch_pool.bn.bias
~fc: Mixed_4b.branch1x1.conv.weight
~fc: Mixed_4b.branch1x1.bn.weight
~fc: Mixed_4b.branch1x1.bn.bias
~fc: Mixed_4b.branch3x3_1.conv.weight
~fc: Mixed_4b.branch3x3_1.bn.weight
~fc: Mixed_4b.branch3x3_1.bn.bias
~fc: Mixed_4b.branch3x3_2.conv.weight
~fc: Mixed_4b.branch3x3_2.bn.weight
~fc: Mixed_4b.branch3x3_2.bn.bias
~fc: Mixed_4b.branch5x5_1.conv.weight
~fc: Mixed_4b.branch5x5_1.bn.weight
~fc: Mixed_4b.branch5x5_1.bn.bias
~fc: Mixed_4b.branch5x5_2.conv.weight
~fc: Mixed_4b.branch5x5_2.bn.weight
~fc: Mixed_4b.branch5x5_2.bn.bias
~fc: Mixed_4b.branch_pool.conv.weight
~fc: Mixed_4b.branch_pool.bn.weight
~fc: Mixed_4b.branch_pool.bn.bias
~fc: Mixed_4c.branch1x1.conv.weight
~fc: Mixed_4c.branch1x1.bn.weight
~fc: Mixed_4c.branch1x1.bn.bias
~fc: Mixed_4c.branch3x3_1.conv.weight
~fc: Mixed_4c.branch3x3_1.bn.weight
~fc: Mixed_4c.branch3x3_1.bn.bias
~fc: Mixed_4c.branch3x3_2.conv.weight
~fc: Mixed_4c.branch3x3_2.bn.weight
~fc: Mixed_4c.branch3x3_2.bn.bias
~fc: Mixed_4c.branch5x5_1.conv.weight
~fc: Mixed_4c.branch5x5_1.bn.weight
~fc: Mixed_4c.branch5x5_1.bn.bias
~fc: Mixed_4c.branch5x5_2.conv.weight
~fc: Mixed_4c.branch5x5_2.bn.weight
~fc: Mixed_4c.branch5x5_2.bn.bias
~fc: Mixed_4c.branch_pool.conv.weight
~fc: Mixed_4c.branch_pool.bn.weight
~fc: Mixed_4c.branch_pool.bn.bias
~fc: Mixed_4d.branch1x1.conv.weight
~fc: Mixed_4d.branch1x1.bn.weight
~fc: Mixed_4d.branch1x1.bn.bias
~fc: Mixed_4d.branch3x3_1.conv.weight
~fc: Mixed_4d.branch3x3_1.bn.weight
~fc: Mixed_4d.branch3x3_1.bn.bias
~fc: Mixed_4d.branch3x3_2.conv.weight
~fc: Mixed_4d.branch3x3_2.bn.weight
~fc: Mixed_4d.branch3x3_2.bn.bias
~fc: Mixed_4d.branch5x5_1.conv.weight
~fc: Mixed_4d.branch5x5_1.bn.weight
~fc: Mixed_4d.branch5x5_1.bn.bias
~fc: Mixed_4d.branch5x5_2.conv.weight
~fc: Mixed_4d.branch5x5_2.bn.weight
~fc: Mixed_4d.branch5x5_2.bn.bias
~fc: Mixed_4d.branch_pool.conv.weight
~fc: Mixed_4d.branch_pool.bn.weight
~fc: Mixed_4d.branch_pool.bn.bias
~fc: Mixed_4e.branch1x1.conv.weight
~fc: Mixed_4e.branch1x1.bn.weight
~fc: Mixed_4e.branch1x1.bn.bias
~fc: Mixed_4e.branch3x3_1.conv.weight
~fc: Mixed_4e.branch3x3_1.bn.weight
~fc: Mixed_4e.branch3x3_1.bn.bias
~fc: Mixed_4e.branch3x3_2.conv.weight
~fc: Mixed_4e.branch3x3_2.bn.weight
~fc: Mixed_4e.branch3x3_2.bn.bias
~fc: Mixed_4e.branch5x5_1.conv.weight
~fc: Mixed_4e.branch5x5_1.bn.weight
~fc: Mixed_4e.branch5x5_1.bn.bias
~fc: Mixed_4e.branch5x5_2.conv.weight
~fc: Mixed_4e.branch5x5_2.bn.weight
~fc: Mixed_4e.branch5x5_2.bn.bias
~fc: Mixed_4e.branch_pool.conv.weight
~fc: Mixed_4e.branch_pool.bn.weight
~fc: Mixed_4e.branch_pool.bn.bias
~fc: Mixed_5a.branch1x1.conv.weight
~fc: Mixed_5a.branch1x1.bn.weight
~fc: Mixed_5a.branch1x1.bn.bias
~fc: Mixed_5a.branch3x3_1.conv.weight
~fc: Mixed_5a.branch3x3_1.bn.weight
~fc: Mixed_5a.branch3x3_1.bn.bias
~fc: Mixed_5a.branch3x3_2.conv.weight
~fc: Mixed_5a.branch3x3_2.bn.weight
~fc: Mixed_5a.branch3x3_2.bn.bias
~fc: Mixed_5a.branch5x5_1.conv.weight
~fc: Mixed_5a.branch5x5_1.bn.weight
~fc: Mixed_5a.branch5x5_1.bn.bias
~fc: Mixed_5a.branch5x5_2.conv.weight
~fc: Mixed_5a.branch5x5_2.bn.weight
~fc: Mixed_5a.branch5x5_2.bn.bias
~fc: Mixed_5a.branch_pool.conv.weight
~fc: Mixed_5a.branch_pool.bn.weight
~fc: Mixed_5a.branch_pool.bn.bias
~fc: Mixed_5b.branch1x1.conv.weight
~fc: Mixed_5b.branch1x1.bn.weight
~fc: Mixed_5b.branch1x1.bn.bias
~fc: Mixed_5b.branch3x3_1.conv.weight
~fc: Mixed_5b.branch3x3_1.bn.weight
~fc: Mixed_5b.branch3x3_1.bn.bias
~fc: Mixed_5b.branch3x3_2.conv.weight
~fc: Mixed_5b.branch3x3_2.bn.weight
~fc: Mixed_5b.branch3x3_2.bn.bias
~fc: Mixed_5b.branch5x5_1.conv.weight
~fc: Mixed_5b.branch5x5_1.bn.weight
~fc: Mixed_5b.branch5x5_1.bn.bias
~fc: Mixed_5b.branch5x5_2.conv.weight
~fc: Mixed_5b.branch5x5_2.bn.weight
~fc: Mixed_5b.branch5x5_2.bn.bias
~fc: Mixed_5b.branch_pool.conv.weight
~fc: Mixed_5b.branch_pool.bn.weight
~fc: Mixed_5b.branch_pool.bn.bias
~fc: fc2.weight
~fc: fc2.bias
fc learning rate: 0.001
not fc learning rate: 0.001
googleNet(
  (Conv2d1): BasicConv2d(
    (conv): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
    (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (Conv2d2): BasicConv2d(
    (conv): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (Mixed_3a): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(4, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_3b): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(8, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_4a): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(120, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(24, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(52, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(120, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(4, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(4, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(12, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(120, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_4b): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(128, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(128, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(28, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(28, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(56, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(128, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(6, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_4c): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(128, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(12, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(12, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_4d): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(128, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(28, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(128, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(36, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(36, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(72, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_4e): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(132, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(132, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(40, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(132, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(8, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(132, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_5a): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(208, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(208, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(40, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(208, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(8, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(208, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_5b): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(208, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(208, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(208, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(12, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(12, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(208, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (dropout_layer): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
)
epoch: 0
Train: [0/422]	Time 5.126 (5.126)	Loss 2.4017 (2.4017)	Prec@1 7.812 (7.812)	Prec@5 42.188 (42.188)
Train: [50/422]	Time 0.064 (0.170)	Loss 0.9853 (1.4610)	Prec@1 70.312 (48.131)	Prec@5 98.438 (90.901)
Train: [100/422]	Time 0.069 (0.068)	Loss 0.8906 (1.2066)	Prec@1 68.750 (56.672)	Prec@5 100.000 (95.125)
Train: [150/422]	Time 0.066 (0.069)	Loss 0.5097 (0.9177)	Prec@1 84.375 (66.797)	Prec@5 98.438 (98.438)
Train: [200/422]	Time 0.081 (0.071)	Loss 0.6919 (0.8239)	Prec@1 78.125 (70.188)	Prec@5 95.312 (98.641)
Train: [250/422]	Time 0.069 (0.073)	Loss 0.9372 (0.7792)	Prec@1 65.625 (71.266)	Prec@5 100.000 (99.047)
Train: [300/422]	Time 0.075 (0.072)	Loss 0.5734 (0.7550)	Prec@1 78.125 (71.766)	Prec@5 100.000 (99.219)
Train: [350/422]	Time 0.069 (0.070)	Loss 0.7298 (0.7227)	Prec@1 68.750 (72.594)	Prec@5 98.438 (99.172)
Train: [400/422]	Time 0.070 (0.071)	Loss 0.6565 (0.7017)	Prec@1 73.438 (73.703)	Prec@5 100.000 (99.203)
Test: [0/47]	Time 0.376 (0.376)	Loss 0.5805 (0.5805)	Prec@1 78.125 (78.125)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.012 (0.089)	Loss 0.7040 (0.6195)	Prec@1 70.312 (76.488)	Prec@5 100.000 (99.628)
Test: [40/47]	Time 0.015 (0.055)	Loss 0.5474 (0.6416)	Prec@1 76.562 (76.143)	Prec@5 100.000 (99.428)
Test: [0/40] Acc 76.067
epoch: 1
Train: [0/422]	Time 0.384 (0.384)	Loss 0.6170 (0.6170)	Prec@1 76.562 (76.562)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.071 (0.079)	Loss 0.5954 (0.6598)	Prec@1 81.250 (73.989)	Prec@5 100.000 (99.357)
Train: [100/422]	Time 0.069 (0.072)	Loss 0.6457 (0.6563)	Prec@1 73.438 (74.641)	Prec@5 100.000 (99.391)
Train: [150/422]	Time 0.068 (0.070)	Loss 0.6179 (0.6501)	Prec@1 79.688 (75.375)	Prec@5 100.000 (99.359)
Train: [200/422]	Time 0.079 (0.072)	Loss 0.6011 (0.6479)	Prec@1 71.875 (75.734)	Prec@5 100.000 (99.203)
Train: [250/422]	Time 0.072 (0.074)	Loss 0.5213 (0.6427)	Prec@1 84.375 (76.391)	Prec@5 98.438 (99.234)
Train: [300/422]	Time 0.067 (0.072)	Loss 0.7093 (0.6165)	Prec@1 71.875 (77.281)	Prec@5 100.000 (99.469)
Train: [350/422]	Time 0.066 (0.070)	Loss 0.5503 (0.5847)	Prec@1 81.250 (78.203)	Prec@5 98.438 (99.594)
Train: [400/422]	Time 0.081 (0.072)	Loss 0.4249 (0.5753)	Prec@1 84.375 (78.344)	Prec@5 100.000 (99.625)
Test: [0/47]	Time 0.344 (0.344)	Loss 0.6197 (0.6197)	Prec@1 76.562 (76.562)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.029)	Loss 0.7101 (0.6221)	Prec@1 70.312 (76.265)	Prec@5 100.000 (99.702)
Test: [40/47]	Time 0.018 (0.022)	Loss 0.5061 (0.6142)	Prec@1 79.688 (76.753)	Prec@5 100.000 (99.657)
Test: [1/40] Acc 76.967
epoch: 2
Train: [0/422]	Time 0.436 (0.436)	Loss 0.6677 (0.6677)	Prec@1 79.688 (79.688)	Prec@5 96.875 (96.875)
Train: [50/422]	Time 0.059 (0.075)	Loss 0.4575 (0.5405)	Prec@1 79.688 (79.596)	Prec@5 100.000 (99.449)
Train: [100/422]	Time 0.087 (0.067)	Loss 0.8118 (0.5457)	Prec@1 75.000 (79.672)	Prec@5 100.000 (99.578)
Train: [150/422]	Time 0.087 (0.076)	Loss 0.7698 (0.5631)	Prec@1 67.188 (79.047)	Prec@5 96.875 (99.547)
Train: [200/422]	Time 0.085 (0.080)	Loss 0.4670 (0.5559)	Prec@1 82.812 (78.969)	Prec@5 100.000 (99.578)
Train: [250/422]	Time 0.070 (0.073)	Loss 0.6138 (0.5365)	Prec@1 76.562 (79.531)	Prec@5 98.438 (99.750)
Train: [300/422]	Time 0.081 (0.071)	Loss 0.5995 (0.5356)	Prec@1 81.250 (79.969)	Prec@5 98.438 (99.688)
Train: [350/422]	Time 0.068 (0.072)	Loss 0.6356 (0.5340)	Prec@1 84.375 (80.688)	Prec@5 100.000 (99.578)
Train: [400/422]	Time 0.090 (0.072)	Loss 0.3922 (0.5267)	Prec@1 87.500 (80.844)	Prec@5 98.438 (99.578)
Test: [0/47]	Time 0.342 (0.342)	Loss 0.4665 (0.4665)	Prec@1 78.125 (78.125)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.031)	Loss 0.5243 (0.4717)	Prec@1 79.688 (81.845)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.015 (0.023)	Loss 0.4151 (0.4595)	Prec@1 84.375 (82.889)	Prec@5 100.000 (99.886)
Test: [2/40] Acc 82.900
epoch: 3
Train: [0/422]	Time 0.431 (0.431)	Loss 0.4186 (0.4186)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.083 (0.077)	Loss 0.5340 (0.5016)	Prec@1 84.375 (81.587)	Prec@5 100.000 (99.571)
Train: [100/422]	Time 0.066 (0.071)	Loss 0.5214 (0.5151)	Prec@1 78.125 (81.203)	Prec@5 100.000 (99.594)
Train: [150/422]	Time 0.084 (0.071)	Loss 0.4957 (0.4996)	Prec@1 81.250 (81.828)	Prec@5 100.000 (99.625)
Train: [200/422]	Time 0.067 (0.070)	Loss 0.4547 (0.4728)	Prec@1 82.812 (82.609)	Prec@5 100.000 (99.641)
Train: [250/422]	Time 0.072 (0.069)	Loss 0.7206 (0.4907)	Prec@1 78.125 (82.000)	Prec@5 98.438 (99.609)
Train: [300/422]	Time 0.071 (0.069)	Loss 0.6537 (0.4986)	Prec@1 84.375 (81.906)	Prec@5 96.875 (99.547)
Train: [350/422]	Time 0.082 (0.071)	Loss 0.4228 (0.4839)	Prec@1 82.812 (82.141)	Prec@5 100.000 (99.500)
Train: [400/422]	Time 0.071 (0.074)	Loss 0.5729 (0.4949)	Prec@1 82.812 (81.625)	Prec@5 100.000 (99.500)
Test: [0/47]	Time 0.356 (0.356)	Loss 0.5024 (0.5024)	Prec@1 76.562 (76.562)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.032)	Loss 0.5025 (0.4615)	Prec@1 82.812 (81.696)	Prec@5 100.000 (99.851)
Test: [40/47]	Time 0.013 (0.023)	Loss 0.4209 (0.4422)	Prec@1 84.375 (83.308)	Prec@5 100.000 (99.809)
Test: [3/40] Acc 83.267
epoch: 4
Train: [0/422]	Time 0.392 (0.392)	Loss 0.5150 (0.5150)	Prec@1 76.562 (76.562)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.064 (0.081)	Loss 0.4524 (0.4612)	Prec@1 87.500 (83.241)	Prec@5 100.000 (99.632)
Train: [100/422]	Time 0.074 (0.073)	Loss 0.5695 (0.4562)	Prec@1 82.812 (83.625)	Prec@5 100.000 (99.625)
Train: [150/422]	Time 0.081 (0.076)	Loss 0.4558 (0.4639)	Prec@1 84.375 (83.250)	Prec@5 100.000 (99.656)
Train: [200/422]	Time 0.080 (0.079)	Loss 0.3981 (0.4717)	Prec@1 85.938 (82.703)	Prec@5 100.000 (99.734)
Train: [250/422]	Time 0.069 (0.076)	Loss 0.3127 (0.4640)	Prec@1 85.938 (82.859)	Prec@5 100.000 (99.594)
Train: [300/422]	Time 0.069 (0.072)	Loss 0.4280 (0.4594)	Prec@1 82.812 (83.219)	Prec@5 100.000 (99.531)
Train: [350/422]	Time 0.075 (0.071)	Loss 0.3867 (0.4645)	Prec@1 85.938 (83.062)	Prec@5 100.000 (99.672)
Train: [400/422]	Time 0.083 (0.077)	Loss 0.3789 (0.4397)	Prec@1 92.188 (83.922)	Prec@5 100.000 (99.734)
Test: [0/47]	Time 0.354 (0.354)	Loss 0.3839 (0.3839)	Prec@1 84.375 (84.375)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.016 (0.032)	Loss 0.4664 (0.3743)	Prec@1 82.812 (85.938)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.024)	Loss 0.4721 (0.3728)	Prec@1 82.812 (86.242)	Prec@5 100.000 (99.848)
Test: [4/40] Acc 85.933
epoch: 5
Train: [0/422]	Time 0.401 (0.401)	Loss 0.4063 (0.4063)	Prec@1 82.812 (82.812)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.053 (0.062)	Loss 0.3617 (0.4209)	Prec@1 87.500 (84.314)	Prec@5 100.000 (99.694)
Train: [100/422]	Time 0.050 (0.053)	Loss 0.3276 (0.4479)	Prec@1 90.625 (83.406)	Prec@5 100.000 (99.578)
Train: [150/422]	Time 0.066 (0.053)	Loss 0.3760 (0.4552)	Prec@1 82.812 (83.156)	Prec@5 100.000 (99.594)
Train: [200/422]	Time 0.075 (0.060)	Loss 0.3552 (0.4349)	Prec@1 82.812 (83.969)	Prec@5 100.000 (99.719)
Train: [250/422]	Time 0.067 (0.067)	Loss 0.4440 (0.4372)	Prec@1 84.375 (83.875)	Prec@5 100.000 (99.781)
Train: [300/422]	Time 0.068 (0.069)	Loss 0.6359 (0.4295)	Prec@1 76.562 (84.266)	Prec@5 96.875 (99.781)
Train: [350/422]	Time 0.071 (0.070)	Loss 0.3227 (0.4242)	Prec@1 92.188 (84.688)	Prec@5 100.000 (99.734)
Train: [400/422]	Time 0.069 (0.072)	Loss 0.4583 (0.4278)	Prec@1 85.938 (84.609)	Prec@5 100.000 (99.750)
Test: [0/47]	Time 0.366 (0.366)	Loss 0.4715 (0.4715)	Prec@1 84.375 (84.375)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.032)	Loss 0.4547 (0.3901)	Prec@1 78.125 (85.640)	Prec@5 100.000 (99.926)
Test: [40/47]	Time 0.013 (0.023)	Loss 0.3988 (0.3795)	Prec@1 82.812 (85.290)	Prec@5 100.000 (99.924)
Test: [5/40] Acc 85.000
epoch: 6
Train: [0/422]	Time 0.446 (0.446)	Loss 0.6155 (0.6155)	Prec@1 75.000 (75.000)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.069 (0.084)	Loss 0.4989 (0.4323)	Prec@1 78.125 (83.946)	Prec@5 100.000 (99.755)
Train: [100/422]	Time 0.070 (0.077)	Loss 0.4848 (0.4181)	Prec@1 81.250 (84.719)	Prec@5 98.438 (99.734)
Train: [150/422]	Time 0.070 (0.074)	Loss 0.4427 (0.4189)	Prec@1 87.500 (85.109)	Prec@5 98.438 (99.766)
Train: [200/422]	Time 0.071 (0.070)	Loss 0.4032 (0.4263)	Prec@1 85.938 (84.922)	Prec@5 100.000 (99.812)
Train: [250/422]	Time 0.070 (0.071)	Loss 0.4591 (0.4057)	Prec@1 85.938 (85.219)	Prec@5 100.000 (99.781)
Train: [300/422]	Time 0.070 (0.071)	Loss 0.3786 (0.4089)	Prec@1 89.062 (84.688)	Prec@5 98.438 (99.797)
Train: [350/422]	Time 0.080 (0.070)	Loss 0.2401 (0.4216)	Prec@1 90.625 (84.438)	Prec@5 100.000 (99.750)
Train: [400/422]	Time 0.059 (0.069)	Loss 0.5260 (0.4231)	Prec@1 79.688 (84.734)	Prec@5 100.000 (99.672)
Test: [0/47]	Time 0.388 (0.388)	Loss 0.3615 (0.3615)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.033)	Loss 0.3901 (0.3362)	Prec@1 87.500 (88.021)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.023)	Loss 0.3005 (0.3281)	Prec@1 90.625 (88.300)	Prec@5 100.000 (99.924)
Test: [6/40] Acc 88.133
epoch: 7
Train: [0/422]	Time 0.469 (0.469)	Loss 0.4473 (0.4473)	Prec@1 82.812 (82.812)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.091 (0.089)	Loss 0.5909 (0.4063)	Prec@1 79.688 (85.417)	Prec@5 100.000 (99.694)
Train: [100/422]	Time 0.062 (0.078)	Loss 0.4924 (0.4080)	Prec@1 78.125 (85.328)	Prec@5 100.000 (99.656)
Train: [150/422]	Time 0.060 (0.070)	Loss 0.3508 (0.3929)	Prec@1 90.625 (85.812)	Prec@5 100.000 (99.703)
Train: [200/422]	Time 0.060 (0.064)	Loss 0.2736 (0.3926)	Prec@1 92.188 (85.734)	Prec@5 100.000 (99.812)
Train: [250/422]	Time 0.059 (0.062)	Loss 0.3553 (0.3929)	Prec@1 85.938 (85.594)	Prec@5 100.000 (99.781)
Train: [300/422]	Time 0.061 (0.061)	Loss 0.1663 (0.3877)	Prec@1 96.875 (85.875)	Prec@5 100.000 (99.719)
Train: [350/422]	Time 0.071 (0.063)	Loss 0.3349 (0.3963)	Prec@1 85.938 (85.609)	Prec@5 100.000 (99.781)
Train: [400/422]	Time 0.074 (0.068)	Loss 0.2665 (0.3852)	Prec@1 90.625 (85.812)	Prec@5 100.000 (99.781)
Test: [0/47]	Time 0.367 (0.367)	Loss 0.4036 (0.4036)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.017 (0.034)	Loss 0.3813 (0.3755)	Prec@1 85.938 (86.607)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.014 (0.024)	Loss 0.3858 (0.3707)	Prec@1 79.688 (86.662)	Prec@5 100.000 (99.848)
Test: [7/40] Acc 86.233
epoch: 8
Train: [0/422]	Time 0.432 (0.432)	Loss 0.3754 (0.3754)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.061 (0.080)	Loss 0.3373 (0.3875)	Prec@1 87.500 (85.938)	Prec@5 100.000 (99.694)
Train: [100/422]	Time 0.081 (0.072)	Loss 0.4131 (0.3881)	Prec@1 82.812 (85.828)	Prec@5 100.000 (99.750)
Train: [150/422]	Time 0.070 (0.077)	Loss 0.3475 (0.3871)	Prec@1 85.938 (85.344)	Prec@5 100.000 (99.766)
Train: [200/422]	Time 0.068 (0.078)	Loss 0.5413 (0.3955)	Prec@1 79.688 (85.016)	Prec@5 100.000 (99.734)
Train: [250/422]	Time 0.071 (0.073)	Loss 0.3022 (0.3988)	Prec@1 87.500 (85.391)	Prec@5 100.000 (99.766)
Train: [300/422]	Time 0.070 (0.073)	Loss 0.4594 (0.3942)	Prec@1 81.250 (85.672)	Prec@5 100.000 (99.750)
Train: [350/422]	Time 0.073 (0.070)	Loss 0.2810 (0.3819)	Prec@1 90.625 (86.312)	Prec@5 100.000 (99.734)
Train: [400/422]	Time 0.073 (0.069)	Loss 0.3290 (0.3728)	Prec@1 85.938 (86.328)	Prec@5 100.000 (99.797)
Test: [0/47]	Time 0.352 (0.352)	Loss 0.3793 (0.3793)	Prec@1 82.812 (82.812)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.016 (0.032)	Loss 0.2946 (0.3452)	Prec@1 85.938 (86.161)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.024)	Loss 0.3090 (0.3395)	Prec@1 92.188 (86.966)	Prec@5 100.000 (99.924)
Test: [8/40] Acc 86.733
epoch: 9
Train: [0/422]	Time 0.406 (0.406)	Loss 0.5993 (0.5993)	Prec@1 76.562 (76.562)	Prec@5 98.438 (98.438)
Train: [50/422]	Time 0.072 (0.080)	Loss 0.3278 (0.3724)	Prec@1 84.375 (85.754)	Prec@5 100.000 (99.786)
Train: [100/422]	Time 0.073 (0.074)	Loss 0.2911 (0.3695)	Prec@1 92.188 (86.359)	Prec@5 100.000 (99.750)
Train: [150/422]	Time 0.067 (0.074)	Loss 0.3869 (0.3738)	Prec@1 82.812 (86.359)	Prec@5 100.000 (99.719)
Train: [200/422]	Time 0.068 (0.070)	Loss 0.3765 (0.3776)	Prec@1 92.188 (86.000)	Prec@5 100.000 (99.766)
Train: [250/422]	Time 0.067 (0.069)	Loss 0.1985 (0.3770)	Prec@1 92.188 (86.000)	Prec@5 100.000 (99.781)
Train: [300/422]	Time 0.075 (0.071)	Loss 0.3027 (0.3619)	Prec@1 85.938 (86.609)	Prec@5 100.000 (99.828)
Train: [350/422]	Time 0.073 (0.071)	Loss 0.4636 (0.3738)	Prec@1 82.812 (86.500)	Prec@5 100.000 (99.734)
Train: [400/422]	Time 0.069 (0.071)	Loss 0.2761 (0.3747)	Prec@1 87.500 (86.500)	Prec@5 100.000 (99.719)
Test: [0/47]	Time 0.357 (0.357)	Loss 0.4008 (0.4008)	Prec@1 81.250 (81.250)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.016 (0.035)	Loss 0.3113 (0.3137)	Prec@1 89.062 (88.318)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.016 (0.026)	Loss 0.3054 (0.3171)	Prec@1 87.500 (88.377)	Prec@5 100.000 (99.886)
Test: [9/40] Acc 87.767
epoch: 10
Train: [0/422]	Time 0.548 (0.548)	Loss 0.4001 (0.4001)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.069 (0.078)	Loss 0.3676 (0.3617)	Prec@1 84.375 (86.734)	Prec@5 100.000 (99.786)
Train: [100/422]	Time 0.072 (0.072)	Loss 0.4549 (0.3652)	Prec@1 82.812 (86.922)	Prec@5 98.438 (99.812)
Train: [150/422]	Time 0.067 (0.073)	Loss 0.3440 (0.3730)	Prec@1 89.062 (86.734)	Prec@5 100.000 (99.828)
Train: [200/422]	Time 0.073 (0.071)	Loss 0.4121 (0.3722)	Prec@1 85.938 (86.312)	Prec@5 100.000 (99.812)
Train: [250/422]	Time 0.073 (0.070)	Loss 0.2696 (0.3607)	Prec@1 87.500 (86.766)	Prec@5 100.000 (99.781)
Train: [300/422]	Time 0.069 (0.070)	Loss 0.2467 (0.3585)	Prec@1 89.062 (87.094)	Prec@5 100.000 (99.734)
Train: [350/422]	Time 0.072 (0.071)	Loss 0.2453 (0.3616)	Prec@1 90.625 (87.094)	Prec@5 100.000 (99.766)
Train: [400/422]	Time 0.072 (0.071)	Loss 0.3561 (0.3555)	Prec@1 87.500 (87.250)	Prec@5 100.000 (99.766)
Test: [0/47]	Time 0.337 (0.337)	Loss 0.3206 (0.3206)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.018 (0.033)	Loss 0.3037 (0.3317)	Prec@1 89.062 (87.723)	Prec@5 100.000 (99.926)
Test: [40/47]	Time 0.015 (0.025)	Loss 0.4176 (0.3256)	Prec@1 79.688 (87.957)	Prec@5 100.000 (99.848)
Test: [10/40] Acc 87.833
epoch: 11
Train: [0/422]	Time 0.421 (0.421)	Loss 0.2345 (0.2345)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.077 (0.081)	Loss 0.2339 (0.3193)	Prec@1 90.625 (88.297)	Prec@5 100.000 (99.847)
Train: [100/422]	Time 0.067 (0.072)	Loss 0.3274 (0.3328)	Prec@1 87.500 (87.578)	Prec@5 100.000 (99.844)
Train: [150/422]	Time 0.067 (0.070)	Loss 0.4322 (0.3426)	Prec@1 81.250 (87.250)	Prec@5 100.000 (99.844)
Train: [200/422]	Time 0.065 (0.071)	Loss 0.2915 (0.3617)	Prec@1 90.625 (86.594)	Prec@5 100.000 (99.812)
Train: [250/422]	Time 0.063 (0.067)	Loss 0.3907 (0.3659)	Prec@1 82.812 (86.219)	Prec@5 100.000 (99.812)
Train: [300/422]	Time 0.073 (0.066)	Loss 0.3216 (0.3468)	Prec@1 90.625 (86.953)	Prec@5 100.000 (99.797)
Train: [350/422]	Time 0.063 (0.069)	Loss 0.4619 (0.3503)	Prec@1 84.375 (87.078)	Prec@5 100.000 (99.812)
Train: [400/422]	Time 0.073 (0.069)	Loss 0.2178 (0.3560)	Prec@1 93.750 (86.750)	Prec@5 100.000 (99.828)
Test: [0/47]	Time 0.370 (0.370)	Loss 0.2958 (0.2958)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.016 (0.031)	Loss 0.3008 (0.3074)	Prec@1 89.062 (88.542)	Prec@5 100.000 (99.851)
Test: [40/47]	Time 0.012 (0.023)	Loss 0.2807 (0.3033)	Prec@1 87.500 (88.796)	Prec@5 100.000 (99.771)
Test: [11/40] Acc 88.367
epoch: 12
Train: [0/422]	Time 0.395 (0.395)	Loss 0.4006 (0.4006)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.061 (0.078)	Loss 0.3961 (0.3556)	Prec@1 79.688 (87.500)	Prec@5 100.000 (99.847)
Train: [100/422]	Time 0.069 (0.068)	Loss 0.2708 (0.3504)	Prec@1 90.625 (87.344)	Prec@5 100.000 (99.844)
Train: [150/422]	Time 0.071 (0.068)	Loss 0.2429 (0.3323)	Prec@1 89.062 (88.047)	Prec@5 100.000 (99.781)
Train: [200/422]	Time 0.074 (0.072)	Loss 0.4302 (0.3274)	Prec@1 82.812 (88.281)	Prec@5 100.000 (99.828)
Train: [250/422]	Time 0.068 (0.071)	Loss 0.2771 (0.3343)	Prec@1 87.500 (87.438)	Prec@5 100.000 (99.875)
Train: [300/422]	Time 0.084 (0.071)	Loss 0.4111 (0.3438)	Prec@1 87.500 (87.094)	Prec@5 98.438 (99.844)
Train: [350/422]	Time 0.081 (0.072)	Loss 0.2455 (0.3481)	Prec@1 89.062 (87.094)	Prec@5 100.000 (99.859)
Train: [400/422]	Time 0.064 (0.072)	Loss 0.3354 (0.3333)	Prec@1 89.062 (87.469)	Prec@5 98.438 (99.828)
Test: [0/47]	Time 0.363 (0.363)	Loss 0.3423 (0.3423)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.032)	Loss 0.2931 (0.3118)	Prec@1 89.062 (88.244)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.014 (0.024)	Loss 0.2500 (0.3015)	Prec@1 92.188 (88.796)	Prec@5 100.000 (99.924)
Test: [12/40] Acc 88.567
epoch: 13
Train: [0/422]	Time 0.413 (0.413)	Loss 0.2997 (0.2997)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.065 (0.073)	Loss 0.2881 (0.3372)	Prec@1 89.062 (87.132)	Prec@5 100.000 (99.877)
Train: [100/422]	Time 0.072 (0.068)	Loss 0.2276 (0.3399)	Prec@1 96.875 (87.500)	Prec@5 100.000 (99.828)
Train: [150/422]	Time 0.067 (0.070)	Loss 0.3550 (0.3300)	Prec@1 89.062 (88.359)	Prec@5 100.000 (99.766)
Train: [200/422]	Time 0.070 (0.070)	Loss 0.2357 (0.3289)	Prec@1 92.188 (88.062)	Prec@5 100.000 (99.766)
Train: [250/422]	Time 0.070 (0.070)	Loss 0.3095 (0.3279)	Prec@1 87.500 (87.656)	Prec@5 100.000 (99.797)
Train: [300/422]	Time 0.082 (0.075)	Loss 0.4890 (0.3210)	Prec@1 84.375 (87.656)	Prec@5 98.438 (99.828)
Train: [350/422]	Time 0.058 (0.076)	Loss 0.2010 (0.3348)	Prec@1 90.625 (87.156)	Prec@5 100.000 (99.766)
Train: [400/422]	Time 0.063 (0.066)	Loss 0.3977 (0.3334)	Prec@1 84.375 (87.578)	Prec@5 100.000 (99.797)
Test: [0/47]	Time 0.367 (0.367)	Loss 0.2805 (0.2805)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.031)	Loss 0.2788 (0.2764)	Prec@1 89.062 (89.732)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.016 (0.024)	Loss 0.3225 (0.2735)	Prec@1 84.375 (89.939)	Prec@5 100.000 (99.924)
Test: [13/40] Acc 89.833
epoch: 14
Train: [0/422]	Time 0.406 (0.406)	Loss 0.4043 (0.4043)	Prec@1 81.250 (81.250)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.069 (0.078)	Loss 0.4075 (0.3312)	Prec@1 85.938 (87.960)	Prec@5 100.000 (99.908)
Train: [100/422]	Time 0.081 (0.072)	Loss 0.2036 (0.3247)	Prec@1 92.188 (88.172)	Prec@5 100.000 (99.875)
Train: [150/422]	Time 0.077 (0.078)	Loss 0.3699 (0.3228)	Prec@1 82.812 (88.031)	Prec@5 100.000 (99.906)
Train: [200/422]	Time 0.070 (0.076)	Loss 0.3278 (0.3237)	Prec@1 90.625 (88.172)	Prec@5 98.438 (99.922)
Train: [250/422]	Time 0.070 (0.070)	Loss 0.2811 (0.3125)	Prec@1 89.062 (88.609)	Prec@5 100.000 (99.875)
Train: [300/422]	Time 0.076 (0.070)	Loss 0.4150 (0.3281)	Prec@1 84.375 (87.922)	Prec@5 100.000 (99.875)
Train: [350/422]	Time 0.066 (0.071)	Loss 0.4622 (0.3433)	Prec@1 87.500 (87.562)	Prec@5 98.438 (99.859)
Train: [400/422]	Time 0.079 (0.071)	Loss 0.5481 (0.3324)	Prec@1 82.812 (88.047)	Prec@5 98.438 (99.859)
Test: [0/47]	Time 0.350 (0.350)	Loss 0.2909 (0.2909)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.033)	Loss 0.3309 (0.2867)	Prec@1 85.938 (89.583)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.014 (0.024)	Loss 0.2113 (0.2826)	Prec@1 90.625 (89.748)	Prec@5 100.000 (99.924)
Test: [14/40] Acc 89.633
epoch: 15
Train: [0/422]	Time 0.393 (0.393)	Loss 0.3142 (0.3142)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.076 (0.077)	Loss 0.2782 (0.3208)	Prec@1 90.625 (88.480)	Prec@5 100.000 (99.755)
Train: [100/422]	Time 0.080 (0.073)	Loss 0.3239 (0.3185)	Prec@1 90.625 (88.500)	Prec@5 100.000 (99.797)
Train: [150/422]	Time 0.073 (0.075)	Loss 0.3076 (0.3167)	Prec@1 93.750 (88.531)	Prec@5 100.000 (99.891)
Train: [200/422]	Time 0.077 (0.075)	Loss 0.3686 (0.3227)	Prec@1 82.812 (88.328)	Prec@5 100.000 (99.938)
Train: [250/422]	Time 0.079 (0.075)	Loss 0.2630 (0.3101)	Prec@1 90.625 (88.641)	Prec@5 100.000 (99.906)
Train: [300/422]	Time 0.068 (0.073)	Loss 0.4245 (0.2911)	Prec@1 82.812 (89.141)	Prec@5 100.000 (99.859)
Train: [350/422]	Time 0.066 (0.070)	Loss 0.1695 (0.3148)	Prec@1 95.312 (88.578)	Prec@5 100.000 (99.828)
Train: [400/422]	Time 0.065 (0.068)	Loss 0.4607 (0.3362)	Prec@1 87.500 (87.891)	Prec@5 100.000 (99.844)
Test: [0/47]	Time 0.357 (0.357)	Loss 0.3600 (0.3600)	Prec@1 82.812 (82.812)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.031)	Loss 0.2365 (0.3430)	Prec@1 92.188 (87.202)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.014 (0.024)	Loss 0.3667 (0.3337)	Prec@1 85.938 (87.424)	Prec@5 100.000 (99.886)
Test: [15/40] Acc 87.300
epoch: 16
Train: [0/422]	Time 0.438 (0.438)	Loss 0.3977 (0.3977)	Prec@1 85.938 (85.938)	Prec@5 98.438 (98.438)
Train: [50/422]	Time 0.068 (0.076)	Loss 0.2537 (0.2999)	Prec@1 92.188 (89.400)	Prec@5 100.000 (99.969)
Train: [100/422]	Time 0.064 (0.067)	Loss 0.3673 (0.3091)	Prec@1 87.500 (88.922)	Prec@5 100.000 (99.969)
Train: [150/422]	Time 0.068 (0.066)	Loss 0.4787 (0.3251)	Prec@1 84.375 (88.125)	Prec@5 100.000 (99.891)
Train: [200/422]	Time 0.071 (0.068)	Loss 0.3202 (0.3203)	Prec@1 85.938 (88.219)	Prec@5 100.000 (99.891)
Train: [250/422]	Time 0.076 (0.071)	Loss 0.2143 (0.3018)	Prec@1 89.062 (88.891)	Prec@5 100.000 (99.969)
Train: [300/422]	Time 0.071 (0.073)	Loss 0.3492 (0.3110)	Prec@1 84.375 (88.359)	Prec@5 100.000 (99.938)
Train: [350/422]	Time 0.074 (0.070)	Loss 0.3534 (0.3270)	Prec@1 84.375 (87.734)	Prec@5 100.000 (99.859)
Train: [400/422]	Time 0.072 (0.070)	Loss 0.2361 (0.3158)	Prec@1 90.625 (88.188)	Prec@5 100.000 (99.844)
Test: [0/47]	Time 0.360 (0.360)	Loss 0.2848 (0.2848)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.031)	Loss 0.4016 (0.3104)	Prec@1 85.938 (88.318)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.023)	Loss 0.2691 (0.2904)	Prec@1 87.500 (88.796)	Prec@5 100.000 (100.000)
Test: [16/40] Acc 88.867
epoch: 17
Train: [0/422]	Time 0.419 (0.419)	Loss 0.3578 (0.3578)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.066 (0.074)	Loss 0.1994 (0.3279)	Prec@1 93.750 (87.806)	Prec@5 100.000 (99.847)
Train: [100/422]	Time 0.072 (0.068)	Loss 0.3221 (0.3164)	Prec@1 87.500 (88.078)	Prec@5 100.000 (99.859)
Train: [150/422]	Time 0.071 (0.069)	Loss 0.2627 (0.3051)	Prec@1 89.062 (88.500)	Prec@5 100.000 (99.844)
Train: [200/422]	Time 0.066 (0.070)	Loss 0.1986 (0.2976)	Prec@1 92.188 (89.094)	Prec@5 100.000 (99.875)
Train: [250/422]	Time 0.069 (0.069)	Loss 0.2305 (0.2893)	Prec@1 90.625 (89.641)	Prec@5 100.000 (99.938)
Train: [300/422]	Time 0.063 (0.067)	Loss 0.2317 (0.2962)	Prec@1 95.312 (89.172)	Prec@5 100.000 (99.922)
Train: [350/422]	Time 0.068 (0.067)	Loss 0.2152 (0.3097)	Prec@1 93.750 (88.750)	Prec@5 100.000 (99.891)
Train: [400/422]	Time 0.067 (0.069)	Loss 0.4127 (0.3100)	Prec@1 84.375 (88.703)	Prec@5 100.000 (99.906)
Test: [0/47]	Time 0.378 (0.378)	Loss 0.2329 (0.2329)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.034)	Loss 0.2553 (0.2618)	Prec@1 90.625 (90.327)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.013 (0.024)	Loss 0.2017 (0.2628)	Prec@1 93.750 (90.434)	Prec@5 100.000 (99.886)
Test: [17/40] Acc 90.333
epoch: 18
Train: [0/422]	Time 0.410 (0.410)	Loss 0.1854 (0.1854)	Prec@1 95.312 (95.312)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.091 (0.081)	Loss 0.2029 (0.2964)	Prec@1 93.750 (88.787)	Prec@5 100.000 (99.908)
Train: [100/422]	Time 0.084 (0.079)	Loss 0.2244 (0.2989)	Prec@1 92.188 (88.969)	Prec@5 100.000 (99.859)
Train: [150/422]	Time 0.079 (0.084)	Loss 0.3603 (0.2940)	Prec@1 85.938 (89.141)	Prec@5 100.000 (99.891)
Train: [200/422]	Time 0.071 (0.078)	Loss 0.3025 (0.2876)	Prec@1 85.938 (89.078)	Prec@5 100.000 (99.875)
Train: [250/422]	Time 0.076 (0.071)	Loss 0.3285 (0.2999)	Prec@1 90.625 (88.844)	Prec@5 100.000 (99.859)
Train: [300/422]	Time 0.068 (0.070)	Loss 0.1923 (0.3067)	Prec@1 92.188 (88.516)	Prec@5 100.000 (99.922)
Train: [350/422]	Time 0.074 (0.070)	Loss 0.2257 (0.2888)	Prec@1 95.312 (89.328)	Prec@5 100.000 (99.875)
Train: [400/422]	Time 0.074 (0.073)	Loss 0.3548 (0.2854)	Prec@1 84.375 (89.797)	Prec@5 100.000 (99.844)
Test: [0/47]	Time 0.355 (0.355)	Loss 0.2070 (0.2070)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.032)	Loss 0.2506 (0.2740)	Prec@1 89.062 (90.699)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.023)	Loss 0.2749 (0.2709)	Prec@1 87.500 (90.320)	Prec@5 100.000 (99.886)
Test: [18/40] Acc 89.833
epoch: 19
Train: [0/422]	Time 0.411 (0.411)	Loss 0.3073 (0.3073)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.072 (0.079)	Loss 0.2621 (0.2743)	Prec@1 90.625 (89.675)	Prec@5 100.000 (99.939)
Train: [100/422]	Time 0.068 (0.073)	Loss 0.1597 (0.2837)	Prec@1 93.750 (89.469)	Prec@5 100.000 (99.875)
Train: [150/422]	Time 0.070 (0.071)	Loss 0.2489 (0.2944)	Prec@1 89.062 (89.328)	Prec@5 100.000 (99.859)
Train: [200/422]	Time 0.071 (0.068)	Loss 0.3833 (0.2922)	Prec@1 87.500 (89.594)	Prec@5 98.438 (99.875)
Train: [250/422]	Time 0.072 (0.070)	Loss 0.2320 (0.2864)	Prec@1 89.062 (89.438)	Prec@5 100.000 (99.906)
Train: [300/422]	Time 0.066 (0.071)	Loss 0.2847 (0.2974)	Prec@1 87.500 (88.812)	Prec@5 100.000 (99.969)
Train: [350/422]	Time 0.067 (0.072)	Loss 0.2787 (0.3042)	Prec@1 89.062 (88.766)	Prec@5 100.000 (99.922)
Train: [400/422]	Time 0.068 (0.070)	Loss 0.1874 (0.2905)	Prec@1 92.188 (89.359)	Prec@5 100.000 (99.891)
Test: [0/47]	Time 0.357 (0.357)	Loss 0.2844 (0.2844)	Prec@1 84.375 (84.375)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.032)	Loss 0.2875 (0.2740)	Prec@1 87.500 (90.179)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.015 (0.024)	Loss 0.2205 (0.2600)	Prec@1 89.062 (90.549)	Prec@5 100.000 (99.886)
Test: [19/40] Acc 90.333
epoch: 20
Train: [0/422]	Time 0.401 (0.401)	Loss 0.2315 (0.2315)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.051 (0.060)	Loss 0.2391 (0.2910)	Prec@1 90.625 (88.756)	Prec@5 100.000 (99.969)
Train: [100/422]	Time 0.052 (0.053)	Loss 0.2800 (0.3005)	Prec@1 89.062 (88.922)	Prec@5 98.438 (99.922)
Train: [150/422]	Time 0.054 (0.052)	Loss 0.3414 (0.2902)	Prec@1 87.500 (89.594)	Prec@5 100.000 (99.906)
Train: [200/422]	Time 0.056 (0.051)	Loss 0.4190 (0.2702)	Prec@1 87.500 (89.922)	Prec@5 100.000 (99.922)
Train: [250/422]	Time 0.062 (0.055)	Loss 0.3458 (0.2771)	Prec@1 82.812 (89.656)	Prec@5 100.000 (99.953)
Train: [300/422]	Time 0.062 (0.061)	Loss 0.4013 (0.2861)	Prec@1 85.938 (89.641)	Prec@5 98.438 (99.922)
Train: [350/422]	Time 0.066 (0.065)	Loss 0.3318 (0.2892)	Prec@1 90.625 (89.719)	Prec@5 100.000 (99.891)
Train: [400/422]	Time 0.068 (0.070)	Loss 0.2369 (0.2905)	Prec@1 85.938 (89.656)	Prec@5 100.000 (99.922)
Test: [0/47]	Time 0.353 (0.353)	Loss 0.2669 (0.2669)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.031)	Loss 0.2797 (0.2649)	Prec@1 93.750 (90.327)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.016 (0.024)	Loss 0.2431 (0.2634)	Prec@1 93.750 (90.739)	Prec@5 100.000 (100.000)
Test: [20/40] Acc 90.500
epoch: 21
Train: [0/422]	Time 0.403 (0.403)	Loss 0.3200 (0.3200)	Prec@1 81.250 (81.250)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.074 (0.077)	Loss 0.1953 (0.2841)	Prec@1 92.188 (89.277)	Prec@5 100.000 (99.939)
Train: [100/422]	Time 0.062 (0.069)	Loss 0.2156 (0.2791)	Prec@1 93.750 (89.656)	Prec@5 100.000 (99.953)
Train: [150/422]	Time 0.064 (0.065)	Loss 0.2865 (0.2811)	Prec@1 90.625 (89.609)	Prec@5 100.000 (99.922)
Train: [200/422]	Time 0.052 (0.062)	Loss 0.3490 (0.2843)	Prec@1 89.062 (89.641)	Prec@5 100.000 (99.859)
Train: [250/422]	Time 0.080 (0.071)	Loss 0.1176 (0.2964)	Prec@1 96.875 (89.172)	Prec@5 100.000 (99.891)
Train: [300/422]	Time 0.069 (0.077)	Loss 0.2800 (0.2840)	Prec@1 87.500 (89.328)	Prec@5 100.000 (99.922)
Train: [350/422]	Time 0.068 (0.073)	Loss 0.2229 (0.2571)	Prec@1 92.188 (90.484)	Prec@5 100.000 (99.922)
Train: [400/422]	Time 0.074 (0.071)	Loss 0.3055 (0.2673)	Prec@1 90.625 (90.234)	Prec@5 100.000 (99.906)
Test: [0/47]	Time 0.360 (0.360)	Loss 0.1968 (0.1968)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.032)	Loss 0.2708 (0.2586)	Prec@1 85.938 (90.327)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.014 (0.023)	Loss 0.2464 (0.2534)	Prec@1 93.750 (91.159)	Prec@5 100.000 (99.924)
Test: [21/40] Acc 91.067
epoch: 22
Train: [0/422]	Time 0.384 (0.384)	Loss 0.2970 (0.2970)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.072 (0.083)	Loss 0.2961 (0.2632)	Prec@1 90.625 (89.859)	Prec@5 98.438 (99.877)
Train: [100/422]	Time 0.084 (0.080)	Loss 0.3885 (0.2738)	Prec@1 84.375 (90.094)	Prec@5 100.000 (99.875)
Train: [150/422]	Time 0.075 (0.078)	Loss 0.2542 (0.2781)	Prec@1 87.500 (90.141)	Prec@5 100.000 (99.906)
Train: [200/422]	Time 0.067 (0.071)	Loss 0.2450 (0.2701)	Prec@1 93.750 (90.000)	Prec@5 100.000 (99.969)
Train: [250/422]	Time 0.070 (0.069)	Loss 0.1104 (0.2669)	Prec@1 98.438 (90.266)	Prec@5 100.000 (99.984)
Train: [300/422]	Time 0.063 (0.066)	Loss 0.2001 (0.2783)	Prec@1 92.188 (89.750)	Prec@5 100.000 (99.906)
Train: [350/422]	Time 0.064 (0.063)	Loss 0.2009 (0.2861)	Prec@1 93.750 (89.641)	Prec@5 100.000 (99.859)
Train: [400/422]	Time 0.062 (0.063)	Loss 0.2827 (0.2766)	Prec@1 87.500 (90.297)	Prec@5 100.000 (99.875)
Test: [0/47]	Time 0.354 (0.354)	Loss 0.2820 (0.2820)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.033)	Loss 0.2874 (0.2668)	Prec@1 87.500 (90.253)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.013 (0.024)	Loss 0.2407 (0.2510)	Prec@1 90.625 (91.120)	Prec@5 100.000 (99.886)
Test: [22/40] Acc 91.033
epoch: 23
Train: [0/422]	Time 0.418 (0.418)	Loss 0.1561 (0.1561)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.063 (0.083)	Loss 0.3299 (0.2729)	Prec@1 87.500 (89.982)	Prec@5 100.000 (99.939)
Train: [100/422]	Time 0.061 (0.069)	Loss 0.3188 (0.2640)	Prec@1 89.062 (90.609)	Prec@5 100.000 (99.969)
Train: [150/422]	Time 0.061 (0.062)	Loss 0.3131 (0.2591)	Prec@1 95.312 (90.844)	Prec@5 100.000 (99.891)
Train: [200/422]	Time 0.069 (0.072)	Loss 0.1608 (0.2659)	Prec@1 93.750 (90.156)	Prec@5 100.000 (99.891)
Train: [250/422]	Time 0.068 (0.075)	Loss 0.1509 (0.2704)	Prec@1 93.750 (89.719)	Prec@5 100.000 (99.953)
Train: [300/422]	Time 0.072 (0.072)	Loss 0.1898 (0.2805)	Prec@1 90.625 (89.328)	Prec@5 100.000 (99.938)
Train: [350/422]	Time 0.066 (0.077)	Loss 0.3177 (0.2883)	Prec@1 90.625 (89.047)	Prec@5 100.000 (99.938)
Train: [400/422]	Time 0.073 (0.073)	Loss 0.3235 (0.2788)	Prec@1 85.938 (89.828)	Prec@5 100.000 (99.875)
Test: [0/47]	Time 0.380 (0.380)	Loss 0.1786 (0.1786)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.031)	Loss 0.3535 (0.2711)	Prec@1 85.938 (89.360)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.013 (0.022)	Loss 0.2072 (0.2697)	Prec@1 95.312 (90.206)	Prec@5 100.000 (99.924)
Test: [23/40] Acc 90.167
epoch: 24
Train: [0/422]	Time 0.410 (0.410)	Loss 0.1877 (0.1877)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.078 (0.079)	Loss 0.2498 (0.2363)	Prec@1 95.312 (91.023)	Prec@5 100.000 (99.908)
Train: [100/422]	Time 0.067 (0.071)	Loss 0.1563 (0.2493)	Prec@1 92.188 (90.266)	Prec@5 100.000 (99.922)
Train: [150/422]	Time 0.073 (0.071)	Loss 0.4247 (0.2641)	Prec@1 87.500 (89.891)	Prec@5 100.000 (99.969)
Train: [200/422]	Time 0.070 (0.072)	Loss 0.2520 (0.2659)	Prec@1 92.188 (90.438)	Prec@5 100.000 (99.969)
Train: [250/422]	Time 0.072 (0.071)	Loss 0.2098 (0.2726)	Prec@1 92.188 (90.172)	Prec@5 100.000 (99.953)
Train: [300/422]	Time 0.066 (0.071)	Loss 0.1789 (0.2727)	Prec@1 93.750 (89.922)	Prec@5 100.000 (99.938)
Train: [350/422]	Time 0.083 (0.073)	Loss 0.2794 (0.2579)	Prec@1 89.062 (90.188)	Prec@5 100.000 (99.906)
Train: [400/422]	Time 0.066 (0.073)	Loss 0.4987 (0.2631)	Prec@1 84.375 (90.000)	Prec@5 100.000 (99.859)
Test: [0/47]	Time 0.378 (0.378)	Loss 0.1901 (0.1901)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.032)	Loss 0.2378 (0.2419)	Prec@1 90.625 (90.923)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.014 (0.023)	Loss 0.2270 (0.2461)	Prec@1 90.625 (91.387)	Prec@5 100.000 (99.924)
Test: [24/40] Acc 91.067
epoch: 25
Train: [0/422]	Time 0.408 (0.408)	Loss 0.1342 (0.1342)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.066 (0.077)	Loss 0.2307 (0.2577)	Prec@1 90.625 (90.870)	Prec@5 100.000 (99.939)
Train: [100/422]	Time 0.068 (0.070)	Loss 0.2655 (0.2626)	Prec@1 89.062 (90.297)	Prec@5 100.000 (99.953)
Train: [150/422]	Time 0.067 (0.069)	Loss 0.4041 (0.2579)	Prec@1 81.250 (89.891)	Prec@5 100.000 (99.953)
Train: [200/422]	Time 0.076 (0.070)	Loss 0.2459 (0.2552)	Prec@1 90.625 (90.203)	Prec@5 100.000 (99.906)
Train: [250/422]	Time 0.071 (0.071)	Loss 0.2906 (0.2715)	Prec@1 89.062 (89.812)	Prec@5 100.000 (99.906)
Train: [300/422]	Time 0.070 (0.071)	Loss 0.2513 (0.2811)	Prec@1 90.625 (89.469)	Prec@5 100.000 (99.891)
Train: [350/422]	Time 0.075 (0.071)	Loss 0.3334 (0.2783)	Prec@1 84.375 (89.688)	Prec@5 100.000 (99.906)
Train: [400/422]	Time 0.069 (0.071)	Loss 0.2547 (0.2668)	Prec@1 89.062 (90.094)	Prec@5 100.000 (99.891)
Test: [0/47]	Time 0.336 (0.336)	Loss 0.2419 (0.2419)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.016 (0.032)	Loss 0.2868 (0.2722)	Prec@1 90.625 (90.179)	Prec@5 100.000 (99.926)
Test: [40/47]	Time 0.013 (0.025)	Loss 0.3667 (0.2747)	Prec@1 85.938 (90.091)	Prec@5 100.000 (99.848)
Test: [25/40] Acc 89.800
epoch: 26
Train: [0/422]	Time 0.422 (0.422)	Loss 0.3146 (0.3146)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.062 (0.068)	Loss 0.4848 (0.2292)	Prec@1 79.688 (91.513)	Prec@5 100.000 (99.939)
Train: [100/422]	Time 0.061 (0.060)	Loss 0.3086 (0.2421)	Prec@1 84.375 (90.797)	Prec@5 100.000 (99.953)
Train: [150/422]	Time 0.060 (0.060)	Loss 0.3464 (0.2613)	Prec@1 89.062 (90.297)	Prec@5 100.000 (99.969)
Train: [200/422]	Time 0.069 (0.063)	Loss 0.3292 (0.2598)	Prec@1 89.062 (90.484)	Prec@5 100.000 (99.938)
Train: [250/422]	Time 0.069 (0.067)	Loss 0.2099 (0.2619)	Prec@1 95.312 (90.406)	Prec@5 100.000 (99.922)
Train: [300/422]	Time 0.070 (0.068)	Loss 0.1843 (0.2707)	Prec@1 93.750 (90.141)	Prec@5 100.000 (99.953)
Train: [350/422]	Time 0.075 (0.074)	Loss 0.1939 (0.2712)	Prec@1 95.312 (90.094)	Prec@5 100.000 (99.875)
Train: [400/422]	Time 0.073 (0.077)	Loss 0.2132 (0.2672)	Prec@1 92.188 (90.359)	Prec@5 98.438 (99.875)
Test: [0/47]	Time 0.339 (0.339)	Loss 0.1843 (0.1843)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.031)	Loss 0.1992 (0.2552)	Prec@1 89.062 (90.625)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.014 (0.023)	Loss 0.2569 (0.2560)	Prec@1 92.188 (90.816)	Prec@5 100.000 (99.962)
Test: [26/40] Acc 90.400
epoch: 27
Train: [0/422]	Time 0.394 (0.394)	Loss 0.1978 (0.1978)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.067 (0.069)	Loss 0.4084 (0.2428)	Prec@1 85.938 (91.085)	Prec@5 100.000 (99.969)
Train: [100/422]	Time 0.069 (0.066)	Loss 0.2303 (0.2483)	Prec@1 92.188 (90.891)	Prec@5 100.000 (99.938)
Train: [150/422]	Time 0.083 (0.071)	Loss 0.2882 (0.2616)	Prec@1 87.500 (90.375)	Prec@5 100.000 (99.938)
Train: [200/422]	Time 0.064 (0.073)	Loss 0.2515 (0.2562)	Prec@1 92.188 (90.516)	Prec@5 100.000 (99.953)
Train: [250/422]	Time 0.072 (0.073)	Loss 0.2707 (0.2486)	Prec@1 89.062 (90.672)	Prec@5 100.000 (99.953)
Train: [300/422]	Time 0.078 (0.071)	Loss 0.2030 (0.2570)	Prec@1 92.188 (90.500)	Prec@5 100.000 (99.922)
Train: [350/422]	Time 0.069 (0.070)	Loss 0.2217 (0.2484)	Prec@1 93.750 (91.078)	Prec@5 100.000 (99.922)
Train: [400/422]	Time 0.078 (0.073)	Loss 0.1616 (0.2492)	Prec@1 90.625 (91.047)	Prec@5 100.000 (99.969)
Test: [0/47]	Time 0.369 (0.369)	Loss 0.2322 (0.2322)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.012 (0.030)	Loss 0.2561 (0.2784)	Prec@1 89.062 (89.881)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.021)	Loss 0.2074 (0.2546)	Prec@1 90.625 (90.701)	Prec@5 100.000 (99.924)
Test: [27/40] Acc 90.400
epoch: 28
Train: [0/422]	Time 0.430 (0.430)	Loss 0.4517 (0.4517)	Prec@1 82.812 (82.812)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.082 (0.070)	Loss 0.2055 (0.2357)	Prec@1 92.188 (91.115)	Prec@5 100.000 (99.877)
Train: [100/422]	Time 0.067 (0.067)	Loss 0.2324 (0.2411)	Prec@1 95.312 (90.859)	Prec@5 100.000 (99.906)
Train: [150/422]	Time 0.063 (0.068)	Loss 0.3769 (0.2529)	Prec@1 85.938 (90.484)	Prec@5 100.000 (99.953)
Train: [200/422]	Time 0.080 (0.069)	Loss 0.4385 (0.2638)	Prec@1 87.500 (90.422)	Prec@5 100.000 (99.922)
Train: [250/422]	Time 0.071 (0.077)	Loss 0.0858 (0.2653)	Prec@1 95.312 (90.109)	Prec@5 100.000 (99.891)
Train: [300/422]	Time 0.065 (0.075)	Loss 0.1630 (0.2593)	Prec@1 93.750 (90.078)	Prec@5 100.000 (99.938)
Train: [350/422]	Time 0.068 (0.069)	Loss 0.2345 (0.2485)	Prec@1 92.188 (91.031)	Prec@5 100.000 (99.953)
Train: [400/422]	Time 0.082 (0.069)	Loss 0.2919 (0.2367)	Prec@1 87.500 (91.516)	Prec@5 100.000 (99.922)
Test: [0/47]	Time 0.349 (0.349)	Loss 0.2008 (0.2008)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.031)	Loss 0.2690 (0.2719)	Prec@1 90.625 (90.179)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.022)	Loss 0.2869 (0.2714)	Prec@1 89.062 (90.282)	Prec@5 100.000 (99.962)
Test: [28/40] Acc 90.267
epoch: 29
Train: [0/422]	Time 0.385 (0.385)	Loss 0.1023 (0.1023)	Prec@1 96.875 (96.875)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.061 (0.079)	Loss 0.2238 (0.2434)	Prec@1 87.500 (90.411)	Prec@5 100.000 (99.969)
Train: [100/422]	Time 0.068 (0.072)	Loss 0.3131 (0.2578)	Prec@1 85.938 (90.203)	Prec@5 100.000 (99.906)
Train: [150/422]	Time 0.071 (0.074)	Loss 0.3340 (0.2576)	Prec@1 85.938 (90.547)	Prec@5 100.000 (99.891)
Train: [200/422]	Time 0.067 (0.073)	Loss 0.1571 (0.2459)	Prec@1 95.312 (90.750)	Prec@5 100.000 (99.906)
Train: [250/422]	Time 0.061 (0.068)	Loss 0.1584 (0.2424)	Prec@1 92.188 (90.938)	Prec@5 100.000 (99.922)
Train: [300/422]	Time 0.061 (0.064)	Loss 0.3013 (0.2441)	Prec@1 90.625 (91.250)	Prec@5 100.000 (99.953)
Train: [350/422]	Time 0.060 (0.062)	Loss 0.2040 (0.2440)	Prec@1 95.312 (91.453)	Prec@5 100.000 (99.938)
Train: [400/422]	Time 0.061 (0.062)	Loss 0.2585 (0.2364)	Prec@1 89.062 (91.234)	Prec@5 100.000 (99.953)
Test: [0/47]	Time 0.346 (0.346)	Loss 0.1460 (0.1460)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.032)	Loss 0.2840 (0.2747)	Prec@1 90.625 (89.583)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.023)	Loss 0.3149 (0.2652)	Prec@1 90.625 (90.358)	Prec@5 100.000 (99.886)
Test: [29/40] Acc 90.467
epoch: 30
Train: [0/422]	Time 0.379 (0.379)	Loss 0.1662 (0.1662)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.067 (0.077)	Loss 0.4095 (0.2502)	Prec@1 82.812 (91.054)	Prec@5 100.000 (99.969)
Train: [100/422]	Time 0.078 (0.070)	Loss 0.1571 (0.2505)	Prec@1 95.312 (90.531)	Prec@5 100.000 (99.969)
Train: [150/422]	Time 0.072 (0.070)	Loss 0.2578 (0.2397)	Prec@1 90.625 (90.672)	Prec@5 100.000 (99.984)
Train: [200/422]	Time 0.080 (0.070)	Loss 0.1874 (0.2377)	Prec@1 93.750 (91.203)	Prec@5 100.000 (100.000)
Train: [250/422]	Time 0.078 (0.070)	Loss 0.3328 (0.2399)	Prec@1 84.375 (91.219)	Prec@5 100.000 (99.984)
Train: [300/422]	Time 0.068 (0.068)	Loss 0.1923 (0.2434)	Prec@1 90.625 (90.953)	Prec@5 100.000 (99.922)
Train: [350/422]	Time 0.069 (0.068)	Loss 0.1890 (0.2481)	Prec@1 89.062 (90.859)	Prec@5 100.000 (99.922)
Train: [400/422]	Time 0.072 (0.070)	Loss 0.3222 (0.2394)	Prec@1 90.625 (91.203)	Prec@5 100.000 (99.953)
Test: [0/47]	Time 0.371 (0.371)	Loss 0.1942 (0.1942)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.012 (0.031)	Loss 0.2362 (0.2675)	Prec@1 92.188 (90.402)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.015 (0.024)	Loss 0.2014 (0.2601)	Prec@1 93.750 (91.235)	Prec@5 100.000 (100.000)
Test: [30/40] Acc 90.733
epoch: 31
Train: [0/422]	Time 0.416 (0.416)	Loss 0.2276 (0.2276)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.082 (0.079)	Loss 0.2644 (0.2318)	Prec@1 90.625 (91.483)	Prec@5 100.000 (99.969)
Train: [100/422]	Time 0.069 (0.072)	Loss 0.2382 (0.2337)	Prec@1 92.188 (91.297)	Prec@5 100.000 (99.969)
Train: [150/422]	Time 0.068 (0.070)	Loss 0.1513 (0.2272)	Prec@1 95.312 (91.672)	Prec@5 100.000 (99.969)
Train: [200/422]	Time 0.069 (0.069)	Loss 0.2399 (0.2129)	Prec@1 89.062 (92.141)	Prec@5 100.000 (99.984)
Train: [250/422]	Time 0.072 (0.071)	Loss 0.1618 (0.2380)	Prec@1 90.625 (91.203)	Prec@5 100.000 (99.953)
Train: [300/422]	Time 0.071 (0.070)	Loss 0.2319 (0.2533)	Prec@1 90.625 (90.906)	Prec@5 100.000 (99.938)
Train: [350/422]	Time 0.072 (0.069)	Loss 0.2468 (0.2385)	Prec@1 90.625 (91.328)	Prec@5 100.000 (99.922)
Train: [400/422]	Time 0.069 (0.070)	Loss 0.1932 (0.2345)	Prec@1 93.750 (91.203)	Prec@5 100.000 (99.938)
Test: [0/47]	Time 0.345 (0.345)	Loss 0.3104 (0.3104)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.016 (0.031)	Loss 0.1996 (0.2895)	Prec@1 90.625 (89.732)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.013 (0.024)	Loss 0.2630 (0.2784)	Prec@1 85.938 (90.358)	Prec@5 100.000 (99.924)
Test: [31/40] Acc 90.333
epoch: 32
Train: [0/422]	Time 0.383 (0.383)	Loss 0.3210 (0.3210)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.070 (0.075)	Loss 0.2394 (0.2286)	Prec@1 85.938 (91.176)	Prec@5 100.000 (100.000)
Train: [100/422]	Time 0.072 (0.071)	Loss 0.2686 (0.2340)	Prec@1 93.750 (91.312)	Prec@5 100.000 (99.969)
Train: [150/422]	Time 0.068 (0.071)	Loss 0.2397 (0.2299)	Prec@1 90.625 (91.391)	Prec@5 100.000 (99.953)
Train: [200/422]	Time 0.076 (0.070)	Loss 0.2572 (0.2250)	Prec@1 92.188 (91.109)	Prec@5 100.000 (99.969)
Train: [250/422]	Time 0.059 (0.068)	Loss 0.1222 (0.2303)	Prec@1 95.312 (91.234)	Prec@5 100.000 (99.938)
Train: [300/422]	Time 0.064 (0.068)	Loss 0.2488 (0.2358)	Prec@1 89.062 (91.438)	Prec@5 100.000 (99.906)
Train: [350/422]	Time 0.070 (0.070)	Loss 0.2548 (0.2483)	Prec@1 87.500 (90.969)	Prec@5 100.000 (99.922)
Train: [400/422]	Time 0.066 (0.068)	Loss 0.1766 (0.2424)	Prec@1 92.188 (91.297)	Prec@5 100.000 (99.891)
Test: [0/47]	Time 0.397 (0.397)	Loss 0.2044 (0.2044)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.017 (0.035)	Loss 0.2436 (0.2383)	Prec@1 92.188 (91.295)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.014 (0.025)	Loss 0.2084 (0.2343)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [32/40] Acc 92.000
epoch: 33
Train: [0/422]	Time 0.413 (0.413)	Loss 0.2276 (0.2276)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.069 (0.078)	Loss 0.2410 (0.2305)	Prec@1 90.625 (91.330)	Prec@5 100.000 (100.000)
Train: [100/422]	Time 0.080 (0.074)	Loss 0.2429 (0.2282)	Prec@1 89.062 (91.516)	Prec@5 100.000 (99.969)
Train: [150/422]	Time 0.068 (0.078)	Loss 0.1719 (0.2189)	Prec@1 95.312 (92.219)	Prec@5 100.000 (99.953)
Train: [200/422]	Time 0.065 (0.073)	Loss 0.1513 (0.2184)	Prec@1 96.875 (92.422)	Prec@5 100.000 (99.938)
Train: [250/422]	Time 0.061 (0.064)	Loss 0.2514 (0.2310)	Prec@1 89.062 (91.734)	Prec@5 100.000 (99.922)
Train: [300/422]	Time 0.061 (0.062)	Loss 0.3122 (0.2405)	Prec@1 87.500 (91.156)	Prec@5 100.000 (99.969)
Train: [350/422]	Time 0.061 (0.062)	Loss 0.2955 (0.2430)	Prec@1 85.938 (91.312)	Prec@5 100.000 (100.000)
Train: [400/422]	Time 0.059 (0.062)	Loss 0.2375 (0.2425)	Prec@1 90.625 (91.312)	Prec@5 100.000 (100.000)
Test: [0/47]	Time 0.352 (0.352)	Loss 0.1606 (0.1606)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.031)	Loss 0.2534 (0.2568)	Prec@1 89.062 (90.179)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.013 (0.022)	Loss 0.2225 (0.2509)	Prec@1 93.750 (90.930)	Prec@5 100.000 (99.924)
Test: [33/40] Acc 90.733
epoch: 34
Train: [0/422]	Time 0.398 (0.398)	Loss 0.0784 (0.0784)	Prec@1 96.875 (96.875)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.059 (0.067)	Loss 0.2903 (0.2243)	Prec@1 89.062 (91.820)	Prec@5 100.000 (99.969)
Train: [100/422]	Time 0.067 (0.061)	Loss 0.1434 (0.2252)	Prec@1 93.750 (91.656)	Prec@5 100.000 (99.953)
Train: [150/422]	Time 0.062 (0.065)	Loss 0.2480 (0.2334)	Prec@1 92.188 (91.266)	Prec@5 100.000 (99.938)
Train: [200/422]	Time 0.064 (0.065)	Loss 0.2352 (0.2338)	Prec@1 90.625 (90.969)	Prec@5 100.000 (99.969)
Train: [250/422]	Time 0.061 (0.063)	Loss 0.1452 (0.2294)	Prec@1 93.750 (91.250)	Prec@5 100.000 (99.984)
Train: [300/422]	Time 0.064 (0.063)	Loss 0.1521 (0.2278)	Prec@1 95.312 (91.625)	Prec@5 100.000 (99.969)
Train: [350/422]	Time 0.075 (0.068)	Loss 0.1566 (0.2160)	Prec@1 93.750 (91.875)	Prec@5 100.000 (99.969)
Train: [400/422]	Time 0.069 (0.074)	Loss 0.1414 (0.2313)	Prec@1 96.875 (91.594)	Prec@5 100.000 (99.969)
Test: [0/47]	Time 0.359 (0.359)	Loss 0.2074 (0.2074)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.031)	Loss 0.2360 (0.2618)	Prec@1 89.062 (90.104)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.014 (0.023)	Loss 0.2371 (0.2468)	Prec@1 92.188 (91.349)	Prec@5 100.000 (99.962)
Test: [34/40] Acc 91.000
epoch: 35
Train: [0/422]	Time 0.380 (0.380)	Loss 0.2128 (0.2128)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.068 (0.067)	Loss 0.1825 (0.2185)	Prec@1 93.750 (92.341)	Prec@5 100.000 (99.908)
Train: [100/422]	Time 0.055 (0.061)	Loss 0.2365 (0.2175)	Prec@1 87.500 (91.969)	Prec@5 100.000 (99.938)
Train: [150/422]	Time 0.056 (0.060)	Loss 0.2801 (0.2189)	Prec@1 92.188 (91.641)	Prec@5 100.000 (99.953)
Train: [200/422]	Time 0.051 (0.055)	Loss 0.2443 (0.2177)	Prec@1 87.500 (91.734)	Prec@5 100.000 (99.953)
Train: [250/422]	Time 0.051 (0.052)	Loss 0.2099 (0.2246)	Prec@1 92.188 (91.422)	Prec@5 100.000 (99.922)
Train: [300/422]	Time 0.051 (0.051)	Loss 0.1932 (0.2372)	Prec@1 92.188 (90.953)	Prec@5 100.000 (99.906)
Train: [350/422]	Time 0.049 (0.051)	Loss 0.1319 (0.2268)	Prec@1 95.312 (91.422)	Prec@5 100.000 (99.938)
Train: [400/422]	Time 0.048 (0.051)	Loss 0.2523 (0.2283)	Prec@1 90.625 (91.469)	Prec@5 100.000 (99.922)
Test: [0/47]	Time 0.313 (0.313)	Loss 0.2295 (0.2295)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.016 (0.030)	Loss 0.2793 (0.2610)	Prec@1 90.625 (90.030)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.013 (0.023)	Loss 0.2068 (0.2575)	Prec@1 95.312 (90.511)	Prec@5 100.000 (99.924)
Test: [35/40] Acc 90.367
epoch: 36
Train: [0/422]	Time 0.405 (0.405)	Loss 0.2399 (0.2399)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.066 (0.074)	Loss 0.2054 (0.2191)	Prec@1 92.188 (91.483)	Prec@5 100.000 (99.969)
Train: [100/422]	Time 0.065 (0.067)	Loss 0.3321 (0.2227)	Prec@1 84.375 (91.453)	Prec@5 100.000 (99.922)
Train: [150/422]	Time 0.068 (0.068)	Loss 0.2764 (0.2255)	Prec@1 89.062 (91.484)	Prec@5 100.000 (99.906)
Train: [200/422]	Time 0.069 (0.070)	Loss 0.2980 (0.2219)	Prec@1 85.938 (91.516)	Prec@5 100.000 (99.953)
Train: [250/422]	Time 0.070 (0.070)	Loss 0.2823 (0.2256)	Prec@1 92.188 (91.484)	Prec@5 100.000 (99.953)
Train: [300/422]	Time 0.070 (0.069)	Loss 0.1809 (0.2283)	Prec@1 92.188 (91.734)	Prec@5 100.000 (99.938)
Train: [350/422]	Time 0.070 (0.068)	Loss 0.2588 (0.2283)	Prec@1 90.625 (91.641)	Prec@5 100.000 (99.969)
Train: [400/422]	Time 0.069 (0.069)	Loss 0.2775 (0.2199)	Prec@1 90.625 (91.875)	Prec@5 100.000 (99.984)
Test: [0/47]	Time 0.344 (0.344)	Loss 0.1824 (0.1824)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.030)	Loss 0.2161 (0.2342)	Prec@1 92.188 (90.625)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.013 (0.022)	Loss 0.2310 (0.2370)	Prec@1 92.188 (91.349)	Prec@5 100.000 (99.962)
Test: [36/40] Acc 91.200
epoch: 37
Train: [0/422]	Time 0.386 (0.386)	Loss 0.2816 (0.2816)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.073 (0.075)	Loss 0.1928 (0.2189)	Prec@1 93.750 (91.697)	Prec@5 100.000 (99.939)
Train: [100/422]	Time 0.066 (0.069)	Loss 0.0681 (0.2094)	Prec@1 98.438 (92.188)	Prec@5 100.000 (99.953)
Train: [150/422]	Time 0.069 (0.069)	Loss 0.3384 (0.2208)	Prec@1 89.062 (92.000)	Prec@5 100.000 (99.969)
Train: [200/422]	Time 0.072 (0.072)	Loss 0.2764 (0.2275)	Prec@1 92.188 (91.516)	Prec@5 100.000 (99.953)
Train: [250/422]	Time 0.069 (0.071)	Loss 0.2007 (0.2113)	Prec@1 92.188 (91.766)	Prec@5 100.000 (99.969)
Train: [300/422]	Time 0.071 (0.069)	Loss 0.1523 (0.2131)	Prec@1 92.188 (91.984)	Prec@5 100.000 (99.969)
Train: [350/422]	Time 0.065 (0.069)	Loss 0.2614 (0.2186)	Prec@1 89.062 (92.141)	Prec@5 100.000 (99.938)
Train: [400/422]	Time 0.091 (0.069)	Loss 0.2554 (0.2229)	Prec@1 89.062 (91.672)	Prec@5 100.000 (99.906)
Test: [0/47]	Time 0.408 (0.408)	Loss 0.2995 (0.2995)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.033)	Loss 0.2397 (0.2554)	Prec@1 89.062 (90.104)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.016 (0.024)	Loss 0.3279 (0.2593)	Prec@1 87.500 (90.511)	Prec@5 100.000 (99.962)
Test: [37/40] Acc 90.333
epoch: 38
Train: [0/422]	Time 0.409 (0.409)	Loss 0.4504 (0.4504)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.086 (0.079)	Loss 0.1992 (0.2074)	Prec@1 89.062 (92.494)	Prec@5 100.000 (99.939)
Train: [100/422]	Time 0.067 (0.072)	Loss 0.3922 (0.2038)	Prec@1 84.375 (92.641)	Prec@5 100.000 (99.969)
Train: [150/422]	Time 0.077 (0.070)	Loss 0.1367 (0.2070)	Prec@1 93.750 (92.234)	Prec@5 100.000 (100.000)
Train: [200/422]	Time 0.066 (0.069)	Loss 0.1899 (0.2084)	Prec@1 93.750 (91.938)	Prec@5 100.000 (99.984)
Train: [250/422]	Time 0.087 (0.073)	Loss 0.2338 (0.2121)	Prec@1 93.750 (92.156)	Prec@5 100.000 (99.984)
Train: [300/422]	Time 0.070 (0.073)	Loss 0.2922 (0.2179)	Prec@1 89.062 (92.156)	Prec@5 100.000 (100.000)
Train: [350/422]	Time 0.071 (0.069)	Loss 0.2203 (0.2221)	Prec@1 93.750 (91.734)	Prec@5 100.000 (99.969)
Train: [400/422]	Time 0.066 (0.068)	Loss 0.1671 (0.2178)	Prec@1 95.312 (91.891)	Prec@5 100.000 (99.953)
Test: [0/47]	Time 0.347 (0.347)	Loss 0.2293 (0.2293)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.016 (0.032)	Loss 0.2078 (0.2760)	Prec@1 90.625 (90.179)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.016 (0.024)	Loss 0.2897 (0.2670)	Prec@1 93.750 (90.511)	Prec@5 100.000 (99.962)
Test: [38/40] Acc 90.467
epoch: 39
Train: [0/422]	Time 0.392 (0.392)	Loss 0.2860 (0.2860)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.081 (0.078)	Loss 0.1761 (0.2025)	Prec@1 93.750 (92.279)	Prec@5 100.000 (100.000)
Train: [100/422]	Time 0.067 (0.070)	Loss 0.2087 (0.2090)	Prec@1 87.500 (92.031)	Prec@5 100.000 (100.000)
Train: [150/422]	Time 0.080 (0.070)	Loss 0.3840 (0.2243)	Prec@1 84.375 (91.328)	Prec@5 100.000 (100.000)
Train: [200/422]	Time 0.068 (0.069)	Loss 0.2199 (0.2252)	Prec@1 93.750 (91.516)	Prec@5 100.000 (99.969)
Train: [250/422]	Time 0.063 (0.069)	Loss 0.1518 (0.2160)	Prec@1 92.188 (91.984)	Prec@5 100.000 (99.969)
Train: [300/422]	Time 0.072 (0.069)	Loss 0.2316 (0.2034)	Prec@1 93.750 (92.375)	Prec@5 100.000 (100.000)
Train: [350/422]	Time 0.078 (0.073)	Loss 0.0914 (0.2029)	Prec@1 98.438 (92.500)	Prec@5 100.000 (99.984)
Train: [400/422]	Time 0.080 (0.078)	Loss 0.1414 (0.2161)	Prec@1 95.312 (91.766)	Prec@5 100.000 (99.922)
Test: [0/47]	Time 0.357 (0.357)	Loss 0.2026 (0.2026)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.030)	Loss 0.2423 (0.2547)	Prec@1 89.062 (90.848)	Prec@5 100.000 (99.926)
Test: [40/47]	Time 0.013 (0.021)	Loss 0.2991 (0.2560)	Prec@1 92.188 (91.387)	Prec@5 100.000 (99.962)
Test: [39/40] Acc 91.067
best ACC: 92.0
