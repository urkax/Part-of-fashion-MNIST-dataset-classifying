split train val random seed:8245
RANDOM SEED: 8245
data catagory: train
len of img: 27000
data catagory: val
len of img: 3000
data catagory: test
len of img: 5000
using device: cuda
~fc: Conv2d1.conv.weight
~fc: Conv2d1.bn.weight
~fc: Conv2d1.bn.bias
~fc: Conv2d2.conv.weight
~fc: Conv2d2.bn.weight
~fc: Conv2d2.bn.bias
~fc: Mixed_3a.branch1x1.conv.weight
~fc: Mixed_3a.branch1x1.bn.weight
~fc: Mixed_3a.branch1x1.bn.bias
~fc: Mixed_3a.branch3x3_1.conv.weight
~fc: Mixed_3a.branch3x3_1.bn.weight
~fc: Mixed_3a.branch3x3_1.bn.bias
~fc: Mixed_3a.branch3x3_2.conv.weight
~fc: Mixed_3a.branch3x3_2.bn.weight
~fc: Mixed_3a.branch3x3_2.bn.bias
~fc: Mixed_3a.branch5x5_1.conv.weight
~fc: Mixed_3a.branch5x5_1.bn.weight
~fc: Mixed_3a.branch5x5_1.bn.bias
~fc: Mixed_3a.branch5x5_2.conv.weight
~fc: Mixed_3a.branch5x5_2.bn.weight
~fc: Mixed_3a.branch5x5_2.bn.bias
~fc: Mixed_3a.branch_pool.conv.weight
~fc: Mixed_3a.branch_pool.bn.weight
~fc: Mixed_3a.branch_pool.bn.bias
~fc: Mixed_3b.branch1x1.conv.weight
~fc: Mixed_3b.branch1x1.bn.weight
~fc: Mixed_3b.branch1x1.bn.bias
~fc: Mixed_3b.branch3x3_1.conv.weight
~fc: Mixed_3b.branch3x3_1.bn.weight
~fc: Mixed_3b.branch3x3_1.bn.bias
~fc: Mixed_3b.branch3x3_2.conv.weight
~fc: Mixed_3b.branch3x3_2.bn.weight
~fc: Mixed_3b.branch3x3_2.bn.bias
~fc: Mixed_3b.branch5x5_1.conv.weight
~fc: Mixed_3b.branch5x5_1.bn.weight
~fc: Mixed_3b.branch5x5_1.bn.bias
~fc: Mixed_3b.branch5x5_2.conv.weight
~fc: Mixed_3b.branch5x5_2.bn.weight
~fc: Mixed_3b.branch5x5_2.bn.bias
~fc: Mixed_3b.branch_pool.conv.weight
~fc: Mixed_3b.branch_pool.bn.weight
~fc: Mixed_3b.branch_pool.bn.bias
~fc: Mixed_4a.branch1x1.conv.weight
~fc: Mixed_4a.branch1x1.bn.weight
~fc: Mixed_4a.branch1x1.bn.bias
~fc: Mixed_4a.branch3x3_1.conv.weight
~fc: Mixed_4a.branch3x3_1.bn.weight
~fc: Mixed_4a.branch3x3_1.bn.bias
~fc: Mixed_4a.branch3x3_2.conv.weight
~fc: Mixed_4a.branch3x3_2.bn.weight
~fc: Mixed_4a.branch3x3_2.bn.bias
~fc: Mixed_4a.branch5x5_1.conv.weight
~fc: Mixed_4a.branch5x5_1.bn.weight
~fc: Mixed_4a.branch5x5_1.bn.bias
~fc: Mixed_4a.branch5x5_2.conv.weight
~fc: Mixed_4a.branch5x5_2.bn.weight
~fc: Mixed_4a.branch5x5_2.bn.bias
~fc: Mixed_4a.branch_pool.conv.weight
~fc: Mixed_4a.branch_pool.bn.weight
~fc: Mixed_4a.branch_pool.bn.bias
~fc: Mixed_4b.branch1x1.conv.weight
~fc: Mixed_4b.branch1x1.bn.weight
~fc: Mixed_4b.branch1x1.bn.bias
~fc: Mixed_4b.branch3x3_1.conv.weight
~fc: Mixed_4b.branch3x3_1.bn.weight
~fc: Mixed_4b.branch3x3_1.bn.bias
~fc: Mixed_4b.branch3x3_2.conv.weight
~fc: Mixed_4b.branch3x3_2.bn.weight
~fc: Mixed_4b.branch3x3_2.bn.bias
~fc: Mixed_4b.branch5x5_1.conv.weight
~fc: Mixed_4b.branch5x5_1.bn.weight
~fc: Mixed_4b.branch5x5_1.bn.bias
~fc: Mixed_4b.branch5x5_2.conv.weight
~fc: Mixed_4b.branch5x5_2.bn.weight
~fc: Mixed_4b.branch5x5_2.bn.bias
~fc: Mixed_4b.branch_pool.conv.weight
~fc: Mixed_4b.branch_pool.bn.weight
~fc: Mixed_4b.branch_pool.bn.bias
~fc: Mixed_4c.branch1x1.conv.weight
~fc: Mixed_4c.branch1x1.bn.weight
~fc: Mixed_4c.branch1x1.bn.bias
~fc: Mixed_4c.branch3x3_1.conv.weight
~fc: Mixed_4c.branch3x3_1.bn.weight
~fc: Mixed_4c.branch3x3_1.bn.bias
~fc: Mixed_4c.branch3x3_2.conv.weight
~fc: Mixed_4c.branch3x3_2.bn.weight
~fc: Mixed_4c.branch3x3_2.bn.bias
~fc: Mixed_4c.branch5x5_1.conv.weight
~fc: Mixed_4c.branch5x5_1.bn.weight
~fc: Mixed_4c.branch5x5_1.bn.bias
~fc: Mixed_4c.branch5x5_2.conv.weight
~fc: Mixed_4c.branch5x5_2.bn.weight
~fc: Mixed_4c.branch5x5_2.bn.bias
~fc: Mixed_4c.branch_pool.conv.weight
~fc: Mixed_4c.branch_pool.bn.weight
~fc: Mixed_4c.branch_pool.bn.bias
~fc: Mixed_4d.branch1x1.conv.weight
~fc: Mixed_4d.branch1x1.bn.weight
~fc: Mixed_4d.branch1x1.bn.bias
~fc: Mixed_4d.branch3x3_1.conv.weight
~fc: Mixed_4d.branch3x3_1.bn.weight
~fc: Mixed_4d.branch3x3_1.bn.bias
~fc: Mixed_4d.branch3x3_2.conv.weight
~fc: Mixed_4d.branch3x3_2.bn.weight
~fc: Mixed_4d.branch3x3_2.bn.bias
~fc: Mixed_4d.branch5x5_1.conv.weight
~fc: Mixed_4d.branch5x5_1.bn.weight
~fc: Mixed_4d.branch5x5_1.bn.bias
~fc: Mixed_4d.branch5x5_2.conv.weight
~fc: Mixed_4d.branch5x5_2.bn.weight
~fc: Mixed_4d.branch5x5_2.bn.bias
~fc: Mixed_4d.branch_pool.conv.weight
~fc: Mixed_4d.branch_pool.bn.weight
~fc: Mixed_4d.branch_pool.bn.bias
~fc: Mixed_4e.branch1x1.conv.weight
~fc: Mixed_4e.branch1x1.bn.weight
~fc: Mixed_4e.branch1x1.bn.bias
~fc: Mixed_4e.branch3x3_1.conv.weight
~fc: Mixed_4e.branch3x3_1.bn.weight
~fc: Mixed_4e.branch3x3_1.bn.bias
~fc: Mixed_4e.branch3x3_2.conv.weight
~fc: Mixed_4e.branch3x3_2.bn.weight
~fc: Mixed_4e.branch3x3_2.bn.bias
~fc: Mixed_4e.branch5x5_1.conv.weight
~fc: Mixed_4e.branch5x5_1.bn.weight
~fc: Mixed_4e.branch5x5_1.bn.bias
~fc: Mixed_4e.branch5x5_2.conv.weight
~fc: Mixed_4e.branch5x5_2.bn.weight
~fc: Mixed_4e.branch5x5_2.bn.bias
~fc: Mixed_4e.branch_pool.conv.weight
~fc: Mixed_4e.branch_pool.bn.weight
~fc: Mixed_4e.branch_pool.bn.bias
~fc: Mixed_5a.branch1x1.conv.weight
~fc: Mixed_5a.branch1x1.bn.weight
~fc: Mixed_5a.branch1x1.bn.bias
~fc: Mixed_5a.branch3x3_1.conv.weight
~fc: Mixed_5a.branch3x3_1.bn.weight
~fc: Mixed_5a.branch3x3_1.bn.bias
~fc: Mixed_5a.branch3x3_2.conv.weight
~fc: Mixed_5a.branch3x3_2.bn.weight
~fc: Mixed_5a.branch3x3_2.bn.bias
~fc: Mixed_5a.branch5x5_1.conv.weight
~fc: Mixed_5a.branch5x5_1.bn.weight
~fc: Mixed_5a.branch5x5_1.bn.bias
~fc: Mixed_5a.branch5x5_2.conv.weight
~fc: Mixed_5a.branch5x5_2.bn.weight
~fc: Mixed_5a.branch5x5_2.bn.bias
~fc: Mixed_5a.branch_pool.conv.weight
~fc: Mixed_5a.branch_pool.bn.weight
~fc: Mixed_5a.branch_pool.bn.bias
~fc: Mixed_5b.branch1x1.conv.weight
~fc: Mixed_5b.branch1x1.bn.weight
~fc: Mixed_5b.branch1x1.bn.bias
~fc: Mixed_5b.branch3x3_1.conv.weight
~fc: Mixed_5b.branch3x3_1.bn.weight
~fc: Mixed_5b.branch3x3_1.bn.bias
~fc: Mixed_5b.branch3x3_2.conv.weight
~fc: Mixed_5b.branch3x3_2.bn.weight
~fc: Mixed_5b.branch3x3_2.bn.bias
~fc: Mixed_5b.branch5x5_1.conv.weight
~fc: Mixed_5b.branch5x5_1.bn.weight
~fc: Mixed_5b.branch5x5_1.bn.bias
~fc: Mixed_5b.branch5x5_2.conv.weight
~fc: Mixed_5b.branch5x5_2.bn.weight
~fc: Mixed_5b.branch5x5_2.bn.bias
~fc: Mixed_5b.branch_pool.conv.weight
~fc: Mixed_5b.branch_pool.bn.weight
~fc: Mixed_5b.branch_pool.bn.bias
~fc: fc2.weight
~fc: fc2.bias
fc learning rate: 0.005
not fc learning rate: 0.005
googleNet(
  (Conv2d1): BasicConv2d(
    (conv): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
    (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (Conv2d2): BasicConv2d(
    (conv): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (Mixed_3a): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(4, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_3b): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(8, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_4a): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(120, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(24, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(52, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(120, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(4, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(4, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(12, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(120, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_4b): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(128, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(128, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(28, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(28, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(56, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(128, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(6, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_4c): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(128, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(12, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(12, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_4d): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(128, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(28, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(128, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(36, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(36, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(72, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_4e): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(132, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(132, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(40, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(132, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(8, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(132, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_5a): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(208, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(208, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(40, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(208, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(8, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(208, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_5b): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(208, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(208, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(208, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(12, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(12, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(208, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (dropout_layer): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
)
epoch: 0
Train: [0/422]	Time 0.518 (0.518)	Loss 2.4017 (2.4017)	Prec@1 7.812 (7.812)	Prec@5 42.188 (42.188)
Train: [50/422]	Time 0.053 (0.062)	Loss 1.5385 (1.9641)	Prec@1 45.312 (27.696)	Prec@5 95.312 (76.900)
Train: [100/422]	Time 0.059 (0.053)	Loss 1.0810 (1.6287)	Prec@1 54.688 (39.859)	Prec@5 98.438 (87.000)
Train: [150/422]	Time 0.053 (0.055)	Loss 0.6660 (1.1827)	Prec@1 78.125 (56.000)	Prec@5 98.438 (97.078)
Train: [200/422]	Time 0.057 (0.058)	Loss 0.9831 (0.9959)	Prec@1 65.625 (63.094)	Prec@5 100.000 (98.109)
Train: [250/422]	Time 0.060 (0.060)	Loss 0.9631 (0.8996)	Prec@1 64.062 (66.875)	Prec@5 98.438 (98.531)
Train: [300/422]	Time 0.064 (0.062)	Loss 0.8617 (0.8479)	Prec@1 68.750 (68.359)	Prec@5 100.000 (98.609)
Train: [350/422]	Time 0.056 (0.063)	Loss 0.7458 (0.8082)	Prec@1 67.188 (69.500)	Prec@5 100.000 (98.781)
Train: [400/422]	Time 0.053 (0.059)	Loss 0.6585 (0.7753)	Prec@1 75.000 (70.953)	Prec@5 100.000 (99.031)
Test: [0/47]	Time 0.357 (0.357)	Loss 0.5218 (0.5218)	Prec@1 79.688 (79.688)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.018 (0.092)	Loss 0.6778 (0.6868)	Prec@1 68.750 (72.693)	Prec@5 98.438 (99.702)
Test: [40/47]	Time 0.014 (0.055)	Loss 0.4959 (0.6940)	Prec@1 82.812 (72.866)	Prec@5 100.000 (99.505)
Test: [0/40] Acc 72.900
epoch: 1
Train: [0/422]	Time 0.466 (0.466)	Loss 0.6856 (0.6856)	Prec@1 68.750 (68.750)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.050 (0.066)	Loss 0.7582 (0.7137)	Prec@1 70.312 (72.641)	Prec@5 98.438 (99.081)
Train: [100/422]	Time 0.050 (0.054)	Loss 0.6932 (0.7034)	Prec@1 78.125 (73.391)	Prec@5 98.438 (99.188)
Train: [150/422]	Time 0.052 (0.051)	Loss 0.6368 (0.6983)	Prec@1 76.562 (74.125)	Prec@5 100.000 (99.203)
Train: [200/422]	Time 0.061 (0.053)	Loss 0.7643 (0.7026)	Prec@1 70.312 (74.062)	Prec@5 100.000 (99.094)
Train: [250/422]	Time 0.061 (0.056)	Loss 0.5646 (0.6881)	Prec@1 81.250 (74.484)	Prec@5 98.438 (99.078)
Train: [300/422]	Time 0.056 (0.060)	Loss 0.6498 (0.6596)	Prec@1 71.875 (75.391)	Prec@5 100.000 (99.125)
Train: [350/422]	Time 0.052 (0.057)	Loss 0.6925 (0.6376)	Prec@1 79.688 (76.031)	Prec@5 96.875 (99.250)
Train: [400/422]	Time 0.052 (0.052)	Loss 0.4890 (0.6345)	Prec@1 82.812 (76.234)	Prec@5 100.000 (99.406)
Test: [0/47]	Time 0.345 (0.345)	Loss 0.5845 (0.5845)	Prec@1 78.125 (78.125)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.032)	Loss 0.7855 (0.6091)	Prec@1 71.875 (76.860)	Prec@5 100.000 (99.926)
Test: [40/47]	Time 0.015 (0.026)	Loss 0.4945 (0.6101)	Prec@1 81.250 (76.448)	Prec@5 100.000 (99.771)
Test: [1/40] Acc 76.633
epoch: 2
Train: [0/422]	Time 0.433 (0.433)	Loss 0.7318 (0.7318)	Prec@1 70.312 (70.312)	Prec@5 98.438 (98.438)
Train: [50/422]	Time 0.054 (0.065)	Loss 0.5218 (0.5952)	Prec@1 73.438 (77.451)	Prec@5 100.000 (99.449)
Train: [100/422]	Time 0.060 (0.057)	Loss 0.7553 (0.6056)	Prec@1 71.875 (77.406)	Prec@5 100.000 (99.469)
Train: [150/422]	Time 0.052 (0.057)	Loss 0.7993 (0.6149)	Prec@1 65.625 (77.188)	Prec@5 98.438 (99.438)
Train: [200/422]	Time 0.056 (0.056)	Loss 0.5648 (0.6030)	Prec@1 79.688 (77.531)	Prec@5 100.000 (99.391)
Train: [250/422]	Time 0.059 (0.057)	Loss 0.7372 (0.5800)	Prec@1 76.562 (78.328)	Prec@5 100.000 (99.531)
Train: [300/422]	Time 0.057 (0.061)	Loss 0.7003 (0.5758)	Prec@1 78.125 (78.547)	Prec@5 98.438 (99.516)
Train: [350/422]	Time 0.054 (0.060)	Loss 0.7195 (0.5837)	Prec@1 78.125 (78.516)	Prec@5 100.000 (99.406)
Train: [400/422]	Time 0.057 (0.057)	Loss 0.4002 (0.5677)	Prec@1 85.938 (78.969)	Prec@5 100.000 (99.484)
Test: [0/47]	Time 0.341 (0.341)	Loss 0.4046 (0.4046)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.031)	Loss 0.5880 (0.4811)	Prec@1 76.562 (81.250)	Prec@5 100.000 (99.851)
Test: [40/47]	Time 0.012 (0.023)	Loss 0.4298 (0.4823)	Prec@1 79.688 (81.364)	Prec@5 100.000 (99.657)
Test: [2/40] Acc 81.433
epoch: 3
Train: [0/422]	Time 0.407 (0.407)	Loss 0.5235 (0.5235)	Prec@1 79.688 (79.688)	Prec@5 98.438 (98.438)
Train: [50/422]	Time 0.059 (0.061)	Loss 0.5495 (0.5439)	Prec@1 85.938 (79.718)	Prec@5 100.000 (99.602)
Train: [100/422]	Time 0.047 (0.054)	Loss 0.6773 (0.5482)	Prec@1 70.312 (79.734)	Prec@5 100.000 (99.484)
Train: [150/422]	Time 0.053 (0.053)	Loss 0.4987 (0.5394)	Prec@1 78.125 (80.016)	Prec@5 100.000 (99.438)
Train: [200/422]	Time 0.053 (0.052)	Loss 0.4967 (0.5276)	Prec@1 81.250 (80.766)	Prec@5 100.000 (99.391)
Train: [250/422]	Time 0.053 (0.052)	Loss 0.6582 (0.5383)	Prec@1 76.562 (80.531)	Prec@5 98.438 (99.422)
Train: [300/422]	Time 0.057 (0.052)	Loss 0.6731 (0.5370)	Prec@1 76.562 (79.953)	Prec@5 98.438 (99.578)
Train: [350/422]	Time 0.057 (0.055)	Loss 0.5336 (0.5184)	Prec@1 78.125 (80.453)	Prec@5 98.438 (99.531)
Train: [400/422]	Time 0.074 (0.059)	Loss 0.6491 (0.5272)	Prec@1 71.875 (80.203)	Prec@5 100.000 (99.531)
Test: [0/47]	Time 0.383 (0.383)	Loss 0.4873 (0.4873)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.032)	Loss 0.5451 (0.4620)	Prec@1 81.250 (82.515)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.012 (0.023)	Loss 0.4665 (0.4675)	Prec@1 84.375 (82.470)	Prec@5 100.000 (99.809)
Test: [3/40] Acc 81.900
epoch: 4
Train: [0/422]	Time 0.412 (0.412)	Loss 0.4712 (0.4712)	Prec@1 75.000 (75.000)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.053 (0.065)	Loss 0.4848 (0.4891)	Prec@1 84.375 (82.629)	Prec@5 98.438 (99.663)
Train: [100/422]	Time 0.060 (0.056)	Loss 0.5379 (0.4935)	Prec@1 81.250 (82.609)	Prec@5 100.000 (99.625)
Train: [150/422]	Time 0.064 (0.056)	Loss 0.5078 (0.5106)	Prec@1 85.938 (81.625)	Prec@5 98.438 (99.594)
Train: [200/422]	Time 0.062 (0.057)	Loss 0.4789 (0.5148)	Prec@1 76.562 (81.078)	Prec@5 100.000 (99.594)
Train: [250/422]	Time 0.048 (0.055)	Loss 0.4182 (0.5019)	Prec@1 85.938 (81.781)	Prec@5 98.438 (99.531)
Train: [300/422]	Time 0.055 (0.055)	Loss 0.4439 (0.5014)	Prec@1 85.938 (81.906)	Prec@5 100.000 (99.547)
Train: [350/422]	Time 0.056 (0.057)	Loss 0.4170 (0.5085)	Prec@1 85.938 (81.328)	Prec@5 100.000 (99.625)
Train: [400/422]	Time 0.052 (0.057)	Loss 0.4162 (0.4883)	Prec@1 81.250 (81.531)	Prec@5 100.000 (99.750)
Test: [0/47]	Time 0.335 (0.335)	Loss 0.3285 (0.3285)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.029)	Loss 0.5401 (0.3951)	Prec@1 81.250 (85.640)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.012 (0.021)	Loss 0.4539 (0.4097)	Prec@1 84.375 (85.366)	Prec@5 100.000 (99.809)
Test: [4/40] Acc 85.000
epoch: 5
Train: [0/422]	Time 0.362 (0.362)	Loss 0.4545 (0.4545)	Prec@1 82.812 (82.812)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.059 (0.060)	Loss 0.3712 (0.4516)	Prec@1 84.375 (83.946)	Prec@5 100.000 (99.510)
Train: [100/422]	Time 0.065 (0.057)	Loss 0.3968 (0.4794)	Prec@1 87.500 (82.719)	Prec@5 100.000 (99.516)
Train: [150/422]	Time 0.060 (0.061)	Loss 0.4454 (0.4877)	Prec@1 85.938 (82.359)	Prec@5 100.000 (99.641)
Train: [200/422]	Time 0.072 (0.060)	Loss 0.3217 (0.4650)	Prec@1 89.062 (83.109)	Prec@5 98.438 (99.781)
Train: [250/422]	Time 0.058 (0.059)	Loss 0.4240 (0.4668)	Prec@1 85.938 (82.641)	Prec@5 100.000 (99.781)
Train: [300/422]	Time 0.052 (0.059)	Loss 0.7323 (0.4639)	Prec@1 78.125 (83.047)	Prec@5 98.438 (99.719)
Train: [350/422]	Time 0.055 (0.059)	Loss 0.3464 (0.4587)	Prec@1 84.375 (83.688)	Prec@5 100.000 (99.703)
Train: [400/422]	Time 0.052 (0.056)	Loss 0.4477 (0.4597)	Prec@1 84.375 (83.172)	Prec@5 100.000 (99.625)
Test: [0/47]	Time 0.367 (0.367)	Loss 0.3719 (0.3719)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.031)	Loss 0.4596 (0.3895)	Prec@1 79.688 (84.896)	Prec@5 100.000 (99.926)
Test: [40/47]	Time 0.011 (0.022)	Loss 0.3404 (0.3901)	Prec@1 84.375 (85.099)	Prec@5 100.000 (99.809)
Test: [5/40] Acc 84.933
epoch: 6
Train: [0/422]	Time 0.452 (0.452)	Loss 0.6140 (0.6140)	Prec@1 76.562 (76.562)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.062 (0.070)	Loss 0.5499 (0.4692)	Prec@1 79.688 (82.629)	Prec@5 100.000 (99.632)
Train: [100/422]	Time 0.059 (0.060)	Loss 0.5684 (0.4541)	Prec@1 76.562 (83.625)	Prec@5 98.438 (99.641)
Train: [150/422]	Time 0.055 (0.057)	Loss 0.5938 (0.4533)	Prec@1 81.250 (84.094)	Prec@5 95.312 (99.641)
Train: [200/422]	Time 0.054 (0.057)	Loss 0.3800 (0.4605)	Prec@1 84.375 (83.641)	Prec@5 100.000 (99.734)
Train: [250/422]	Time 0.055 (0.057)	Loss 0.4798 (0.4399)	Prec@1 81.250 (83.891)	Prec@5 100.000 (99.797)
Train: [300/422]	Time 0.057 (0.057)	Loss 0.5616 (0.4466)	Prec@1 78.125 (83.484)	Prec@5 96.875 (99.672)
Train: [350/422]	Time 0.061 (0.058)	Loss 0.3023 (0.4480)	Prec@1 89.062 (83.297)	Prec@5 100.000 (99.625)
Train: [400/422]	Time 0.059 (0.061)	Loss 0.5000 (0.4444)	Prec@1 78.125 (83.500)	Prec@5 100.000 (99.625)
Test: [0/47]	Time 0.368 (0.368)	Loss 0.3481 (0.3481)	Prec@1 81.250 (81.250)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.017 (0.035)	Loss 0.4275 (0.3864)	Prec@1 81.250 (84.970)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.015 (0.026)	Loss 0.3152 (0.3862)	Prec@1 90.625 (85.671)	Prec@5 100.000 (99.924)
Test: [6/40] Acc 85.767
epoch: 7
Train: [0/422]	Time 0.409 (0.409)	Loss 0.4523 (0.4523)	Prec@1 81.250 (81.250)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.061 (0.067)	Loss 0.5794 (0.4273)	Prec@1 81.250 (84.222)	Prec@5 100.000 (99.632)
Train: [100/422]	Time 0.062 (0.059)	Loss 0.4619 (0.4259)	Prec@1 82.812 (84.406)	Prec@5 100.000 (99.703)
Train: [150/422]	Time 0.063 (0.060)	Loss 0.3977 (0.4170)	Prec@1 82.812 (84.672)	Prec@5 100.000 (99.734)
Train: [200/422]	Time 0.060 (0.064)	Loss 0.2967 (0.4243)	Prec@1 89.062 (84.266)	Prec@5 100.000 (99.703)
Train: [250/422]	Time 0.055 (0.061)	Loss 0.2876 (0.4251)	Prec@1 89.062 (84.156)	Prec@5 100.000 (99.719)
Train: [300/422]	Time 0.055 (0.058)	Loss 0.2389 (0.4175)	Prec@1 90.625 (84.641)	Prec@5 100.000 (99.750)
Train: [350/422]	Time 0.054 (0.057)	Loss 0.3689 (0.4286)	Prec@1 85.938 (84.391)	Prec@5 100.000 (99.750)
Train: [400/422]	Time 0.054 (0.056)	Loss 0.2634 (0.4164)	Prec@1 89.062 (84.906)	Prec@5 100.000 (99.766)
Test: [0/47]	Time 0.358 (0.358)	Loss 0.4261 (0.4261)	Prec@1 81.250 (81.250)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.030)	Loss 0.4462 (0.4103)	Prec@1 85.938 (84.524)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.013 (0.022)	Loss 0.4294 (0.4124)	Prec@1 81.250 (84.337)	Prec@5 100.000 (99.924)
Test: [7/40] Acc 84.100
epoch: 8
Train: [0/422]	Time 0.378 (0.378)	Loss 0.4345 (0.4345)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.050 (0.058)	Loss 0.3254 (0.4033)	Prec@1 87.500 (85.784)	Prec@5 100.000 (99.724)
Train: [100/422]	Time 0.058 (0.051)	Loss 0.4919 (0.4095)	Prec@1 75.000 (85.391)	Prec@5 100.000 (99.703)
Train: [150/422]	Time 0.065 (0.058)	Loss 0.3446 (0.4154)	Prec@1 92.188 (84.922)	Prec@5 98.438 (99.703)
Train: [200/422]	Time 0.052 (0.064)	Loss 0.4699 (0.4194)	Prec@1 85.938 (84.625)	Prec@5 100.000 (99.656)
Train: [250/422]	Time 0.060 (0.059)	Loss 0.3206 (0.4139)	Prec@1 89.062 (84.812)	Prec@5 98.438 (99.656)
Train: [300/422]	Time 0.057 (0.057)	Loss 0.4263 (0.4156)	Prec@1 81.250 (85.312)	Prec@5 100.000 (99.656)
Train: [350/422]	Time 0.057 (0.057)	Loss 0.3510 (0.4196)	Prec@1 90.625 (85.016)	Prec@5 98.438 (99.703)
Train: [400/422]	Time 0.064 (0.057)	Loss 0.3758 (0.4080)	Prec@1 87.500 (85.031)	Prec@5 98.438 (99.844)
Test: [0/47]	Time 0.358 (0.358)	Loss 0.2674 (0.2674)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.012 (0.029)	Loss 0.3697 (0.3263)	Prec@1 84.375 (88.095)	Prec@5 100.000 (99.926)
Test: [40/47]	Time 0.012 (0.021)	Loss 0.3476 (0.3289)	Prec@1 89.062 (88.110)	Prec@5 100.000 (99.848)
Test: [8/40] Acc 87.833
epoch: 9
Train: [0/422]	Time 0.374 (0.374)	Loss 0.5429 (0.5429)	Prec@1 79.688 (79.688)	Prec@5 98.438 (98.438)
Train: [50/422]	Time 0.056 (0.064)	Loss 0.3641 (0.3945)	Prec@1 79.688 (85.417)	Prec@5 100.000 (99.663)
Train: [100/422]	Time 0.060 (0.058)	Loss 0.2859 (0.3973)	Prec@1 92.188 (85.406)	Prec@5 100.000 (99.703)
Train: [150/422]	Time 0.060 (0.060)	Loss 0.4529 (0.4094)	Prec@1 81.250 (85.141)	Prec@5 100.000 (99.750)
Train: [200/422]	Time 0.068 (0.061)	Loss 0.3906 (0.4070)	Prec@1 87.500 (85.219)	Prec@5 100.000 (99.750)
Train: [250/422]	Time 0.056 (0.062)	Loss 0.1986 (0.4055)	Prec@1 90.625 (85.094)	Prec@5 100.000 (99.688)
Train: [300/422]	Time 0.061 (0.062)	Loss 0.3395 (0.3963)	Prec@1 85.938 (85.500)	Prec@5 100.000 (99.766)
Train: [350/422]	Time 0.062 (0.061)	Loss 0.5413 (0.4024)	Prec@1 81.250 (85.750)	Prec@5 100.000 (99.703)
Train: [400/422]	Time 0.060 (0.060)	Loss 0.2834 (0.3993)	Prec@1 89.062 (86.109)	Prec@5 100.000 (99.578)
Test: [0/47]	Time 0.410 (0.410)	Loss 0.3353 (0.3353)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.036)	Loss 0.4008 (0.3366)	Prec@1 87.500 (87.426)	Prec@5 100.000 (99.926)
Test: [40/47]	Time 0.015 (0.025)	Loss 0.3732 (0.3460)	Prec@1 87.500 (87.500)	Prec@5 100.000 (99.886)
Test: [9/40] Acc 87.100
epoch: 10
Train: [0/422]	Time 0.388 (0.388)	Loss 0.3100 (0.3100)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.072 (0.067)	Loss 0.4347 (0.3828)	Prec@1 78.125 (86.734)	Prec@5 100.000 (99.663)
Train: [100/422]	Time 0.057 (0.062)	Loss 0.3697 (0.3885)	Prec@1 85.938 (86.531)	Prec@5 100.000 (99.703)
Train: [150/422]	Time 0.060 (0.062)	Loss 0.3221 (0.3863)	Prec@1 87.500 (86.188)	Prec@5 100.000 (99.750)
Train: [200/422]	Time 0.062 (0.063)	Loss 0.3923 (0.3903)	Prec@1 85.938 (85.641)	Prec@5 100.000 (99.766)
Train: [250/422]	Time 0.056 (0.061)	Loss 0.3174 (0.3942)	Prec@1 90.625 (85.406)	Prec@5 100.000 (99.766)
Train: [300/422]	Time 0.058 (0.060)	Loss 0.2916 (0.3837)	Prec@1 87.500 (86.203)	Prec@5 100.000 (99.750)
Train: [350/422]	Time 0.063 (0.058)	Loss 0.3106 (0.3818)	Prec@1 89.062 (86.609)	Prec@5 100.000 (99.750)
Train: [400/422]	Time 0.069 (0.061)	Loss 0.2948 (0.3803)	Prec@1 85.938 (86.281)	Prec@5 100.000 (99.750)
Test: [0/47]	Time 0.397 (0.397)	Loss 0.3191 (0.3191)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.034)	Loss 0.3235 (0.3420)	Prec@1 92.188 (88.244)	Prec@5 100.000 (99.926)
Test: [40/47]	Time 0.014 (0.024)	Loss 0.3668 (0.3422)	Prec@1 84.375 (88.338)	Prec@5 100.000 (99.848)
Test: [10/40] Acc 88.133
epoch: 11
Train: [0/422]	Time 0.426 (0.426)	Loss 0.3199 (0.3199)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.061 (0.067)	Loss 0.2998 (0.3496)	Prec@1 87.500 (87.163)	Prec@5 100.000 (99.847)
Train: [100/422]	Time 0.056 (0.059)	Loss 0.3648 (0.3627)	Prec@1 87.500 (86.516)	Prec@5 100.000 (99.844)
Train: [150/422]	Time 0.060 (0.059)	Loss 0.3725 (0.3718)	Prec@1 81.250 (86.203)	Prec@5 100.000 (99.828)
Train: [200/422]	Time 0.064 (0.060)	Loss 0.2771 (0.3783)	Prec@1 92.188 (86.031)	Prec@5 100.000 (99.812)
Train: [250/422]	Time 0.062 (0.062)	Loss 0.3923 (0.3811)	Prec@1 78.125 (86.203)	Prec@5 100.000 (99.781)
Train: [300/422]	Time 0.065 (0.063)	Loss 0.3098 (0.3761)	Prec@1 89.062 (86.406)	Prec@5 100.000 (99.797)
Train: [350/422]	Time 0.066 (0.066)	Loss 0.4824 (0.3810)	Prec@1 84.375 (85.984)	Prec@5 100.000 (99.781)
Train: [400/422]	Time 0.065 (0.069)	Loss 0.2828 (0.3823)	Prec@1 93.750 (86.031)	Prec@5 100.000 (99.703)
Test: [0/47]	Time 0.324 (0.324)	Loss 0.3118 (0.3118)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.012 (0.028)	Loss 0.3674 (0.3237)	Prec@1 85.938 (87.798)	Prec@5 100.000 (99.926)
Test: [40/47]	Time 0.019 (0.022)	Loss 0.2044 (0.3123)	Prec@1 93.750 (88.567)	Prec@5 100.000 (99.809)
Test: [11/40] Acc 88.333
epoch: 12
Train: [0/422]	Time 0.373 (0.373)	Loss 0.3821 (0.3821)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.055 (0.064)	Loss 0.4065 (0.3734)	Prec@1 82.812 (86.703)	Prec@5 100.000 (99.847)
Train: [100/422]	Time 0.055 (0.057)	Loss 0.3102 (0.3794)	Prec@1 92.188 (86.484)	Prec@5 100.000 (99.750)
Train: [150/422]	Time 0.060 (0.057)	Loss 0.2366 (0.3598)	Prec@1 92.188 (86.938)	Prec@5 100.000 (99.766)
Train: [200/422]	Time 0.056 (0.057)	Loss 0.5279 (0.3531)	Prec@1 82.812 (86.922)	Prec@5 100.000 (99.797)
Train: [250/422]	Time 0.052 (0.057)	Loss 0.2609 (0.3630)	Prec@1 90.625 (86.688)	Prec@5 100.000 (99.781)
Train: [300/422]	Time 0.059 (0.058)	Loss 0.4177 (0.3659)	Prec@1 85.938 (86.859)	Prec@5 98.438 (99.859)
Train: [350/422]	Time 0.058 (0.059)	Loss 0.2438 (0.3664)	Prec@1 89.062 (86.641)	Prec@5 100.000 (99.922)
Train: [400/422]	Time 0.055 (0.059)	Loss 0.3755 (0.3543)	Prec@1 87.500 (87.016)	Prec@5 98.438 (99.828)
Test: [0/47]	Time 0.333 (0.333)	Loss 0.2962 (0.2962)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.030)	Loss 0.3132 (0.3228)	Prec@1 87.500 (87.574)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.017 (0.023)	Loss 0.2796 (0.3156)	Prec@1 90.625 (88.453)	Prec@5 100.000 (99.886)
Test: [12/40] Acc 88.267
epoch: 13
Train: [0/422]	Time 0.387 (0.387)	Loss 0.2794 (0.2794)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.050 (0.061)	Loss 0.3436 (0.3603)	Prec@1 87.500 (86.489)	Prec@5 100.000 (99.816)
Train: [100/422]	Time 0.050 (0.052)	Loss 0.2528 (0.3586)	Prec@1 92.188 (86.953)	Prec@5 100.000 (99.750)
Train: [150/422]	Time 0.049 (0.051)	Loss 0.3823 (0.3550)	Prec@1 85.938 (87.344)	Prec@5 100.000 (99.734)
Train: [200/422]	Time 0.069 (0.055)	Loss 0.2844 (0.3583)	Prec@1 87.500 (87.047)	Prec@5 100.000 (99.844)
Train: [250/422]	Time 0.068 (0.063)	Loss 0.3686 (0.3538)	Prec@1 90.625 (87.094)	Prec@5 98.438 (99.812)
Train: [300/422]	Time 0.056 (0.067)	Loss 0.5804 (0.3469)	Prec@1 78.125 (87.453)	Prec@5 95.312 (99.688)
Train: [350/422]	Time 0.060 (0.064)	Loss 0.2592 (0.3547)	Prec@1 92.188 (87.266)	Prec@5 100.000 (99.656)
Train: [400/422]	Time 0.062 (0.062)	Loss 0.4304 (0.3639)	Prec@1 84.375 (86.938)	Prec@5 100.000 (99.766)
Test: [0/47]	Time 0.383 (0.383)	Loss 0.2227 (0.2227)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.033)	Loss 0.3573 (0.2935)	Prec@1 82.812 (88.839)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.023)	Loss 0.2811 (0.2963)	Prec@1 92.188 (89.253)	Prec@5 100.000 (99.924)
Test: [13/40] Acc 89.200
epoch: 14
Train: [0/422]	Time 0.448 (0.448)	Loss 0.4026 (0.4026)	Prec@1 85.938 (85.938)	Prec@5 98.438 (98.438)
Train: [50/422]	Time 0.069 (0.062)	Loss 0.3609 (0.3493)	Prec@1 85.938 (87.010)	Prec@5 100.000 (99.786)
Train: [100/422]	Time 0.056 (0.058)	Loss 0.1857 (0.3468)	Prec@1 93.750 (87.141)	Prec@5 100.000 (99.812)
Train: [150/422]	Time 0.067 (0.060)	Loss 0.3431 (0.3371)	Prec@1 85.938 (87.594)	Prec@5 100.000 (99.844)
Train: [200/422]	Time 0.065 (0.063)	Loss 0.4242 (0.3332)	Prec@1 85.938 (88.047)	Prec@5 98.438 (99.859)
Train: [250/422]	Time 0.057 (0.064)	Loss 0.3574 (0.3375)	Prec@1 87.500 (87.672)	Prec@5 100.000 (99.844)
Train: [300/422]	Time 0.068 (0.064)	Loss 0.4417 (0.3505)	Prec@1 85.938 (86.922)	Prec@5 100.000 (99.844)
Train: [350/422]	Time 0.068 (0.063)	Loss 0.4715 (0.3614)	Prec@1 84.375 (86.688)	Prec@5 100.000 (99.812)
Train: [400/422]	Time 0.056 (0.064)	Loss 0.6447 (0.3556)	Prec@1 81.250 (86.984)	Prec@5 98.438 (99.828)
Test: [0/47]	Time 0.400 (0.400)	Loss 0.2982 (0.2982)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.018 (0.033)	Loss 0.3456 (0.3029)	Prec@1 82.812 (88.690)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.016 (0.025)	Loss 0.2554 (0.2996)	Prec@1 89.062 (89.329)	Prec@5 100.000 (99.962)
Test: [14/40] Acc 89.100
epoch: 15
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
learning rate: 5.00e-04
Train: [0/422]	Time 0.411 (0.411)	Loss 0.3397 (0.3397)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.059 (0.065)	Loss 0.3090 (0.3365)	Prec@1 89.062 (88.235)	Prec@5 100.000 (99.877)
Train: [100/422]	Time 0.057 (0.059)	Loss 0.2665 (0.3289)	Prec@1 89.062 (88.453)	Prec@5 100.000 (99.859)
Train: [150/422]	Time 0.056 (0.058)	Loss 0.2253 (0.3148)	Prec@1 92.188 (88.703)	Prec@5 100.000 (99.859)
Train: [200/422]	Time 0.058 (0.058)	Loss 0.4036 (0.3044)	Prec@1 81.250 (89.016)	Prec@5 100.000 (99.875)
Train: [250/422]	Time 0.054 (0.059)	Loss 0.2762 (0.2966)	Prec@1 87.500 (89.312)	Prec@5 100.000 (99.844)
Train: [300/422]	Time 0.055 (0.058)	Loss 0.4111 (0.2890)	Prec@1 85.938 (89.469)	Prec@5 100.000 (99.766)
Train: [350/422]	Time 0.061 (0.058)	Loss 0.2065 (0.3036)	Prec@1 93.750 (88.969)	Prec@5 100.000 (99.766)
Train: [400/422]	Time 0.066 (0.060)	Loss 0.3375 (0.3161)	Prec@1 87.500 (88.391)	Prec@5 100.000 (99.812)
Test: [0/47]	Time 0.380 (0.380)	Loss 0.2886 (0.2886)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.016 (0.035)	Loss 0.2939 (0.2752)	Prec@1 85.938 (89.807)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.015 (0.025)	Loss 0.2753 (0.2709)	Prec@1 89.062 (90.130)	Prec@5 100.000 (99.962)
Test: [15/40] Acc 90.000
epoch: 16
Train: [0/422]	Time 0.412 (0.412)	Loss 0.2996 (0.2996)	Prec@1 89.062 (89.062)	Prec@5 98.438 (98.438)
Train: [50/422]	Time 0.075 (0.069)	Loss 0.4020 (0.2883)	Prec@1 85.938 (89.828)	Prec@5 100.000 (99.877)
Train: [100/422]	Time 0.059 (0.061)	Loss 0.2632 (0.2931)	Prec@1 90.625 (89.547)	Prec@5 100.000 (99.891)
Train: [150/422]	Time 0.053 (0.058)	Loss 0.4446 (0.2995)	Prec@1 81.250 (89.422)	Prec@5 100.000 (99.906)
Train: [200/422]	Time 0.055 (0.059)	Loss 0.3079 (0.3036)	Prec@1 85.938 (89.062)	Prec@5 100.000 (99.891)
Train: [250/422]	Time 0.056 (0.059)	Loss 0.2299 (0.2907)	Prec@1 90.625 (89.219)	Prec@5 100.000 (99.891)
Train: [300/422]	Time 0.055 (0.058)	Loss 0.3180 (0.2946)	Prec@1 87.500 (89.328)	Prec@5 100.000 (99.938)
Train: [350/422]	Time 0.060 (0.058)	Loss 0.3309 (0.3067)	Prec@1 85.938 (89.203)	Prec@5 100.000 (99.875)
Train: [400/422]	Time 0.055 (0.058)	Loss 0.1936 (0.3003)	Prec@1 93.750 (89.438)	Prec@5 100.000 (99.766)
Test: [0/47]	Time 0.377 (0.377)	Loss 0.2631 (0.2631)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.033)	Loss 0.2934 (0.2730)	Prec@1 89.062 (90.327)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.013 (0.023)	Loss 0.2601 (0.2653)	Prec@1 92.188 (90.587)	Prec@5 100.000 (99.962)
Test: [16/40] Acc 90.267
epoch: 17
Train: [0/422]	Time 0.408 (0.408)	Loss 0.3051 (0.3051)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.055 (0.064)	Loss 0.1522 (0.3055)	Prec@1 95.312 (89.185)	Prec@5 100.000 (99.908)
Train: [100/422]	Time 0.063 (0.058)	Loss 0.3037 (0.2941)	Prec@1 89.062 (89.672)	Prec@5 100.000 (99.875)
Train: [150/422]	Time 0.064 (0.060)	Loss 0.2621 (0.2845)	Prec@1 92.188 (89.688)	Prec@5 98.438 (99.844)
Train: [200/422]	Time 0.060 (0.059)	Loss 0.2132 (0.2830)	Prec@1 90.625 (89.359)	Prec@5 100.000 (99.875)
Train: [250/422]	Time 0.058 (0.059)	Loss 0.2295 (0.2825)	Prec@1 90.625 (89.609)	Prec@5 100.000 (99.906)
Train: [300/422]	Time 0.057 (0.058)	Loss 0.1500 (0.2849)	Prec@1 95.312 (89.609)	Prec@5 100.000 (99.891)
Train: [350/422]	Time 0.055 (0.060)	Loss 0.2723 (0.2923)	Prec@1 90.625 (89.453)	Prec@5 100.000 (99.828)
Train: [400/422]	Time 0.058 (0.061)	Loss 0.3560 (0.2954)	Prec@1 85.938 (89.391)	Prec@5 100.000 (99.766)
Test: [0/47]	Time 0.348 (0.348)	Loss 0.2621 (0.2621)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.031)	Loss 0.2911 (0.2723)	Prec@1 87.500 (90.030)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.015 (0.024)	Loss 0.2530 (0.2637)	Prec@1 92.188 (90.549)	Prec@5 100.000 (99.962)
Test: [17/40] Acc 90.533
epoch: 18
Train: [0/422]	Time 0.374 (0.374)	Loss 0.1748 (0.1748)	Prec@1 96.875 (96.875)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.056 (0.059)	Loss 0.1817 (0.2951)	Prec@1 93.750 (89.553)	Prec@5 100.000 (99.877)
Train: [100/422]	Time 0.048 (0.053)	Loss 0.2163 (0.2958)	Prec@1 90.625 (89.719)	Prec@5 100.000 (99.859)
Train: [150/422]	Time 0.050 (0.053)	Loss 0.3691 (0.2910)	Prec@1 87.500 (89.906)	Prec@5 100.000 (99.875)
Train: [200/422]	Time 0.052 (0.054)	Loss 0.3366 (0.2862)	Prec@1 87.500 (89.594)	Prec@5 100.000 (99.875)
Train: [250/422]	Time 0.071 (0.059)	Loss 0.3524 (0.2949)	Prec@1 89.062 (88.984)	Prec@5 98.438 (99.859)
Train: [300/422]	Time 0.057 (0.062)	Loss 0.1992 (0.2956)	Prec@1 92.188 (89.141)	Prec@5 100.000 (99.859)
Train: [350/422]	Time 0.060 (0.059)	Loss 0.2611 (0.2726)	Prec@1 92.188 (90.031)	Prec@5 100.000 (99.922)
Train: [400/422]	Time 0.061 (0.059)	Loss 0.2600 (0.2710)	Prec@1 89.062 (90.141)	Prec@5 100.000 (99.953)
Test: [0/47]	Time 0.335 (0.335)	Loss 0.2695 (0.2695)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.017 (0.031)	Loss 0.2739 (0.2688)	Prec@1 89.062 (90.476)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.022)	Loss 0.2770 (0.2615)	Prec@1 92.188 (90.701)	Prec@5 100.000 (99.962)
Test: [18/40] Acc 90.567
epoch: 19
Train: [0/422]	Time 0.373 (0.373)	Loss 0.2704 (0.2704)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.055 (0.064)	Loss 0.1871 (0.2678)	Prec@1 92.188 (89.798)	Prec@5 100.000 (99.908)
Train: [100/422]	Time 0.051 (0.055)	Loss 0.1357 (0.2748)	Prec@1 93.750 (89.844)	Prec@5 100.000 (99.906)
Train: [150/422]	Time 0.072 (0.056)	Loss 0.3128 (0.2884)	Prec@1 90.625 (89.641)	Prec@5 100.000 (99.844)
Train: [200/422]	Time 0.061 (0.063)	Loss 0.3193 (0.2886)	Prec@1 89.062 (89.328)	Prec@5 100.000 (99.828)
Train: [250/422]	Time 0.065 (0.063)	Loss 0.2581 (0.2819)	Prec@1 93.750 (89.188)	Prec@5 100.000 (99.812)
Train: [300/422]	Time 0.070 (0.064)	Loss 0.2889 (0.2841)	Prec@1 89.062 (89.672)	Prec@5 100.000 (99.812)
Train: [350/422]	Time 0.055 (0.064)	Loss 0.3593 (0.2894)	Prec@1 87.500 (89.969)	Prec@5 100.000 (99.891)
Train: [400/422]	Time 0.059 (0.061)	Loss 0.2628 (0.2804)	Prec@1 87.500 (89.891)	Prec@5 100.000 (99.922)
Test: [0/47]	Time 0.345 (0.345)	Loss 0.2887 (0.2887)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.030)	Loss 0.2593 (0.2761)	Prec@1 90.625 (90.476)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.022)	Loss 0.2454 (0.2659)	Prec@1 93.750 (90.892)	Prec@5 100.000 (99.962)
Test: [19/40] Acc 90.733
epoch: 20
Train: [0/422]	Time 0.369 (0.369)	Loss 0.2666 (0.2666)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.058 (0.065)	Loss 0.1647 (0.2887)	Prec@1 93.750 (89.675)	Prec@5 100.000 (99.816)
Train: [100/422]	Time 0.051 (0.056)	Loss 0.2523 (0.2853)	Prec@1 92.188 (90.016)	Prec@5 100.000 (99.844)
Train: [150/422]	Time 0.052 (0.052)	Loss 0.2957 (0.2796)	Prec@1 84.375 (90.250)	Prec@5 100.000 (99.875)
Train: [200/422]	Time 0.050 (0.051)	Loss 0.4623 (0.2695)	Prec@1 87.500 (90.422)	Prec@5 96.875 (99.844)
Train: [250/422]	Time 0.051 (0.051)	Loss 0.3283 (0.2634)	Prec@1 85.938 (90.516)	Prec@5 100.000 (99.875)
Train: [300/422]	Time 0.050 (0.051)	Loss 0.3167 (0.2707)	Prec@1 87.500 (90.438)	Prec@5 100.000 (99.938)
Train: [350/422]	Time 0.050 (0.051)	Loss 0.1832 (0.2771)	Prec@1 92.188 (90.156)	Prec@5 100.000 (99.922)
Train: [400/422]	Time 0.059 (0.053)	Loss 0.2548 (0.2836)	Prec@1 90.625 (89.672)	Prec@5 100.000 (99.891)
Test: [0/47]	Time 0.370 (0.370)	Loss 0.2511 (0.2511)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.032)	Loss 0.2686 (0.2652)	Prec@1 87.500 (90.699)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.013 (0.023)	Loss 0.2457 (0.2587)	Prec@1 92.188 (91.082)	Prec@5 100.000 (99.962)
Test: [20/40] Acc 90.967
epoch: 21
Train: [0/422]	Time 0.368 (0.368)	Loss 0.2947 (0.2947)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.057 (0.062)	Loss 0.2756 (0.2806)	Prec@1 87.500 (89.308)	Prec@5 100.000 (99.847)
Train: [100/422]	Time 0.058 (0.057)	Loss 0.2773 (0.2781)	Prec@1 93.750 (89.703)	Prec@5 100.000 (99.906)
Train: [150/422]	Time 0.062 (0.058)	Loss 0.3331 (0.2837)	Prec@1 90.625 (89.906)	Prec@5 100.000 (99.922)
Train: [200/422]	Time 0.057 (0.059)	Loss 0.4203 (0.2835)	Prec@1 87.500 (89.734)	Prec@5 100.000 (99.906)
Train: [250/422]	Time 0.056 (0.058)	Loss 0.1389 (0.2836)	Prec@1 96.875 (89.312)	Prec@5 100.000 (99.938)
Train: [300/422]	Time 0.070 (0.056)	Loss 0.2902 (0.2738)	Prec@1 87.500 (89.781)	Prec@5 100.000 (99.906)
Train: [350/422]	Time 0.057 (0.058)	Loss 0.2933 (0.2579)	Prec@1 90.625 (90.703)	Prec@5 100.000 (99.922)
Train: [400/422]	Time 0.057 (0.058)	Loss 0.3163 (0.2676)	Prec@1 87.500 (90.516)	Prec@5 100.000 (99.938)
Test: [0/47]	Time 0.348 (0.348)	Loss 0.2770 (0.2770)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.032)	Loss 0.2591 (0.2717)	Prec@1 90.625 (90.402)	Prec@5 100.000 (99.926)
Test: [40/47]	Time 0.011 (0.023)	Loss 0.2432 (0.2623)	Prec@1 92.188 (90.816)	Prec@5 100.000 (99.924)
Test: [21/40] Acc 90.700
epoch: 22
Train: [0/422]	Time 0.402 (0.402)	Loss 0.3950 (0.3950)	Prec@1 90.625 (90.625)	Prec@5 96.875 (96.875)
Train: [50/422]	Time 0.055 (0.062)	Loss 0.2639 (0.2581)	Prec@1 90.625 (90.196)	Prec@5 98.438 (99.816)
Train: [100/422]	Time 0.057 (0.056)	Loss 0.3460 (0.2712)	Prec@1 89.062 (90.031)	Prec@5 100.000 (99.859)
Train: [150/422]	Time 0.054 (0.057)	Loss 0.2262 (0.2789)	Prec@1 92.188 (90.078)	Prec@5 100.000 (99.859)
Train: [200/422]	Time 0.056 (0.057)	Loss 0.2164 (0.2727)	Prec@1 90.625 (89.859)	Prec@5 100.000 (99.875)
Train: [250/422]	Time 0.051 (0.057)	Loss 0.1522 (0.2692)	Prec@1 96.875 (89.953)	Prec@5 100.000 (99.922)
Train: [300/422]	Time 0.056 (0.055)	Loss 0.2234 (0.2764)	Prec@1 93.750 (90.188)	Prec@5 100.000 (99.922)
Train: [350/422]	Time 0.068 (0.056)	Loss 0.2215 (0.2843)	Prec@1 93.750 (90.031)	Prec@5 100.000 (99.859)
Train: [400/422]	Time 0.050 (0.056)	Loss 0.2416 (0.2726)	Prec@1 92.188 (90.328)	Prec@5 100.000 (99.844)
Test: [0/47]	Time 0.345 (0.345)	Loss 0.2530 (0.2530)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.030)	Loss 0.2547 (0.2731)	Prec@1 90.625 (90.030)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.022)	Loss 0.2599 (0.2603)	Prec@1 92.188 (90.587)	Prec@5 100.000 (99.962)
Test: [22/40] Acc 90.433
epoch: 23
Train: [0/422]	Time 0.363 (0.363)	Loss 0.1659 (0.1659)	Prec@1 96.875 (96.875)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.058 (0.064)	Loss 0.2772 (0.2673)	Prec@1 92.188 (90.717)	Prec@5 100.000 (99.847)
Train: [100/422]	Time 0.068 (0.063)	Loss 0.2217 (0.2579)	Prec@1 89.062 (90.938)	Prec@5 100.000 (99.906)
Train: [150/422]	Time 0.055 (0.064)	Loss 0.3371 (0.2530)	Prec@1 92.188 (90.766)	Prec@5 100.000 (99.953)
Train: [200/422]	Time 0.053 (0.058)	Loss 0.1841 (0.2642)	Prec@1 95.312 (90.203)	Prec@5 100.000 (99.922)
Train: [250/422]	Time 0.052 (0.056)	Loss 0.1268 (0.2741)	Prec@1 96.875 (90.078)	Prec@5 100.000 (99.891)
Train: [300/422]	Time 0.060 (0.058)	Loss 0.1748 (0.2808)	Prec@1 93.750 (89.828)	Prec@5 100.000 (99.922)
Train: [350/422]	Time 0.060 (0.060)	Loss 0.2738 (0.2837)	Prec@1 92.188 (89.656)	Prec@5 100.000 (99.922)
Train: [400/422]	Time 0.058 (0.059)	Loss 0.2850 (0.2776)	Prec@1 87.500 (90.141)	Prec@5 100.000 (99.891)
Test: [0/47]	Time 0.353 (0.353)	Loss 0.2431 (0.2431)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.029)	Loss 0.2669 (0.2684)	Prec@1 90.625 (90.327)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.021)	Loss 0.2416 (0.2611)	Prec@1 92.188 (90.854)	Prec@5 100.000 (99.962)
Test: [23/40] Acc 90.700
epoch: 24
Train: [0/422]	Time 0.459 (0.459)	Loss 0.1488 (0.1488)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.061 (0.068)	Loss 0.1596 (0.2552)	Prec@1 96.875 (91.023)	Prec@5 100.000 (99.939)
Train: [100/422]	Time 0.066 (0.059)	Loss 0.1499 (0.2653)	Prec@1 96.875 (90.453)	Prec@5 100.000 (99.938)
Train: [150/422]	Time 0.056 (0.064)	Loss 0.4050 (0.2723)	Prec@1 87.500 (90.016)	Prec@5 100.000 (99.891)
Train: [200/422]	Time 0.065 (0.064)	Loss 0.2469 (0.2665)	Prec@1 93.750 (90.141)	Prec@5 100.000 (99.859)
Train: [250/422]	Time 0.055 (0.058)	Loss 0.2352 (0.2722)	Prec@1 87.500 (89.922)	Prec@5 100.000 (99.906)
Train: [300/422]	Time 0.055 (0.056)	Loss 0.1338 (0.2751)	Prec@1 95.312 (90.031)	Prec@5 100.000 (99.906)
Train: [350/422]	Time 0.058 (0.056)	Loss 0.2538 (0.2607)	Prec@1 92.188 (90.781)	Prec@5 100.000 (99.906)
Train: [400/422]	Time 0.057 (0.057)	Loss 0.3690 (0.2620)	Prec@1 85.938 (90.766)	Prec@5 100.000 (99.859)
Test: [0/47]	Time 0.346 (0.346)	Loss 0.2451 (0.2451)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.017 (0.033)	Loss 0.2522 (0.2681)	Prec@1 89.062 (90.774)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.014 (0.024)	Loss 0.2199 (0.2607)	Prec@1 93.750 (90.930)	Prec@5 100.000 (99.962)
Test: [24/40] Acc 90.833
epoch: 25
Train: [0/422]	Time 0.368 (0.368)	Loss 0.1960 (0.1960)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.050 (0.061)	Loss 0.2158 (0.2825)	Prec@1 93.750 (90.043)	Prec@5 100.000 (99.694)
Train: [100/422]	Time 0.049 (0.053)	Loss 0.2938 (0.2811)	Prec@1 87.500 (89.688)	Prec@5 100.000 (99.797)
Train: [150/422]	Time 0.056 (0.053)	Loss 0.2846 (0.2649)	Prec@1 90.625 (89.953)	Prec@5 100.000 (99.938)
Train: [200/422]	Time 0.046 (0.054)	Loss 0.2615 (0.2616)	Prec@1 92.188 (89.922)	Prec@5 100.000 (99.938)
Train: [250/422]	Time 0.066 (0.053)	Loss 0.3123 (0.2773)	Prec@1 90.625 (89.828)	Prec@5 100.000 (99.797)
Train: [300/422]	Time 0.056 (0.057)	Loss 0.2752 (0.2820)	Prec@1 87.500 (90.312)	Prec@5 100.000 (99.797)
Train: [350/422]	Time 0.058 (0.057)	Loss 0.3349 (0.2820)	Prec@1 89.062 (89.922)	Prec@5 100.000 (99.891)
Train: [400/422]	Time 0.056 (0.055)	Loss 0.2823 (0.2698)	Prec@1 90.625 (90.172)	Prec@5 100.000 (99.922)
Test: [0/47]	Time 0.352 (0.352)	Loss 0.2544 (0.2544)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.031)	Loss 0.2463 (0.2720)	Prec@1 92.188 (90.104)	Prec@5 100.000 (99.926)
Test: [40/47]	Time 0.012 (0.022)	Loss 0.2266 (0.2626)	Prec@1 93.750 (90.549)	Prec@5 100.000 (99.924)
Test: [25/40] Acc 90.433
epoch: 26
Train: [0/422]	Time 0.377 (0.377)	Loss 0.2909 (0.2909)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.063 (0.067)	Loss 0.5040 (0.2405)	Prec@1 81.250 (91.238)	Prec@5 100.000 (99.847)
Train: [100/422]	Time 0.058 (0.062)	Loss 0.3820 (0.2551)	Prec@1 85.938 (90.562)	Prec@5 98.438 (99.828)
Train: [150/422]	Time 0.063 (0.061)	Loss 0.3188 (0.2698)	Prec@1 92.188 (90.109)	Prec@5 100.000 (99.906)
Train: [200/422]	Time 0.055 (0.059)	Loss 0.3152 (0.2700)	Prec@1 90.625 (90.375)	Prec@5 100.000 (99.969)
Train: [250/422]	Time 0.059 (0.057)	Loss 0.2631 (0.2687)	Prec@1 92.188 (90.469)	Prec@5 100.000 (99.891)
Train: [300/422]	Time 0.056 (0.057)	Loss 0.1749 (0.2687)	Prec@1 95.312 (90.391)	Prec@5 100.000 (99.828)
Train: [350/422]	Time 0.059 (0.058)	Loss 0.2312 (0.2761)	Prec@1 93.750 (90.297)	Prec@5 100.000 (99.797)
Train: [400/422]	Time 0.056 (0.058)	Loss 0.2485 (0.2691)	Prec@1 89.062 (90.688)	Prec@5 100.000 (99.859)
Test: [0/47]	Time 0.370 (0.370)	Loss 0.2516 (0.2516)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.033)	Loss 0.2640 (0.2591)	Prec@1 90.625 (90.327)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.014 (0.024)	Loss 0.2368 (0.2584)	Prec@1 92.188 (90.663)	Prec@5 100.000 (99.962)
Test: [26/40] Acc 90.433
epoch: 27
Train: [0/422]	Time 0.389 (0.389)	Loss 0.1671 (0.1671)	Prec@1 95.312 (95.312)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.062 (0.070)	Loss 0.2849 (0.2657)	Prec@1 87.500 (90.748)	Prec@5 100.000 (99.877)
Train: [100/422]	Time 0.057 (0.060)	Loss 0.1424 (0.2696)	Prec@1 95.312 (90.156)	Prec@5 100.000 (99.844)
Train: [150/422]	Time 0.057 (0.057)	Loss 0.2770 (0.2741)	Prec@1 90.625 (89.406)	Prec@5 100.000 (99.844)
Train: [200/422]	Time 0.058 (0.056)	Loss 0.1416 (0.2660)	Prec@1 93.750 (90.203)	Prec@5 100.000 (99.844)
Train: [250/422]	Time 0.056 (0.056)	Loss 0.2772 (0.2602)	Prec@1 85.938 (90.734)	Prec@5 100.000 (99.875)
Train: [300/422]	Time 0.054 (0.056)	Loss 0.1904 (0.2657)	Prec@1 93.750 (90.641)	Prec@5 100.000 (99.891)
Train: [350/422]	Time 0.064 (0.057)	Loss 0.2909 (0.2589)	Prec@1 87.500 (90.922)	Prec@5 100.000 (99.875)
Train: [400/422]	Time 0.063 (0.063)	Loss 0.1777 (0.2495)	Prec@1 95.312 (91.047)	Prec@5 100.000 (99.891)
Test: [0/47]	Time 0.375 (0.375)	Loss 0.2291 (0.2291)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.031)	Loss 0.2563 (0.2597)	Prec@1 89.062 (90.327)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.023)	Loss 0.2465 (0.2550)	Prec@1 93.750 (90.892)	Prec@5 100.000 (99.924)
Test: [27/40] Acc 90.767
epoch: 28
Train: [0/422]	Time 0.377 (0.377)	Loss 0.4230 (0.4230)	Prec@1 82.812 (82.812)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.050 (0.057)	Loss 0.2352 (0.2498)	Prec@1 92.188 (90.839)	Prec@5 100.000 (99.847)
Train: [100/422]	Time 0.050 (0.050)	Loss 0.2360 (0.2533)	Prec@1 92.188 (90.859)	Prec@5 100.000 (99.891)
Train: [150/422]	Time 0.064 (0.054)	Loss 0.3863 (0.2675)	Prec@1 82.812 (90.453)	Prec@5 100.000 (99.891)
Train: [200/422]	Time 0.064 (0.063)	Loss 0.3760 (0.2703)	Prec@1 87.500 (90.469)	Prec@5 100.000 (99.859)
Train: [250/422]	Time 0.058 (0.063)	Loss 0.1537 (0.2657)	Prec@1 90.625 (90.516)	Prec@5 100.000 (99.875)
Train: [300/422]	Time 0.057 (0.059)	Loss 0.1715 (0.2694)	Prec@1 93.750 (90.000)	Prec@5 100.000 (99.859)
Train: [350/422]	Time 0.055 (0.058)	Loss 0.2990 (0.2683)	Prec@1 89.062 (89.938)	Prec@5 100.000 (99.844)
Train: [400/422]	Time 0.051 (0.056)	Loss 0.2887 (0.2575)	Prec@1 87.500 (90.641)	Prec@5 100.000 (99.891)
Test: [0/47]	Time 0.350 (0.350)	Loss 0.2511 (0.2511)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.031)	Loss 0.2711 (0.2631)	Prec@1 87.500 (90.253)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.015 (0.023)	Loss 0.2274 (0.2576)	Prec@1 93.750 (90.701)	Prec@5 100.000 (99.962)
Test: [28/40] Acc 90.633
epoch: 29
Train: [0/422]	Time 0.403 (0.403)	Loss 0.0960 (0.0960)	Prec@1 96.875 (96.875)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.055 (0.063)	Loss 0.2562 (0.2551)	Prec@1 90.625 (91.146)	Prec@5 100.000 (99.908)
Train: [100/422]	Time 0.058 (0.056)	Loss 0.3139 (0.2635)	Prec@1 89.062 (90.797)	Prec@5 100.000 (99.875)
Train: [150/422]	Time 0.054 (0.056)	Loss 0.3385 (0.2639)	Prec@1 89.062 (90.562)	Prec@5 100.000 (99.906)
Train: [200/422]	Time 0.048 (0.054)	Loss 0.1729 (0.2580)	Prec@1 93.750 (90.625)	Prec@5 100.000 (99.938)
Train: [250/422]	Time 0.050 (0.051)	Loss 0.1828 (0.2543)	Prec@1 93.750 (90.594)	Prec@5 100.000 (99.906)
Train: [300/422]	Time 0.053 (0.050)	Loss 0.2329 (0.2542)	Prec@1 95.312 (90.578)	Prec@5 100.000 (99.922)
Train: [350/422]	Time 0.056 (0.054)	Loss 0.1555 (0.2607)	Prec@1 93.750 (90.469)	Prec@5 100.000 (99.922)
Train: [400/422]	Time 0.055 (0.056)	Loss 0.2802 (0.2553)	Prec@1 89.062 (90.578)	Prec@5 100.000 (99.938)
Test: [0/47]	Time 0.363 (0.363)	Loss 0.2439 (0.2439)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.032)	Loss 0.2386 (0.2646)	Prec@1 90.625 (90.476)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.013 (0.024)	Loss 0.2693 (0.2595)	Prec@1 93.750 (90.777)	Prec@5 100.000 (99.962)
Test: [29/40] Acc 90.467
epoch: 30
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
learning rate: 5.00e-05
Train: [0/422]	Time 0.381 (0.381)	Loss 0.1333 (0.1333)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.056 (0.061)	Loss 0.3967 (0.2713)	Prec@1 82.812 (90.319)	Prec@5 100.000 (99.939)
Train: [100/422]	Time 0.051 (0.057)	Loss 0.2515 (0.2692)	Prec@1 92.188 (90.266)	Prec@5 100.000 (99.922)
Train: [150/422]	Time 0.056 (0.056)	Loss 0.2474 (0.2587)	Prec@1 90.625 (90.641)	Prec@5 100.000 (99.891)
Train: [200/422]	Time 0.057 (0.055)	Loss 0.2566 (0.2498)	Prec@1 90.625 (90.859)	Prec@5 100.000 (99.844)
Train: [250/422]	Time 0.071 (0.062)	Loss 0.2843 (0.2463)	Prec@1 90.625 (90.812)	Prec@5 100.000 (99.844)
Train: [300/422]	Time 0.057 (0.067)	Loss 0.1671 (0.2461)	Prec@1 95.312 (91.094)	Prec@5 100.000 (99.875)
Train: [350/422]	Time 0.058 (0.063)	Loss 0.2335 (0.2514)	Prec@1 89.062 (90.969)	Prec@5 100.000 (99.875)
Train: [400/422]	Time 0.056 (0.059)	Loss 0.2548 (0.2519)	Prec@1 90.625 (90.797)	Prec@5 100.000 (99.891)
Test: [0/47]	Time 0.344 (0.344)	Loss 0.2427 (0.2427)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.031)	Loss 0.2407 (0.2623)	Prec@1 90.625 (90.551)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.022)	Loss 0.2573 (0.2554)	Prec@1 93.750 (90.854)	Prec@5 100.000 (99.962)
Test: [30/40] Acc 90.667
epoch: 31
Train: [0/422]	Time 0.403 (0.403)	Loss 0.2257 (0.2257)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.057 (0.060)	Loss 0.2483 (0.2542)	Prec@1 92.188 (90.686)	Prec@5 100.000 (99.939)
Train: [100/422]	Time 0.058 (0.055)	Loss 0.2778 (0.2550)	Prec@1 90.625 (90.688)	Prec@5 100.000 (99.938)
Train: [150/422]	Time 0.064 (0.057)	Loss 0.1800 (0.2460)	Prec@1 92.188 (91.156)	Prec@5 100.000 (99.938)
Train: [200/422]	Time 0.065 (0.062)	Loss 0.2851 (0.2328)	Prec@1 92.188 (91.703)	Prec@5 100.000 (99.938)
Train: [250/422]	Time 0.068 (0.067)	Loss 0.1737 (0.2496)	Prec@1 95.312 (91.328)	Prec@5 100.000 (99.875)
Train: [300/422]	Time 0.059 (0.066)	Loss 0.2922 (0.2550)	Prec@1 85.938 (91.375)	Prec@5 100.000 (99.875)
Train: [350/422]	Time 0.066 (0.063)	Loss 0.2897 (0.2433)	Prec@1 87.500 (91.547)	Prec@5 100.000 (99.938)
Train: [400/422]	Time 0.056 (0.060)	Loss 0.1672 (0.2517)	Prec@1 92.188 (90.672)	Prec@5 100.000 (99.922)
Test: [0/47]	Time 0.360 (0.360)	Loss 0.2462 (0.2462)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.016 (0.034)	Loss 0.2327 (0.2610)	Prec@1 90.625 (90.848)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.014 (0.024)	Loss 0.2524 (0.2552)	Prec@1 93.750 (91.082)	Prec@5 100.000 (99.962)
Test: [31/40] Acc 90.867
epoch: 32
Train: [0/422]	Time 0.381 (0.381)	Loss 0.1615 (0.1615)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.056 (0.064)	Loss 0.2334 (0.2518)	Prec@1 89.062 (90.839)	Prec@5 100.000 (99.939)
Train: [100/422]	Time 0.061 (0.057)	Loss 0.3597 (0.2628)	Prec@1 84.375 (90.672)	Prec@5 100.000 (99.844)
Train: [150/422]	Time 0.057 (0.058)	Loss 0.2709 (0.2545)	Prec@1 89.062 (90.969)	Prec@5 100.000 (99.828)
Train: [200/422]	Time 0.059 (0.059)	Loss 0.3060 (0.2476)	Prec@1 87.500 (90.969)	Prec@5 100.000 (99.922)
Train: [250/422]	Time 0.060 (0.058)	Loss 0.1516 (0.2478)	Prec@1 95.312 (90.969)	Prec@5 100.000 (99.922)
Train: [300/422]	Time 0.053 (0.060)	Loss 0.1959 (0.2500)	Prec@1 89.062 (90.984)	Prec@5 100.000 (99.891)
Train: [350/422]	Time 0.055 (0.062)	Loss 0.2296 (0.2670)	Prec@1 89.062 (90.312)	Prec@5 100.000 (99.891)
Train: [400/422]	Time 0.066 (0.061)	Loss 0.2377 (0.2616)	Prec@1 95.312 (90.547)	Prec@5 100.000 (99.891)
Test: [0/47]	Time 0.348 (0.348)	Loss 0.2473 (0.2473)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.016 (0.031)	Loss 0.2305 (0.2616)	Prec@1 90.625 (90.699)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.022)	Loss 0.2429 (0.2552)	Prec@1 93.750 (91.120)	Prec@5 100.000 (99.962)
Test: [32/40] Acc 90.933
epoch: 33
Train: [0/422]	Time 0.372 (0.372)	Loss 0.2419 (0.2419)	Prec@1 90.625 (90.625)	Prec@5 98.438 (98.438)
Train: [50/422]	Time 0.051 (0.057)	Loss 0.3079 (0.2566)	Prec@1 90.625 (90.472)	Prec@5 98.438 (99.724)
Train: [100/422]	Time 0.059 (0.053)	Loss 0.3079 (0.2548)	Prec@1 89.062 (90.781)	Prec@5 100.000 (99.828)
Train: [150/422]	Time 0.067 (0.062)	Loss 0.1788 (0.2452)	Prec@1 92.188 (91.344)	Prec@5 100.000 (99.891)
Train: [200/422]	Time 0.061 (0.067)	Loss 0.1730 (0.2360)	Prec@1 96.875 (91.812)	Prec@5 100.000 (99.906)
Train: [250/422]	Time 0.056 (0.065)	Loss 0.2789 (0.2471)	Prec@1 87.500 (91.281)	Prec@5 100.000 (99.906)
Train: [300/422]	Time 0.063 (0.060)	Loss 0.3019 (0.2589)	Prec@1 89.062 (90.344)	Prec@5 100.000 (99.922)
Train: [350/422]	Time 0.059 (0.057)	Loss 0.3189 (0.2565)	Prec@1 87.500 (90.531)	Prec@5 100.000 (99.938)
Train: [400/422]	Time 0.061 (0.058)	Loss 0.2490 (0.2594)	Prec@1 90.625 (90.594)	Prec@5 100.000 (99.938)
Test: [0/47]	Time 0.343 (0.343)	Loss 0.2451 (0.2451)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.033)	Loss 0.2465 (0.2643)	Prec@1 89.062 (90.923)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.024)	Loss 0.2410 (0.2561)	Prec@1 93.750 (91.082)	Prec@5 100.000 (99.962)
Test: [33/40] Acc 90.900
epoch: 34
Train: [0/422]	Time 0.377 (0.377)	Loss 0.0912 (0.0912)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.056 (0.052)	Loss 0.2337 (0.2620)	Prec@1 87.500 (90.656)	Prec@5 100.000 (99.877)
Train: [100/422]	Time 0.067 (0.055)	Loss 0.1205 (0.2579)	Prec@1 96.875 (90.750)	Prec@5 100.000 (99.844)
Train: [150/422]	Time 0.064 (0.065)	Loss 0.3056 (0.2569)	Prec@1 85.938 (90.594)	Prec@5 100.000 (99.875)
Train: [200/422]	Time 0.068 (0.065)	Loss 0.3001 (0.2582)	Prec@1 85.938 (90.484)	Prec@5 100.000 (99.938)
Train: [250/422]	Time 0.064 (0.065)	Loss 0.1670 (0.2546)	Prec@1 93.750 (90.969)	Prec@5 100.000 (99.938)
Train: [300/422]	Time 0.049 (0.062)	Loss 0.2569 (0.2475)	Prec@1 92.188 (91.328)	Prec@5 100.000 (99.922)
Train: [350/422]	Time 0.067 (0.059)	Loss 0.2070 (0.2349)	Prec@1 92.188 (91.484)	Prec@5 100.000 (99.906)
Train: [400/422]	Time 0.053 (0.062)	Loss 0.2114 (0.2390)	Prec@1 90.625 (91.328)	Prec@5 100.000 (99.906)
Test: [0/47]	Time 0.354 (0.354)	Loss 0.2454 (0.2454)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.032)	Loss 0.2374 (0.2619)	Prec@1 90.625 (90.848)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.023)	Loss 0.2474 (0.2547)	Prec@1 93.750 (91.044)	Prec@5 100.000 (99.962)
Test: [34/40] Acc 90.867
epoch: 35
Train: [0/422]	Time 0.383 (0.383)	Loss 0.2401 (0.2401)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.063 (0.061)	Loss 0.2724 (0.2516)	Prec@1 87.500 (90.257)	Prec@5 100.000 (99.908)
Train: [100/422]	Time 0.062 (0.060)	Loss 0.2731 (0.2517)	Prec@1 90.625 (90.766)	Prec@5 100.000 (99.875)
Train: [150/422]	Time 0.058 (0.061)	Loss 0.2148 (0.2498)	Prec@1 92.188 (91.266)	Prec@5 100.000 (99.875)
Train: [200/422]	Time 0.054 (0.056)	Loss 0.3291 (0.2393)	Prec@1 87.500 (91.422)	Prec@5 100.000 (99.891)
Train: [250/422]	Time 0.058 (0.056)	Loss 0.2588 (0.2380)	Prec@1 92.188 (91.453)	Prec@5 100.000 (99.859)
Train: [300/422]	Time 0.055 (0.055)	Loss 0.2115 (0.2545)	Prec@1 93.750 (90.938)	Prec@5 100.000 (99.859)
Train: [350/422]	Time 0.054 (0.055)	Loss 0.2278 (0.2568)	Prec@1 92.188 (90.797)	Prec@5 100.000 (99.906)
Train: [400/422]	Time 0.055 (0.056)	Loss 0.1992 (0.2616)	Prec@1 90.625 (90.781)	Prec@5 100.000 (99.906)
Test: [0/47]	Time 0.372 (0.372)	Loss 0.2537 (0.2537)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.016 (0.033)	Loss 0.2377 (0.2642)	Prec@1 90.625 (90.848)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.024)	Loss 0.2466 (0.2564)	Prec@1 93.750 (91.082)	Prec@5 100.000 (99.962)
Test: [35/40] Acc 90.967
epoch: 36
Train: [0/422]	Time 0.459 (0.459)	Loss 0.2886 (0.2886)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.054 (0.062)	Loss 0.1548 (0.2375)	Prec@1 95.312 (92.004)	Prec@5 100.000 (99.939)
Train: [100/422]	Time 0.059 (0.055)	Loss 0.2754 (0.2420)	Prec@1 90.625 (91.438)	Prec@5 100.000 (99.906)
Train: [150/422]	Time 0.070 (0.060)	Loss 0.2970 (0.2494)	Prec@1 85.938 (90.781)	Prec@5 100.000 (99.906)
Train: [200/422]	Time 0.053 (0.062)	Loss 0.2839 (0.2587)	Prec@1 89.062 (90.500)	Prec@5 100.000 (99.938)
Train: [250/422]	Time 0.056 (0.061)	Loss 0.3721 (0.2640)	Prec@1 84.375 (90.516)	Prec@5 100.000 (99.891)
Train: [300/422]	Time 0.056 (0.061)	Loss 0.1945 (0.2582)	Prec@1 92.188 (90.625)	Prec@5 100.000 (99.875)
Train: [350/422]	Time 0.056 (0.060)	Loss 0.2019 (0.2506)	Prec@1 90.625 (90.812)	Prec@5 100.000 (99.906)
Train: [400/422]	Time 0.064 (0.062)	Loss 0.3806 (0.2449)	Prec@1 84.375 (91.203)	Prec@5 100.000 (99.906)
Test: [0/47]	Time 0.349 (0.349)	Loss 0.2469 (0.2469)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.033)	Loss 0.2342 (0.2614)	Prec@1 90.625 (90.699)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.015 (0.024)	Loss 0.2555 (0.2539)	Prec@1 93.750 (91.006)	Prec@5 100.000 (99.962)
Test: [36/40] Acc 90.767
epoch: 37
Train: [0/422]	Time 0.377 (0.377)	Loss 0.2843 (0.2843)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.051 (0.056)	Loss 0.2970 (0.2690)	Prec@1 89.062 (90.196)	Prec@5 100.000 (99.847)
Train: [100/422]	Time 0.067 (0.058)	Loss 0.1764 (0.2508)	Prec@1 92.188 (90.953)	Prec@5 100.000 (99.859)
Train: [150/422]	Time 0.055 (0.068)	Loss 0.2126 (0.2444)	Prec@1 89.062 (91.391)	Prec@5 100.000 (99.922)
Train: [200/422]	Time 0.062 (0.064)	Loss 0.2373 (0.2538)	Prec@1 92.188 (90.766)	Prec@5 100.000 (99.969)
Train: [250/422]	Time 0.055 (0.058)	Loss 0.1886 (0.2519)	Prec@1 93.750 (90.594)	Prec@5 100.000 (99.984)
Train: [300/422]	Time 0.057 (0.058)	Loss 0.2357 (0.2409)	Prec@1 90.625 (91.172)	Prec@5 100.000 (99.922)
Train: [350/422]	Time 0.072 (0.059)	Loss 0.2378 (0.2418)	Prec@1 92.188 (91.281)	Prec@5 100.000 (99.859)
Train: [400/422]	Time 0.057 (0.063)	Loss 0.2459 (0.2504)	Prec@1 90.625 (91.016)	Prec@5 100.000 (99.906)
Test: [0/47]	Time 0.364 (0.364)	Loss 0.2515 (0.2515)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.032)	Loss 0.2323 (0.2651)	Prec@1 90.625 (90.923)	Prec@5 100.000 (99.926)
Test: [40/47]	Time 0.014 (0.023)	Loss 0.2537 (0.2564)	Prec@1 93.750 (91.235)	Prec@5 100.000 (99.924)
Test: [37/40] Acc 91.133
epoch: 38
Train: [0/422]	Time 0.428 (0.428)	Loss 0.4280 (0.4280)	Prec@1 81.250 (81.250)	Prec@5 98.438 (98.438)
Train: [50/422]	Time 0.054 (0.063)	Loss 0.2490 (0.2488)	Prec@1 90.625 (90.748)	Prec@5 100.000 (99.847)
Train: [100/422]	Time 0.052 (0.054)	Loss 0.5011 (0.2470)	Prec@1 78.125 (91.031)	Prec@5 100.000 (99.891)
Train: [150/422]	Time 0.056 (0.057)	Loss 0.2183 (0.2487)	Prec@1 90.625 (91.172)	Prec@5 100.000 (99.938)
Train: [200/422]	Time 0.051 (0.058)	Loss 0.2723 (0.2463)	Prec@1 89.062 (91.031)	Prec@5 100.000 (99.938)
Train: [250/422]	Time 0.050 (0.052)	Loss 0.2821 (0.2461)	Prec@1 92.188 (91.000)	Prec@5 100.000 (99.938)
Train: [300/422]	Time 0.064 (0.054)	Loss 0.3969 (0.2496)	Prec@1 89.062 (91.125)	Prec@5 98.438 (99.922)
Train: [350/422]	Time 0.064 (0.057)	Loss 0.2665 (0.2490)	Prec@1 90.625 (91.094)	Prec@5 100.000 (99.891)
Train: [400/422]	Time 0.058 (0.058)	Loss 0.2897 (0.2484)	Prec@1 90.625 (91.094)	Prec@5 100.000 (99.922)
Test: [0/47]	Time 0.404 (0.404)	Loss 0.2478 (0.2478)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.016 (0.035)	Loss 0.2358 (0.2639)	Prec@1 89.062 (90.774)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.012 (0.025)	Loss 0.2395 (0.2545)	Prec@1 93.750 (91.120)	Prec@5 100.000 (99.962)
Test: [38/40] Acc 90.967
epoch: 39
Train: [0/422]	Time 0.395 (0.395)	Loss 0.2561 (0.2561)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.060 (0.066)	Loss 0.3039 (0.2383)	Prec@1 90.625 (91.759)	Prec@5 98.438 (99.939)
Train: [100/422]	Time 0.058 (0.058)	Loss 0.2199 (0.2478)	Prec@1 92.188 (91.391)	Prec@5 100.000 (99.938)
Train: [150/422]	Time 0.059 (0.058)	Loss 0.4541 (0.2635)	Prec@1 84.375 (90.797)	Prec@5 100.000 (99.953)
Train: [200/422]	Time 0.057 (0.058)	Loss 0.2218 (0.2565)	Prec@1 93.750 (90.938)	Prec@5 100.000 (99.969)
Train: [250/422]	Time 0.061 (0.057)	Loss 0.2146 (0.2417)	Prec@1 90.625 (91.375)	Prec@5 100.000 (99.891)
Train: [300/422]	Time 0.060 (0.058)	Loss 0.2882 (0.2359)	Prec@1 90.625 (91.578)	Prec@5 100.000 (99.828)
Train: [350/422]	Time 0.059 (0.059)	Loss 0.1495 (0.2385)	Prec@1 96.875 (91.219)	Prec@5 100.000 (99.891)
Train: [400/422]	Time 0.053 (0.057)	Loss 0.2242 (0.2574)	Prec@1 93.750 (90.391)	Prec@5 100.000 (99.938)
Test: [0/47]	Time 0.358 (0.358)	Loss 0.2413 (0.2413)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.033)	Loss 0.2365 (0.2638)	Prec@1 90.625 (90.402)	Prec@5 100.000 (100.000)
Test: [40/47]	Time 0.013 (0.023)	Loss 0.2319 (0.2533)	Prec@1 93.750 (90.968)	Prec@5 100.000 (99.962)
Test: [39/40] Acc 90.800
best ACC: 91.13333333333334
