split train val random seed:2481
RANDOM SEED: 2481
data catagory: train
len of img: 27000
data catagory: val
len of img: 3000
data catagory: test
len of img: 5000
using device: cuda
~fc: Conv2d1.conv.weight
~fc: Conv2d1.bn.weight
~fc: Conv2d1.bn.bias
~fc: Conv2d2.conv.weight
~fc: Conv2d2.bn.weight
~fc: Conv2d2.bn.bias
~fc: Mixed_3a.branch1x1.conv.weight
~fc: Mixed_3a.branch1x1.bn.weight
~fc: Mixed_3a.branch1x1.bn.bias
~fc: Mixed_3a.branch3x3_1.conv.weight
~fc: Mixed_3a.branch3x3_1.bn.weight
~fc: Mixed_3a.branch3x3_1.bn.bias
~fc: Mixed_3a.branch3x3_2.conv.weight
~fc: Mixed_3a.branch3x3_2.bn.weight
~fc: Mixed_3a.branch3x3_2.bn.bias
~fc: Mixed_3a.branch5x5_1.conv.weight
~fc: Mixed_3a.branch5x5_1.bn.weight
~fc: Mixed_3a.branch5x5_1.bn.bias
~fc: Mixed_3a.branch5x5_2.conv.weight
~fc: Mixed_3a.branch5x5_2.bn.weight
~fc: Mixed_3a.branch5x5_2.bn.bias
~fc: Mixed_3a.branch_pool.conv.weight
~fc: Mixed_3a.branch_pool.bn.weight
~fc: Mixed_3a.branch_pool.bn.bias
~fc: Mixed_3b.branch1x1.conv.weight
~fc: Mixed_3b.branch1x1.bn.weight
~fc: Mixed_3b.branch1x1.bn.bias
~fc: Mixed_3b.branch3x3_1.conv.weight
~fc: Mixed_3b.branch3x3_1.bn.weight
~fc: Mixed_3b.branch3x3_1.bn.bias
~fc: Mixed_3b.branch3x3_2.conv.weight
~fc: Mixed_3b.branch3x3_2.bn.weight
~fc: Mixed_3b.branch3x3_2.bn.bias
~fc: Mixed_3b.branch5x5_1.conv.weight
~fc: Mixed_3b.branch5x5_1.bn.weight
~fc: Mixed_3b.branch5x5_1.bn.bias
~fc: Mixed_3b.branch5x5_2.conv.weight
~fc: Mixed_3b.branch5x5_2.bn.weight
~fc: Mixed_3b.branch5x5_2.bn.bias
~fc: Mixed_3b.branch_pool.conv.weight
~fc: Mixed_3b.branch_pool.bn.weight
~fc: Mixed_3b.branch_pool.bn.bias
~fc: Mixed_4a.branch1x1.conv.weight
~fc: Mixed_4a.branch1x1.bn.weight
~fc: Mixed_4a.branch1x1.bn.bias
~fc: Mixed_4a.branch3x3_1.conv.weight
~fc: Mixed_4a.branch3x3_1.bn.weight
~fc: Mixed_4a.branch3x3_1.bn.bias
~fc: Mixed_4a.branch3x3_2.conv.weight
~fc: Mixed_4a.branch3x3_2.bn.weight
~fc: Mixed_4a.branch3x3_2.bn.bias
~fc: Mixed_4a.branch5x5_1.conv.weight
~fc: Mixed_4a.branch5x5_1.bn.weight
~fc: Mixed_4a.branch5x5_1.bn.bias
~fc: Mixed_4a.branch5x5_2.conv.weight
~fc: Mixed_4a.branch5x5_2.bn.weight
~fc: Mixed_4a.branch5x5_2.bn.bias
~fc: Mixed_4a.branch_pool.conv.weight
~fc: Mixed_4a.branch_pool.bn.weight
~fc: Mixed_4a.branch_pool.bn.bias
~fc: Mixed_4b.branch1x1.conv.weight
~fc: Mixed_4b.branch1x1.bn.weight
~fc: Mixed_4b.branch1x1.bn.bias
~fc: Mixed_4b.branch3x3_1.conv.weight
~fc: Mixed_4b.branch3x3_1.bn.weight
~fc: Mixed_4b.branch3x3_1.bn.bias
~fc: Mixed_4b.branch3x3_2.conv.weight
~fc: Mixed_4b.branch3x3_2.bn.weight
~fc: Mixed_4b.branch3x3_2.bn.bias
~fc: Mixed_4b.branch5x5_1.conv.weight
~fc: Mixed_4b.branch5x5_1.bn.weight
~fc: Mixed_4b.branch5x5_1.bn.bias
~fc: Mixed_4b.branch5x5_2.conv.weight
~fc: Mixed_4b.branch5x5_2.bn.weight
~fc: Mixed_4b.branch5x5_2.bn.bias
~fc: Mixed_4b.branch_pool.conv.weight
~fc: Mixed_4b.branch_pool.bn.weight
~fc: Mixed_4b.branch_pool.bn.bias
~fc: Mixed_4c.branch1x1.conv.weight
~fc: Mixed_4c.branch1x1.bn.weight
~fc: Mixed_4c.branch1x1.bn.bias
~fc: Mixed_4c.branch3x3_1.conv.weight
~fc: Mixed_4c.branch3x3_1.bn.weight
~fc: Mixed_4c.branch3x3_1.bn.bias
~fc: Mixed_4c.branch3x3_2.conv.weight
~fc: Mixed_4c.branch3x3_2.bn.weight
~fc: Mixed_4c.branch3x3_2.bn.bias
~fc: Mixed_4c.branch5x5_1.conv.weight
~fc: Mixed_4c.branch5x5_1.bn.weight
~fc: Mixed_4c.branch5x5_1.bn.bias
~fc: Mixed_4c.branch5x5_2.conv.weight
~fc: Mixed_4c.branch5x5_2.bn.weight
~fc: Mixed_4c.branch5x5_2.bn.bias
~fc: Mixed_4c.branch_pool.conv.weight
~fc: Mixed_4c.branch_pool.bn.weight
~fc: Mixed_4c.branch_pool.bn.bias
~fc: Mixed_4d.branch1x1.conv.weight
~fc: Mixed_4d.branch1x1.bn.weight
~fc: Mixed_4d.branch1x1.bn.bias
~fc: Mixed_4d.branch3x3_1.conv.weight
~fc: Mixed_4d.branch3x3_1.bn.weight
~fc: Mixed_4d.branch3x3_1.bn.bias
~fc: Mixed_4d.branch3x3_2.conv.weight
~fc: Mixed_4d.branch3x3_2.bn.weight
~fc: Mixed_4d.branch3x3_2.bn.bias
~fc: Mixed_4d.branch5x5_1.conv.weight
~fc: Mixed_4d.branch5x5_1.bn.weight
~fc: Mixed_4d.branch5x5_1.bn.bias
~fc: Mixed_4d.branch5x5_2.conv.weight
~fc: Mixed_4d.branch5x5_2.bn.weight
~fc: Mixed_4d.branch5x5_2.bn.bias
~fc: Mixed_4d.branch_pool.conv.weight
~fc: Mixed_4d.branch_pool.bn.weight
~fc: Mixed_4d.branch_pool.bn.bias
~fc: Mixed_4e.branch1x1.conv.weight
~fc: Mixed_4e.branch1x1.bn.weight
~fc: Mixed_4e.branch1x1.bn.bias
~fc: Mixed_4e.branch3x3_1.conv.weight
~fc: Mixed_4e.branch3x3_1.bn.weight
~fc: Mixed_4e.branch3x3_1.bn.bias
~fc: Mixed_4e.branch3x3_2.conv.weight
~fc: Mixed_4e.branch3x3_2.bn.weight
~fc: Mixed_4e.branch3x3_2.bn.bias
~fc: Mixed_4e.branch5x5_1.conv.weight
~fc: Mixed_4e.branch5x5_1.bn.weight
~fc: Mixed_4e.branch5x5_1.bn.bias
~fc: Mixed_4e.branch5x5_2.conv.weight
~fc: Mixed_4e.branch5x5_2.bn.weight
~fc: Mixed_4e.branch5x5_2.bn.bias
~fc: Mixed_4e.branch_pool.conv.weight
~fc: Mixed_4e.branch_pool.bn.weight
~fc: Mixed_4e.branch_pool.bn.bias
~fc: Mixed_5a.branch1x1.conv.weight
~fc: Mixed_5a.branch1x1.bn.weight
~fc: Mixed_5a.branch1x1.bn.bias
~fc: Mixed_5a.branch3x3_1.conv.weight
~fc: Mixed_5a.branch3x3_1.bn.weight
~fc: Mixed_5a.branch3x3_1.bn.bias
~fc: Mixed_5a.branch3x3_2.conv.weight
~fc: Mixed_5a.branch3x3_2.bn.weight
~fc: Mixed_5a.branch3x3_2.bn.bias
~fc: Mixed_5a.branch5x5_1.conv.weight
~fc: Mixed_5a.branch5x5_1.bn.weight
~fc: Mixed_5a.branch5x5_1.bn.bias
~fc: Mixed_5a.branch5x5_2.conv.weight
~fc: Mixed_5a.branch5x5_2.bn.weight
~fc: Mixed_5a.branch5x5_2.bn.bias
~fc: Mixed_5a.branch_pool.conv.weight
~fc: Mixed_5a.branch_pool.bn.weight
~fc: Mixed_5a.branch_pool.bn.bias
~fc: Mixed_5b.branch1x1.conv.weight
~fc: Mixed_5b.branch1x1.bn.weight
~fc: Mixed_5b.branch1x1.bn.bias
~fc: Mixed_5b.branch3x3_1.conv.weight
~fc: Mixed_5b.branch3x3_1.bn.weight
~fc: Mixed_5b.branch3x3_1.bn.bias
~fc: Mixed_5b.branch3x3_2.conv.weight
~fc: Mixed_5b.branch3x3_2.bn.weight
~fc: Mixed_5b.branch3x3_2.bn.bias
~fc: Mixed_5b.branch5x5_1.conv.weight
~fc: Mixed_5b.branch5x5_1.bn.weight
~fc: Mixed_5b.branch5x5_1.bn.bias
~fc: Mixed_5b.branch5x5_2.conv.weight
~fc: Mixed_5b.branch5x5_2.bn.weight
~fc: Mixed_5b.branch5x5_2.bn.bias
~fc: Mixed_5b.branch_pool.conv.weight
~fc: Mixed_5b.branch_pool.bn.weight
~fc: Mixed_5b.branch_pool.bn.bias
~fc: fc2.weight
~fc: fc2.bias
fc learning rate: 0.001
not fc learning rate: 0.001
googleNet(
  (Conv2d1): BasicConv2d(
    (conv): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
    (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (Conv2d2): BasicConv2d(
    (conv): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (Mixed_3a): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(4, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_3b): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(8, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_4a): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(120, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(24, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(52, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(120, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(4, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(4, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(12, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(120, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_4b): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(128, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(128, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(28, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(28, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(56, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(128, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(6, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_4c): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(128, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(12, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(12, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_4d): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(128, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(28, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(128, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(36, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(36, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(72, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_4e): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(132, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(132, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(40, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(132, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(8, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(132, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_5a): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(208, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(208, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(40, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(208, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(8, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(208, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (Mixed_5b): InceptionModule(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(208, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(208, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch3x3_2): BasicConv2d(
      (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_1): BasicConv2d(
      (conv): Conv2d(208, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(12, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch5x5_2): BasicConv2d(
      (conv): Conv2d(12, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch_pool): BasicConv2d(
      (conv): Conv2d(208, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (dropout_layer): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
)
epoch: 0
Train: [0/422]	Time 4.944 (4.944)	Loss 2.3467 (2.3467)	Prec@1 9.375 (9.375)	Prec@5 51.562 (51.562)
Train: [50/422]	Time 0.067 (0.164)	Loss 1.0639 (1.4179)	Prec@1 64.062 (48.713)	Prec@5 95.312 (92.371)
Train: [100/422]	Time 0.075 (0.068)	Loss 0.9495 (1.1850)	Prec@1 64.062 (57.062)	Prec@5 98.438 (95.484)
Train: [150/422]	Time 0.067 (0.070)	Loss 0.7743 (0.9150)	Prec@1 68.750 (66.391)	Prec@5 98.438 (98.312)
Train: [200/422]	Time 0.084 (0.072)	Loss 0.9285 (0.8485)	Prec@1 57.812 (68.297)	Prec@5 98.438 (98.797)
Train: [250/422]	Time 0.066 (0.070)	Loss 0.5840 (0.8094)	Prec@1 78.125 (69.391)	Prec@5 100.000 (98.812)
Train: [300/422]	Time 0.069 (0.067)	Loss 0.8377 (0.7479)	Prec@1 67.188 (72.328)	Prec@5 100.000 (99.156)
Train: [350/422]	Time 0.066 (0.067)	Loss 0.5175 (0.7152)	Prec@1 82.812 (73.828)	Prec@5 100.000 (99.328)
Train: [400/422]	Time 0.066 (0.066)	Loss 0.6904 (0.7146)	Prec@1 75.000 (73.359)	Prec@5 98.438 (99.188)
Test: [0/47]	Time 0.354 (0.354)	Loss 0.7978 (0.7978)	Prec@1 68.750 (68.750)	Prec@5 96.875 (96.875)
Test: [20/47]	Time 0.015 (0.270)	Loss 0.6689 (0.5733)	Prec@1 76.562 (78.199)	Prec@5 98.438 (99.330)
Test: [40/47]	Time 0.014 (0.146)	Loss 0.7295 (0.5935)	Prec@1 79.688 (77.668)	Prec@5 98.438 (99.428)
Test: [0/40] Acc 77.633
epoch: 1
Train: [0/422]	Time 0.534 (0.534)	Loss 0.5799 (0.5799)	Prec@1 81.250 (81.250)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.060 (0.067)	Loss 0.7555 (0.6282)	Prec@1 67.188 (76.379)	Prec@5 98.438 (99.632)
Train: [100/422]	Time 0.061 (0.059)	Loss 0.7052 (0.6577)	Prec@1 75.000 (75.141)	Prec@5 100.000 (99.500)
Train: [150/422]	Time 0.064 (0.060)	Loss 0.4641 (0.6717)	Prec@1 84.375 (74.906)	Prec@5 100.000 (99.281)
Train: [200/422]	Time 0.062 (0.063)	Loss 0.5811 (0.6330)	Prec@1 78.125 (76.359)	Prec@5 100.000 (99.359)
Train: [250/422]	Time 0.068 (0.065)	Loss 0.6585 (0.6100)	Prec@1 73.438 (76.547)	Prec@5 98.438 (99.469)
Train: [300/422]	Time 0.063 (0.066)	Loss 0.5060 (0.6129)	Prec@1 81.250 (76.766)	Prec@5 100.000 (99.344)
Train: [350/422]	Time 0.064 (0.066)	Loss 0.5786 (0.5972)	Prec@1 79.688 (77.828)	Prec@5 100.000 (99.406)
Train: [400/422]	Time 0.066 (0.065)	Loss 0.6649 (0.5831)	Prec@1 78.125 (78.438)	Prec@5 98.438 (99.406)
Test: [0/47]	Time 0.327 (0.327)	Loss 0.6348 (0.6348)	Prec@1 75.000 (75.000)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.012 (0.027)	Loss 0.5799 (0.4964)	Prec@1 76.562 (81.324)	Prec@5 100.000 (99.479)
Test: [40/47]	Time 0.011 (0.020)	Loss 0.6435 (0.5090)	Prec@1 82.812 (81.707)	Prec@5 100.000 (99.543)
Test: [1/40] Acc 81.933
epoch: 2
Train: [0/422]	Time 0.368 (0.368)	Loss 0.4723 (0.4723)	Prec@1 82.812 (82.812)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.066 (0.065)	Loss 0.6437 (0.5372)	Prec@1 78.125 (79.657)	Prec@5 98.438 (99.694)
Train: [100/422]	Time 0.063 (0.063)	Loss 0.7143 (0.5481)	Prec@1 73.438 (79.453)	Prec@5 100.000 (99.594)
Train: [150/422]	Time 0.067 (0.065)	Loss 0.4781 (0.5580)	Prec@1 82.812 (79.156)	Prec@5 100.000 (99.516)
Train: [200/422]	Time 0.064 (0.065)	Loss 0.5320 (0.5462)	Prec@1 79.688 (79.797)	Prec@5 100.000 (99.578)
Train: [250/422]	Time 0.063 (0.065)	Loss 0.6381 (0.5274)	Prec@1 81.250 (81.094)	Prec@5 100.000 (99.641)
Train: [300/422]	Time 0.066 (0.065)	Loss 0.5250 (0.5201)	Prec@1 81.250 (80.922)	Prec@5 98.438 (99.688)
Train: [350/422]	Time 0.061 (0.065)	Loss 0.5808 (0.5238)	Prec@1 75.000 (80.109)	Prec@5 100.000 (99.594)
Train: [400/422]	Time 0.054 (0.065)	Loss 0.5485 (0.5357)	Prec@1 82.812 (79.953)	Prec@5 100.000 (99.359)
Test: [0/47]	Time 0.305 (0.305)	Loss 0.5504 (0.5504)	Prec@1 79.688 (79.688)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.012 (0.026)	Loss 0.5356 (0.4748)	Prec@1 82.812 (82.217)	Prec@5 100.000 (99.702)
Test: [40/47]	Time 0.011 (0.019)	Loss 0.5807 (0.5091)	Prec@1 84.375 (81.745)	Prec@5 100.000 (99.619)
Test: [2/40] Acc 81.867
epoch: 3
Train: [0/422]	Time 0.370 (0.370)	Loss 0.6191 (0.6191)	Prec@1 73.438 (73.438)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.066 (0.072)	Loss 0.4211 (0.4893)	Prec@1 89.062 (82.322)	Prec@5 100.000 (99.786)
Train: [100/422]	Time 0.066 (0.066)	Loss 0.2918 (0.4884)	Prec@1 92.188 (82.547)	Prec@5 100.000 (99.641)
Train: [150/422]	Time 0.068 (0.066)	Loss 0.4741 (0.5061)	Prec@1 85.938 (82.109)	Prec@5 100.000 (99.516)
Train: [200/422]	Time 0.067 (0.066)	Loss 1.0117 (0.5178)	Prec@1 65.625 (81.484)	Prec@5 98.438 (99.531)
Train: [250/422]	Time 0.076 (0.066)	Loss 0.4967 (0.5018)	Prec@1 79.688 (81.953)	Prec@5 100.000 (99.516)
Train: [300/422]	Time 0.063 (0.067)	Loss 0.5493 (0.4849)	Prec@1 82.812 (82.531)	Prec@5 98.438 (99.500)
Train: [350/422]	Time 0.074 (0.067)	Loss 0.5103 (0.4819)	Prec@1 81.250 (82.469)	Prec@5 98.438 (99.609)
Train: [400/422]	Time 0.075 (0.067)	Loss 0.3432 (0.4813)	Prec@1 85.938 (82.453)	Prec@5 100.000 (99.656)
Test: [0/47]	Time 0.337 (0.337)	Loss 0.4040 (0.4040)	Prec@1 84.375 (84.375)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.011 (0.029)	Loss 0.4073 (0.4358)	Prec@1 87.500 (84.375)	Prec@5 100.000 (99.628)
Test: [40/47]	Time 0.011 (0.021)	Loss 0.6855 (0.4652)	Prec@1 71.875 (83.308)	Prec@5 98.438 (99.695)
Test: [3/40] Acc 83.400
epoch: 4
Train: [0/422]	Time 0.358 (0.358)	Loss 0.4929 (0.4929)	Prec@1 84.375 (84.375)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.064 (0.070)	Loss 0.4994 (0.4650)	Prec@1 78.125 (82.966)	Prec@5 100.000 (99.632)
Train: [100/422]	Time 0.065 (0.064)	Loss 0.4763 (0.4704)	Prec@1 81.250 (82.891)	Prec@5 100.000 (99.578)
Train: [150/422]	Time 0.070 (0.066)	Loss 0.4176 (0.4937)	Prec@1 85.938 (82.578)	Prec@5 100.000 (99.484)
Train: [200/422]	Time 0.065 (0.068)	Loss 0.3242 (0.4925)	Prec@1 87.500 (82.562)	Prec@5 100.000 (99.531)
Train: [250/422]	Time 0.064 (0.068)	Loss 0.4625 (0.4746)	Prec@1 84.375 (82.250)	Prec@5 100.000 (99.656)
Train: [300/422]	Time 0.075 (0.069)	Loss 0.5137 (0.4480)	Prec@1 82.812 (83.141)	Prec@5 100.000 (99.703)
Train: [350/422]	Time 0.078 (0.070)	Loss 0.3624 (0.4417)	Prec@1 89.062 (83.578)	Prec@5 100.000 (99.734)
Train: [400/422]	Time 0.069 (0.071)	Loss 0.3137 (0.4697)	Prec@1 89.062 (82.609)	Prec@5 100.000 (99.641)
Test: [0/47]	Time 0.338 (0.338)	Loss 0.4398 (0.4398)	Prec@1 78.125 (78.125)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.012 (0.029)	Loss 0.5453 (0.4065)	Prec@1 81.250 (84.598)	Prec@5 100.000 (99.628)
Test: [40/47]	Time 0.011 (0.021)	Loss 0.5303 (0.4323)	Prec@1 82.812 (83.880)	Prec@5 100.000 (99.695)
Test: [4/40] Acc 84.067
epoch: 5
Train: [0/422]	Time 0.391 (0.391)	Loss 0.5570 (0.5570)	Prec@1 79.688 (79.688)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.063 (0.068)	Loss 0.4231 (0.4540)	Prec@1 84.375 (83.180)	Prec@5 100.000 (99.694)
Train: [100/422]	Time 0.065 (0.063)	Loss 0.2611 (0.4528)	Prec@1 90.625 (83.453)	Prec@5 100.000 (99.703)
Train: [150/422]	Time 0.064 (0.065)	Loss 0.4090 (0.4486)	Prec@1 87.500 (83.688)	Prec@5 100.000 (99.688)
Train: [200/422]	Time 0.065 (0.064)	Loss 0.3084 (0.4352)	Prec@1 87.500 (84.047)	Prec@5 100.000 (99.609)
Train: [250/422]	Time 0.065 (0.064)	Loss 0.4375 (0.4335)	Prec@1 85.938 (84.250)	Prec@5 100.000 (99.672)
Train: [300/422]	Time 0.063 (0.064)	Loss 0.3157 (0.4406)	Prec@1 89.062 (83.922)	Prec@5 98.438 (99.656)
Train: [350/422]	Time 0.065 (0.064)	Loss 0.4972 (0.4451)	Prec@1 81.250 (83.344)	Prec@5 100.000 (99.594)
Train: [400/422]	Time 0.070 (0.067)	Loss 0.4562 (0.4392)	Prec@1 81.250 (83.609)	Prec@5 100.000 (99.688)
Test: [0/47]	Time 0.347 (0.347)	Loss 0.4385 (0.4385)	Prec@1 84.375 (84.375)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.032)	Loss 0.3851 (0.4110)	Prec@1 81.250 (83.333)	Prec@5 100.000 (99.702)
Test: [40/47]	Time 0.012 (0.023)	Loss 0.6354 (0.4419)	Prec@1 78.125 (82.393)	Prec@5 98.438 (99.695)
Test: [5/40] Acc 82.433
epoch: 6
Train: [0/422]	Time 0.376 (0.376)	Loss 0.3344 (0.3344)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.059 (0.065)	Loss 0.4729 (0.4403)	Prec@1 85.938 (83.854)	Prec@5 98.438 (99.540)
Train: [100/422]	Time 0.059 (0.059)	Loss 0.3408 (0.4353)	Prec@1 89.062 (84.312)	Prec@5 100.000 (99.656)
Train: [150/422]	Time 0.053 (0.058)	Loss 0.3842 (0.4251)	Prec@1 85.938 (84.781)	Prec@5 100.000 (99.750)
Train: [200/422]	Time 0.058 (0.057)	Loss 0.3015 (0.4129)	Prec@1 90.625 (84.812)	Prec@5 100.000 (99.750)
Train: [250/422]	Time 0.065 (0.058)	Loss 0.5158 (0.4247)	Prec@1 81.250 (84.203)	Prec@5 100.000 (99.766)
Train: [300/422]	Time 0.059 (0.060)	Loss 0.4964 (0.4287)	Prec@1 81.250 (84.906)	Prec@5 100.000 (99.625)
Train: [350/422]	Time 0.075 (0.063)	Loss 0.3921 (0.4052)	Prec@1 82.812 (85.547)	Prec@5 100.000 (99.672)
Train: [400/422]	Time 0.058 (0.064)	Loss 0.4639 (0.4200)	Prec@1 78.125 (84.062)	Prec@5 100.000 (99.719)
Test: [0/47]	Time 0.351 (0.351)	Loss 0.3700 (0.3700)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.012 (0.030)	Loss 0.4021 (0.3574)	Prec@1 90.625 (87.351)	Prec@5 100.000 (99.851)
Test: [40/47]	Time 0.011 (0.021)	Loss 0.4201 (0.3835)	Prec@1 85.938 (86.852)	Prec@5 98.438 (99.695)
Test: [6/40] Acc 86.833
epoch: 7
Train: [0/422]	Time 0.352 (0.352)	Loss 0.5145 (0.5145)	Prec@1 75.000 (75.000)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.065 (0.063)	Loss 0.4901 (0.4277)	Prec@1 82.812 (84.528)	Prec@5 96.875 (99.449)
Train: [100/422]	Time 0.066 (0.059)	Loss 0.4981 (0.4114)	Prec@1 81.250 (85.031)	Prec@5 96.875 (99.578)
Train: [150/422]	Time 0.064 (0.064)	Loss 0.5467 (0.3948)	Prec@1 79.688 (85.438)	Prec@5 100.000 (99.766)
Train: [200/422]	Time 0.064 (0.067)	Loss 0.5388 (0.4020)	Prec@1 79.688 (85.016)	Prec@5 100.000 (99.766)
Train: [250/422]	Time 0.068 (0.066)	Loss 0.3730 (0.4011)	Prec@1 85.938 (85.219)	Prec@5 98.438 (99.750)
Train: [300/422]	Time 0.067 (0.065)	Loss 0.3405 (0.3973)	Prec@1 85.938 (85.359)	Prec@5 100.000 (99.781)
Train: [350/422]	Time 0.065 (0.065)	Loss 0.3232 (0.4109)	Prec@1 90.625 (85.047)	Prec@5 100.000 (99.750)
Train: [400/422]	Time 0.067 (0.065)	Loss 0.8685 (0.4070)	Prec@1 71.875 (85.875)	Prec@5 100.000 (99.625)
Test: [0/47]	Time 0.350 (0.350)	Loss 0.3879 (0.3879)	Prec@1 84.375 (84.375)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.015 (0.040)	Loss 0.3686 (0.3597)	Prec@1 82.812 (86.235)	Prec@5 100.000 (99.926)
Test: [40/47]	Time 0.014 (0.027)	Loss 0.4399 (0.3935)	Prec@1 85.938 (85.899)	Prec@5 100.000 (99.924)
Test: [7/40] Acc 86.033
epoch: 8
Train: [0/422]	Time 0.401 (0.401)	Loss 0.4189 (0.4189)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.064 (0.073)	Loss 0.3676 (0.3972)	Prec@1 85.938 (85.202)	Prec@5 100.000 (99.755)
Train: [100/422]	Time 0.063 (0.066)	Loss 0.5064 (0.3939)	Prec@1 84.375 (85.375)	Prec@5 100.000 (99.766)
Train: [150/422]	Time 0.065 (0.066)	Loss 0.3559 (0.4110)	Prec@1 87.500 (84.906)	Prec@5 100.000 (99.766)
Train: [200/422]	Time 0.067 (0.068)	Loss 0.4699 (0.4098)	Prec@1 84.375 (85.203)	Prec@5 100.000 (99.703)
Train: [250/422]	Time 0.065 (0.069)	Loss 0.2773 (0.3816)	Prec@1 87.500 (86.438)	Prec@5 100.000 (99.703)
Train: [300/422]	Time 0.066 (0.068)	Loss 0.2568 (0.3761)	Prec@1 90.625 (86.500)	Prec@5 100.000 (99.766)
Train: [350/422]	Time 0.073 (0.068)	Loss 0.3080 (0.3730)	Prec@1 87.500 (86.359)	Prec@5 100.000 (99.766)
Train: [400/422]	Time 0.075 (0.070)	Loss 0.5908 (0.3770)	Prec@1 79.688 (85.906)	Prec@5 98.438 (99.781)
Test: [0/47]	Time 0.342 (0.342)	Loss 0.2659 (0.2659)	Prec@1 87.500 (87.500)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.013 (0.030)	Loss 0.3030 (0.3198)	Prec@1 90.625 (87.500)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.012 (0.021)	Loss 0.4567 (0.3517)	Prec@1 85.938 (86.776)	Prec@5 100.000 (99.848)
Test: [8/40] Acc 86.867
epoch: 9
Train: [0/422]	Time 0.407 (0.407)	Loss 0.3626 (0.3626)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.065 (0.069)	Loss 0.3378 (0.3822)	Prec@1 87.500 (85.662)	Prec@5 100.000 (99.816)
Train: [100/422]	Time 0.070 (0.064)	Loss 0.3658 (0.3802)	Prec@1 92.188 (86.172)	Prec@5 100.000 (99.812)
Train: [150/422]	Time 0.065 (0.065)	Loss 0.4001 (0.3760)	Prec@1 81.250 (86.094)	Prec@5 100.000 (99.875)
Train: [200/422]	Time 0.070 (0.066)	Loss 0.3908 (0.3690)	Prec@1 85.938 (86.094)	Prec@5 100.000 (99.891)
Train: [250/422]	Time 0.065 (0.068)	Loss 0.5007 (0.3663)	Prec@1 82.812 (86.578)	Prec@5 100.000 (99.750)
Train: [300/422]	Time 0.066 (0.067)	Loss 0.2696 (0.3840)	Prec@1 89.062 (85.781)	Prec@5 100.000 (99.688)
Train: [350/422]	Time 0.062 (0.065)	Loss 0.2801 (0.3883)	Prec@1 87.500 (85.391)	Prec@5 100.000 (99.797)
Train: [400/422]	Time 0.064 (0.068)	Loss 0.2896 (0.3670)	Prec@1 90.625 (86.688)	Prec@5 100.000 (99.844)
Test: [0/47]	Time 0.360 (0.360)	Loss 0.3249 (0.3249)	Prec@1 84.375 (84.375)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.014 (0.030)	Loss 0.3855 (0.3430)	Prec@1 84.375 (87.723)	Prec@5 100.000 (99.628)
Test: [40/47]	Time 0.012 (0.022)	Loss 0.3903 (0.3815)	Prec@1 85.938 (86.547)	Prec@5 100.000 (99.695)
Test: [9/40] Acc 86.867
epoch: 10
Train: [0/422]	Time 0.373 (0.373)	Loss 0.2616 (0.2616)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.060 (0.064)	Loss 0.4371 (0.3533)	Prec@1 82.812 (87.684)	Prec@5 100.000 (99.816)
Train: [100/422]	Time 0.057 (0.058)	Loss 0.3654 (0.3492)	Prec@1 85.938 (87.688)	Prec@5 100.000 (99.797)
Train: [150/422]	Time 0.064 (0.059)	Loss 0.2548 (0.3657)	Prec@1 89.062 (86.969)	Prec@5 100.000 (99.828)
Train: [200/422]	Time 0.064 (0.062)	Loss 0.2623 (0.3736)	Prec@1 90.625 (86.391)	Prec@5 100.000 (99.859)
Train: [250/422]	Time 0.067 (0.065)	Loss 0.3279 (0.3746)	Prec@1 92.188 (86.047)	Prec@5 98.438 (99.781)
Train: [300/422]	Time 0.069 (0.066)	Loss 0.3110 (0.3696)	Prec@1 90.625 (86.203)	Prec@5 100.000 (99.734)
Train: [350/422]	Time 0.064 (0.066)	Loss 0.3533 (0.3538)	Prec@1 84.375 (86.984)	Prec@5 100.000 (99.703)
Train: [400/422]	Time 0.065 (0.067)	Loss 0.3833 (0.3569)	Prec@1 89.062 (86.969)	Prec@5 100.000 (99.750)
Test: [0/47]	Time 0.359 (0.359)	Loss 0.3458 (0.3458)	Prec@1 84.375 (84.375)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.031)	Loss 0.3354 (0.3395)	Prec@1 87.500 (87.202)	Prec@5 100.000 (99.628)
Test: [40/47]	Time 0.012 (0.022)	Loss 0.3837 (0.3724)	Prec@1 87.500 (86.471)	Prec@5 100.000 (99.733)
Test: [10/40] Acc 86.500
epoch: 11
Train: [0/422]	Time 0.374 (0.374)	Loss 0.5451 (0.5451)	Prec@1 79.688 (79.688)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.067 (0.071)	Loss 0.3602 (0.3440)	Prec@1 82.812 (86.795)	Prec@5 100.000 (99.877)
Train: [100/422]	Time 0.063 (0.065)	Loss 0.5339 (0.3577)	Prec@1 78.125 (86.938)	Prec@5 100.000 (99.828)
Train: [150/422]	Time 0.072 (0.066)	Loss 0.3340 (0.3537)	Prec@1 87.500 (87.375)	Prec@5 100.000 (99.812)
Train: [200/422]	Time 0.064 (0.065)	Loss 0.3664 (0.3505)	Prec@1 84.375 (86.844)	Prec@5 100.000 (99.766)
Train: [250/422]	Time 0.067 (0.065)	Loss 0.3582 (0.3612)	Prec@1 87.500 (86.375)	Prec@5 100.000 (99.734)
Train: [300/422]	Time 0.068 (0.066)	Loss 0.3371 (0.3555)	Prec@1 89.062 (87.047)	Prec@5 100.000 (99.828)
Train: [350/422]	Time 0.069 (0.066)	Loss 0.3473 (0.3610)	Prec@1 84.375 (87.172)	Prec@5 100.000 (99.875)
Train: [400/422]	Time 0.062 (0.065)	Loss 0.6288 (0.3597)	Prec@1 79.688 (87.359)	Prec@5 98.438 (99.812)
Test: [0/47]	Time 0.320 (0.320)	Loss 0.2712 (0.2712)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.029)	Loss 0.3216 (0.3081)	Prec@1 87.500 (88.170)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.016 (0.023)	Loss 0.3620 (0.3415)	Prec@1 87.500 (86.890)	Prec@5 100.000 (99.848)
Test: [11/40] Acc 86.867
epoch: 12
Train: [0/422]	Time 0.384 (0.384)	Loss 0.3349 (0.3349)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.060 (0.071)	Loss 0.2581 (0.3359)	Prec@1 95.312 (87.898)	Prec@5 100.000 (99.877)
Train: [100/422]	Time 0.064 (0.063)	Loss 0.5678 (0.3430)	Prec@1 76.562 (87.375)	Prec@5 100.000 (99.812)
Train: [150/422]	Time 0.063 (0.063)	Loss 0.2348 (0.3509)	Prec@1 87.500 (87.031)	Prec@5 100.000 (99.750)
Train: [200/422]	Time 0.067 (0.064)	Loss 0.2414 (0.3375)	Prec@1 89.062 (87.641)	Prec@5 100.000 (99.812)
Train: [250/422]	Time 0.065 (0.066)	Loss 0.5320 (0.3337)	Prec@1 79.688 (88.078)	Prec@5 100.000 (99.906)
Train: [300/422]	Time 0.067 (0.069)	Loss 0.3978 (0.3454)	Prec@1 89.062 (87.812)	Prec@5 98.438 (99.797)
Train: [350/422]	Time 0.063 (0.067)	Loss 0.4222 (0.3374)	Prec@1 81.250 (87.766)	Prec@5 100.000 (99.781)
Train: [400/422]	Time 0.064 (0.066)	Loss 0.3195 (0.3564)	Prec@1 87.500 (86.859)	Prec@5 100.000 (99.906)
Test: [0/47]	Time 0.309 (0.309)	Loss 0.3218 (0.3218)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.015 (0.028)	Loss 0.3066 (0.3411)	Prec@1 87.500 (86.830)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.011 (0.020)	Loss 0.4049 (0.3640)	Prec@1 85.938 (85.899)	Prec@5 100.000 (99.733)
Test: [12/40] Acc 86.133
epoch: 13
Train: [0/422]	Time 0.372 (0.372)	Loss 0.2929 (0.2929)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.061 (0.064)	Loss 0.4629 (0.3333)	Prec@1 82.812 (88.021)	Prec@5 100.000 (99.877)
Train: [100/422]	Time 0.066 (0.062)	Loss 0.3248 (0.3388)	Prec@1 87.500 (87.500)	Prec@5 100.000 (99.859)
Train: [150/422]	Time 0.067 (0.066)	Loss 0.4204 (0.3391)	Prec@1 85.938 (87.312)	Prec@5 100.000 (99.891)
Train: [200/422]	Time 0.068 (0.066)	Loss 0.3023 (0.3341)	Prec@1 89.062 (87.422)	Prec@5 100.000 (99.922)
Train: [250/422]	Time 0.066 (0.065)	Loss 0.4194 (0.3489)	Prec@1 84.375 (87.219)	Prec@5 100.000 (99.844)
Train: [300/422]	Time 0.066 (0.065)	Loss 0.4731 (0.3460)	Prec@1 81.250 (87.281)	Prec@5 100.000 (99.844)
Train: [350/422]	Time 0.060 (0.065)	Loss 0.4097 (0.3332)	Prec@1 84.375 (87.359)	Prec@5 100.000 (99.891)
Train: [400/422]	Time 0.061 (0.062)	Loss 0.2048 (0.3489)	Prec@1 95.312 (87.266)	Prec@5 100.000 (99.812)
Test: [0/47]	Time 0.336 (0.336)	Loss 0.2449 (0.2449)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.029)	Loss 0.3422 (0.3016)	Prec@1 89.062 (89.137)	Prec@5 100.000 (99.851)
Test: [40/47]	Time 0.011 (0.021)	Loss 0.4541 (0.3283)	Prec@1 87.500 (87.995)	Prec@5 100.000 (99.886)
Test: [13/40] Acc 88.233
epoch: 14
Train: [0/422]	Time 0.382 (0.382)	Loss 0.4553 (0.4553)	Prec@1 82.812 (82.812)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.062 (0.072)	Loss 0.2368 (0.3017)	Prec@1 93.750 (88.817)	Prec@5 100.000 (99.908)
Train: [100/422]	Time 0.065 (0.066)	Loss 0.3089 (0.3170)	Prec@1 90.625 (88.266)	Prec@5 100.000 (99.828)
Train: [150/422]	Time 0.066 (0.068)	Loss 0.1629 (0.3382)	Prec@1 95.312 (87.672)	Prec@5 100.000 (99.812)
Train: [200/422]	Time 0.066 (0.068)	Loss 0.1973 (0.3384)	Prec@1 92.188 (87.688)	Prec@5 100.000 (99.875)
Train: [250/422]	Time 0.065 (0.066)	Loss 0.3538 (0.3350)	Prec@1 82.812 (87.688)	Prec@5 98.438 (99.906)
Train: [300/422]	Time 0.063 (0.065)	Loss 0.3971 (0.3360)	Prec@1 79.688 (87.438)	Prec@5 100.000 (99.906)
Train: [350/422]	Time 0.068 (0.066)	Loss 0.2696 (0.3294)	Prec@1 85.938 (87.516)	Prec@5 100.000 (99.828)
Train: [400/422]	Time 0.064 (0.069)	Loss 0.2601 (0.3244)	Prec@1 92.188 (87.922)	Prec@5 100.000 (99.812)
Test: [0/47]	Time 0.351 (0.351)	Loss 0.2453 (0.2453)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.029)	Loss 0.3490 (0.3215)	Prec@1 84.375 (88.318)	Prec@5 100.000 (99.851)
Test: [40/47]	Time 0.011 (0.021)	Loss 0.3620 (0.3475)	Prec@1 84.375 (87.424)	Prec@5 100.000 (99.886)
Test: [14/40] Acc 87.333
epoch: 15
Train: [0/422]	Time 0.385 (0.385)	Loss 0.2450 (0.2450)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.065 (0.073)	Loss 0.3864 (0.3162)	Prec@1 87.500 (88.113)	Prec@5 100.000 (99.847)
Train: [100/422]	Time 0.063 (0.066)	Loss 0.2876 (0.3312)	Prec@1 87.500 (87.766)	Prec@5 100.000 (99.859)
Train: [150/422]	Time 0.067 (0.066)	Loss 0.2625 (0.3373)	Prec@1 90.625 (87.953)	Prec@5 100.000 (99.891)
Train: [200/422]	Time 0.068 (0.066)	Loss 0.2526 (0.3245)	Prec@1 92.188 (88.656)	Prec@5 100.000 (99.828)
Train: [250/422]	Time 0.066 (0.067)	Loss 0.4852 (0.3278)	Prec@1 81.250 (88.219)	Prec@5 100.000 (99.797)
Train: [300/422]	Time 0.069 (0.068)	Loss 0.3293 (0.3259)	Prec@1 85.938 (87.953)	Prec@5 100.000 (99.859)
Train: [350/422]	Time 0.066 (0.067)	Loss 0.3710 (0.3189)	Prec@1 82.812 (88.188)	Prec@5 100.000 (99.875)
Train: [400/422]	Time 0.067 (0.067)	Loss 0.4010 (0.3267)	Prec@1 82.812 (87.969)	Prec@5 100.000 (99.828)
Test: [0/47]	Time 0.345 (0.345)	Loss 0.2370 (0.2370)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.031)	Loss 0.4205 (0.3107)	Prec@1 84.375 (88.467)	Prec@5 100.000 (99.926)
Test: [40/47]	Time 0.011 (0.022)	Loss 0.3402 (0.3452)	Prec@1 87.500 (87.195)	Prec@5 100.000 (99.848)
Test: [15/40] Acc 87.333
epoch: 16
Train: [0/422]	Time 0.423 (0.423)	Loss 0.3507 (0.3507)	Prec@1 85.938 (85.938)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.070 (0.077)	Loss 0.2937 (0.3018)	Prec@1 89.062 (88.725)	Prec@5 100.000 (99.908)
Train: [100/422]	Time 0.065 (0.069)	Loss 0.2822 (0.3150)	Prec@1 89.062 (88.297)	Prec@5 100.000 (99.859)
Train: [150/422]	Time 0.066 (0.068)	Loss 0.3565 (0.3150)	Prec@1 87.500 (88.109)	Prec@5 100.000 (99.875)
Train: [200/422]	Time 0.069 (0.067)	Loss 0.1725 (0.3104)	Prec@1 92.188 (88.188)	Prec@5 100.000 (99.828)
Train: [250/422]	Time 0.059 (0.065)	Loss 0.2574 (0.3219)	Prec@1 89.062 (88.516)	Prec@5 98.438 (99.703)
Train: [300/422]	Time 0.058 (0.061)	Loss 0.2437 (0.3186)	Prec@1 93.750 (88.891)	Prec@5 100.000 (99.766)
Train: [350/422]	Time 0.067 (0.062)	Loss 0.2877 (0.3260)	Prec@1 92.188 (88.531)	Prec@5 100.000 (99.891)
Train: [400/422]	Time 0.047 (0.062)	Loss 0.4465 (0.3277)	Prec@1 87.500 (88.109)	Prec@5 100.000 (99.812)
Test: [0/47]	Time 0.352 (0.352)	Loss 0.2539 (0.2539)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.031)	Loss 0.2900 (0.2879)	Prec@1 89.062 (88.988)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.012 (0.022)	Loss 0.3813 (0.3255)	Prec@1 89.062 (87.614)	Prec@5 100.000 (99.809)
Test: [16/40] Acc 87.867
epoch: 17
Train: [0/422]	Time 0.387 (0.387)	Loss 0.2781 (0.2781)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.064 (0.074)	Loss 0.2145 (0.3007)	Prec@1 95.312 (89.338)	Prec@5 100.000 (99.969)
Train: [100/422]	Time 0.066 (0.066)	Loss 0.1646 (0.2989)	Prec@1 90.625 (88.969)	Prec@5 100.000 (99.938)
Train: [150/422]	Time 0.068 (0.066)	Loss 0.5064 (0.3054)	Prec@1 84.375 (88.453)	Prec@5 100.000 (99.891)
Train: [200/422]	Time 0.066 (0.066)	Loss 0.3963 (0.3201)	Prec@1 82.812 (88.312)	Prec@5 100.000 (99.891)
Train: [250/422]	Time 0.065 (0.065)	Loss 0.5139 (0.3251)	Prec@1 81.250 (88.078)	Prec@5 100.000 (99.875)
Train: [300/422]	Time 0.062 (0.065)	Loss 0.3462 (0.3227)	Prec@1 90.625 (88.203)	Prec@5 100.000 (99.828)
Train: [350/422]	Time 0.067 (0.063)	Loss 0.3216 (0.3201)	Prec@1 90.625 (88.641)	Prec@5 100.000 (99.797)
Train: [400/422]	Time 0.064 (0.064)	Loss 0.1809 (0.3078)	Prec@1 95.312 (88.922)	Prec@5 100.000 (99.844)
Test: [0/47]	Time 0.316 (0.316)	Loss 0.2092 (0.2092)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.027)	Loss 0.3725 (0.3260)	Prec@1 84.375 (87.872)	Prec@5 98.438 (99.777)
Test: [40/47]	Time 0.012 (0.020)	Loss 0.4430 (0.3541)	Prec@1 85.938 (87.576)	Prec@5 100.000 (99.848)
Test: [17/40] Acc 87.567
epoch: 18
Train: [0/422]	Time 0.380 (0.380)	Loss 0.1879 (0.1879)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.059 (0.066)	Loss 0.1595 (0.2903)	Prec@1 98.438 (90.227)	Prec@5 100.000 (99.908)
Train: [100/422]	Time 0.060 (0.060)	Loss 0.4451 (0.2881)	Prec@1 81.250 (89.703)	Prec@5 100.000 (99.906)
Train: [150/422]	Time 0.060 (0.060)	Loss 0.3602 (0.3039)	Prec@1 76.562 (88.703)	Prec@5 100.000 (99.812)
Train: [200/422]	Time 0.062 (0.060)	Loss 0.4391 (0.3225)	Prec@1 81.250 (88.109)	Prec@5 98.438 (99.781)
Train: [250/422]	Time 0.059 (0.059)	Loss 0.2025 (0.3179)	Prec@1 90.625 (88.484)	Prec@5 100.000 (99.859)
Train: [300/422]	Time 0.065 (0.059)	Loss 0.3807 (0.3077)	Prec@1 85.938 (89.094)	Prec@5 100.000 (99.875)
Train: [350/422]	Time 0.059 (0.059)	Loss 0.2481 (0.3054)	Prec@1 90.625 (88.859)	Prec@5 100.000 (99.875)
Train: [400/422]	Time 0.062 (0.063)	Loss 0.3404 (0.3026)	Prec@1 89.062 (88.625)	Prec@5 100.000 (99.828)
Test: [0/47]	Time 0.345 (0.345)	Loss 0.2479 (0.2479)	Prec@1 90.625 (90.625)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.012 (0.030)	Loss 0.3008 (0.3061)	Prec@1 90.625 (88.393)	Prec@5 100.000 (99.702)
Test: [40/47]	Time 0.011 (0.021)	Loss 0.2862 (0.3304)	Prec@1 87.500 (87.995)	Prec@5 100.000 (99.771)
Test: [18/40] Acc 88.200
epoch: 19
Train: [0/422]	Time 0.382 (0.382)	Loss 0.1043 (0.1043)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.045 (0.055)	Loss 0.3814 (0.3034)	Prec@1 84.375 (88.971)	Prec@5 100.000 (99.847)
Train: [100/422]	Time 0.045 (0.048)	Loss 0.2833 (0.2912)	Prec@1 90.625 (89.141)	Prec@5 100.000 (99.828)
Train: [150/422]	Time 0.049 (0.048)	Loss 0.2596 (0.2882)	Prec@1 90.625 (89.609)	Prec@5 100.000 (99.859)
Train: [200/422]	Time 0.047 (0.048)	Loss 0.1866 (0.3038)	Prec@1 93.750 (89.125)	Prec@5 100.000 (99.891)
Train: [250/422]	Time 0.048 (0.049)	Loss 0.2394 (0.2940)	Prec@1 93.750 (89.203)	Prec@5 100.000 (99.906)
Train: [300/422]	Time 0.076 (0.051)	Loss 0.2270 (0.2854)	Prec@1 92.188 (89.750)	Prec@5 100.000 (99.891)
Train: [350/422]	Time 0.064 (0.059)	Loss 0.3875 (0.3044)	Prec@1 84.375 (88.719)	Prec@5 100.000 (99.875)
Train: [400/422]	Time 0.071 (0.066)	Loss 0.3682 (0.3164)	Prec@1 82.812 (88.203)	Prec@5 100.000 (99.906)
Test: [0/47]	Time 0.339 (0.339)	Loss 0.2332 (0.2332)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.012 (0.030)	Loss 0.3203 (0.3072)	Prec@1 89.062 (88.021)	Prec@5 100.000 (99.702)
Test: [40/47]	Time 0.013 (0.021)	Loss 0.4031 (0.3301)	Prec@1 84.375 (87.157)	Prec@5 100.000 (99.809)
Test: [19/40] Acc 87.400
epoch: 20
Train: [0/422]	Time 0.392 (0.392)	Loss 0.3313 (0.3313)	Prec@1 87.500 (87.500)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.065 (0.073)	Loss 0.2987 (0.2873)	Prec@1 87.500 (89.246)	Prec@5 100.000 (99.877)
Train: [100/422]	Time 0.066 (0.066)	Loss 0.5282 (0.2948)	Prec@1 85.938 (88.812)	Prec@5 100.000 (99.906)
Train: [150/422]	Time 0.063 (0.067)	Loss 0.2677 (0.2909)	Prec@1 93.750 (89.344)	Prec@5 100.000 (99.906)
Train: [200/422]	Time 0.061 (0.067)	Loss 0.2565 (0.2826)	Prec@1 89.062 (89.938)	Prec@5 100.000 (99.906)
Train: [250/422]	Time 0.065 (0.066)	Loss 0.3723 (0.2837)	Prec@1 87.500 (89.438)	Prec@5 100.000 (99.953)
Train: [300/422]	Time 0.067 (0.066)	Loss 0.3143 (0.2968)	Prec@1 89.062 (88.938)	Prec@5 100.000 (99.906)
Train: [350/422]	Time 0.066 (0.065)	Loss 0.2772 (0.3006)	Prec@1 90.625 (89.062)	Prec@5 100.000 (99.891)
Train: [400/422]	Time 0.064 (0.064)	Loss 0.2460 (0.2895)	Prec@1 87.500 (89.594)	Prec@5 100.000 (99.891)
Test: [0/47]	Time 0.327 (0.327)	Loss 0.2476 (0.2476)	Prec@1 90.625 (90.625)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.015 (0.030)	Loss 0.3518 (0.2931)	Prec@1 87.500 (89.955)	Prec@5 98.438 (99.628)
Test: [40/47]	Time 0.012 (0.022)	Loss 0.3845 (0.3287)	Prec@1 85.938 (88.720)	Prec@5 100.000 (99.771)
Test: [20/40] Acc 88.933
epoch: 21
Train: [0/422]	Time 0.397 (0.397)	Loss 0.4330 (0.4330)	Prec@1 82.812 (82.812)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.068 (0.069)	Loss 0.2968 (0.2714)	Prec@1 87.500 (89.951)	Prec@5 100.000 (99.908)
Train: [100/422]	Time 0.066 (0.067)	Loss 0.4635 (0.2761)	Prec@1 79.688 (89.891)	Prec@5 100.000 (99.875)
Train: [150/422]	Time 0.075 (0.072)	Loss 0.3235 (0.2833)	Prec@1 85.938 (89.641)	Prec@5 100.000 (99.891)
Train: [200/422]	Time 0.066 (0.069)	Loss 0.3049 (0.2864)	Prec@1 85.938 (89.188)	Prec@5 100.000 (99.938)
Train: [250/422]	Time 0.064 (0.067)	Loss 0.3289 (0.2907)	Prec@1 92.188 (89.062)	Prec@5 100.000 (99.906)
Train: [300/422]	Time 0.069 (0.066)	Loss 0.2870 (0.2899)	Prec@1 89.062 (89.609)	Prec@5 100.000 (99.859)
Train: [350/422]	Time 0.067 (0.066)	Loss 0.3291 (0.2772)	Prec@1 85.938 (90.109)	Prec@5 98.438 (99.844)
Train: [400/422]	Time 0.067 (0.067)	Loss 0.2028 (0.2780)	Prec@1 93.750 (89.906)	Prec@5 100.000 (99.875)
Test: [0/47]	Time 0.350 (0.350)	Loss 0.1771 (0.1771)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.030)	Loss 0.2719 (0.2758)	Prec@1 87.500 (90.179)	Prec@5 100.000 (99.702)
Test: [40/47]	Time 0.012 (0.021)	Loss 0.4100 (0.3082)	Prec@1 82.812 (88.910)	Prec@5 100.000 (99.771)
Test: [21/40] Acc 88.900
epoch: 22
Train: [0/422]	Time 0.416 (0.416)	Loss 0.1850 (0.1850)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.066 (0.071)	Loss 0.1656 (0.2723)	Prec@1 92.188 (89.154)	Prec@5 100.000 (99.939)
Train: [100/422]	Time 0.067 (0.065)	Loss 0.3346 (0.2756)	Prec@1 87.500 (89.391)	Prec@5 100.000 (99.875)
Train: [150/422]	Time 0.062 (0.064)	Loss 0.1525 (0.2810)	Prec@1 93.750 (89.719)	Prec@5 100.000 (99.859)
Train: [200/422]	Time 0.063 (0.065)	Loss 0.5987 (0.2828)	Prec@1 84.375 (89.641)	Prec@5 100.000 (99.922)
Train: [250/422]	Time 0.066 (0.065)	Loss 0.1938 (0.2718)	Prec@1 92.188 (90.047)	Prec@5 100.000 (99.969)
Train: [300/422]	Time 0.057 (0.062)	Loss 0.3618 (0.2652)	Prec@1 82.812 (90.609)	Prec@5 100.000 (99.969)
Train: [350/422]	Time 0.058 (0.059)	Loss 0.1473 (0.2765)	Prec@1 93.750 (90.016)	Prec@5 100.000 (99.891)
Train: [400/422]	Time 0.059 (0.059)	Loss 0.3001 (0.2842)	Prec@1 84.375 (89.688)	Prec@5 100.000 (99.875)
Test: [0/47]	Time 0.341 (0.341)	Loss 0.1801 (0.1801)	Prec@1 92.188 (92.188)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.012 (0.030)	Loss 0.2131 (0.2846)	Prec@1 95.312 (90.253)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.012 (0.021)	Loss 0.4280 (0.3229)	Prec@1 89.062 (88.338)	Prec@5 100.000 (99.809)
Test: [22/40] Acc 88.700
epoch: 23
Train: [0/422]	Time 0.378 (0.378)	Loss 0.2907 (0.2907)	Prec@1 93.750 (93.750)	Prec@5 98.438 (98.438)
Train: [50/422]	Time 0.068 (0.074)	Loss 0.1940 (0.2792)	Prec@1 92.188 (89.491)	Prec@5 100.000 (99.939)
Train: [100/422]	Time 0.066 (0.065)	Loss 0.2380 (0.2671)	Prec@1 87.500 (89.922)	Prec@5 100.000 (99.984)
Train: [150/422]	Time 0.063 (0.063)	Loss 0.3367 (0.2564)	Prec@1 85.938 (90.562)	Prec@5 100.000 (99.969)
Train: [200/422]	Time 0.064 (0.063)	Loss 0.2870 (0.2748)	Prec@1 85.938 (90.016)	Prec@5 100.000 (99.922)
Train: [250/422]	Time 0.065 (0.065)	Loss 0.1758 (0.2827)	Prec@1 95.312 (89.750)	Prec@5 100.000 (99.906)
Train: [300/422]	Time 0.059 (0.062)	Loss 0.2647 (0.2662)	Prec@1 87.500 (90.406)	Prec@5 100.000 (99.875)
Train: [350/422]	Time 0.060 (0.059)	Loss 0.3876 (0.2722)	Prec@1 90.625 (89.938)	Prec@5 100.000 (99.875)
Train: [400/422]	Time 0.061 (0.059)	Loss 0.2591 (0.2872)	Prec@1 92.188 (89.391)	Prec@5 100.000 (99.906)
Test: [0/47]	Time 0.356 (0.356)	Loss 0.2755 (0.2755)	Prec@1 89.062 (89.062)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.012 (0.031)	Loss 0.2513 (0.3013)	Prec@1 92.188 (89.062)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.011 (0.022)	Loss 0.4710 (0.3348)	Prec@1 84.375 (87.805)	Prec@5 100.000 (99.809)
Test: [23/40] Acc 88.167
epoch: 24
Train: [0/422]	Time 0.358 (0.358)	Loss 0.2884 (0.2884)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.045 (0.054)	Loss 0.3757 (0.2823)	Prec@1 89.062 (89.706)	Prec@5 100.000 (99.939)
Train: [100/422]	Time 0.046 (0.048)	Loss 0.1804 (0.2804)	Prec@1 95.312 (89.688)	Prec@5 100.000 (99.938)
Train: [150/422]	Time 0.054 (0.049)	Loss 0.1572 (0.2679)	Prec@1 95.312 (90.141)	Prec@5 100.000 (99.953)
Train: [200/422]	Time 0.048 (0.050)	Loss 0.1756 (0.2538)	Prec@1 95.312 (90.500)	Prec@5 100.000 (99.984)
Train: [250/422]	Time 0.054 (0.048)	Loss 0.2351 (0.2573)	Prec@1 93.750 (90.516)	Prec@5 100.000 (99.938)
Train: [300/422]	Time 0.047 (0.049)	Loss 0.2108 (0.2660)	Prec@1 90.625 (90.203)	Prec@5 100.000 (99.891)
Train: [350/422]	Time 0.047 (0.049)	Loss 0.3633 (0.2783)	Prec@1 82.812 (89.688)	Prec@5 98.438 (99.859)
Train: [400/422]	Time 0.047 (0.049)	Loss 0.3613 (0.2797)	Prec@1 87.500 (89.562)	Prec@5 98.438 (99.859)
Test: [0/47]	Time 0.316 (0.316)	Loss 0.2073 (0.2073)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.028)	Loss 0.3041 (0.3060)	Prec@1 92.188 (89.509)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.011 (0.020)	Loss 0.3844 (0.3297)	Prec@1 87.500 (88.415)	Prec@5 100.000 (99.848)
Test: [24/40] Acc 88.533
epoch: 25
Train: [0/422]	Time 0.421 (0.421)	Loss 0.2304 (0.2304)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.067 (0.072)	Loss 0.2953 (0.2402)	Prec@1 85.938 (90.809)	Prec@5 100.000 (99.969)
Train: [100/422]	Time 0.068 (0.066)	Loss 0.1218 (0.2472)	Prec@1 98.438 (90.781)	Prec@5 100.000 (99.984)
Train: [150/422]	Time 0.062 (0.065)	Loss 0.4061 (0.2583)	Prec@1 84.375 (90.438)	Prec@5 100.000 (99.969)
Train: [200/422]	Time 0.066 (0.064)	Loss 0.3747 (0.2738)	Prec@1 81.250 (89.844)	Prec@5 100.000 (99.922)
Train: [250/422]	Time 0.062 (0.064)	Loss 0.4376 (0.2694)	Prec@1 85.938 (89.828)	Prec@5 100.000 (99.922)
Train: [300/422]	Time 0.059 (0.063)	Loss 0.3487 (0.2674)	Prec@1 87.500 (89.984)	Prec@5 100.000 (99.891)
Train: [350/422]	Time 0.059 (0.060)	Loss 0.2501 (0.2762)	Prec@1 92.188 (89.906)	Prec@5 100.000 (99.875)
Train: [400/422]	Time 0.061 (0.059)	Loss 0.1938 (0.2653)	Prec@1 93.750 (90.078)	Prec@5 100.000 (99.906)
Test: [0/47]	Time 0.351 (0.351)	Loss 0.2115 (0.2115)	Prec@1 93.750 (93.750)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.015 (0.031)	Loss 0.2952 (0.2939)	Prec@1 87.500 (89.881)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.014 (0.023)	Loss 0.3607 (0.3145)	Prec@1 85.938 (88.681)	Prec@5 100.000 (99.848)
Test: [25/40] Acc 88.800
epoch: 26
Train: [0/422]	Time 0.421 (0.421)	Loss 0.2473 (0.2473)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.071 (0.073)	Loss 0.3141 (0.2658)	Prec@1 85.938 (90.349)	Prec@5 100.000 (100.000)
Train: [100/422]	Time 0.062 (0.066)	Loss 0.2094 (0.2629)	Prec@1 93.750 (90.391)	Prec@5 100.000 (99.984)
Train: [150/422]	Time 0.064 (0.067)	Loss 0.2474 (0.2626)	Prec@1 92.188 (90.359)	Prec@5 100.000 (99.984)
Train: [200/422]	Time 0.065 (0.067)	Loss 0.2048 (0.2656)	Prec@1 90.625 (89.891)	Prec@5 100.000 (99.938)
Train: [250/422]	Time 0.064 (0.065)	Loss 0.1280 (0.2594)	Prec@1 95.312 (90.203)	Prec@5 100.000 (99.906)
Train: [300/422]	Time 0.066 (0.064)	Loss 0.2067 (0.2619)	Prec@1 93.750 (90.562)	Prec@5 100.000 (99.938)
Train: [350/422]	Time 0.067 (0.065)	Loss 0.2098 (0.2610)	Prec@1 90.625 (90.438)	Prec@5 100.000 (99.969)
Train: [400/422]	Time 0.066 (0.065)	Loss 0.4358 (0.2599)	Prec@1 87.500 (90.406)	Prec@5 100.000 (99.984)
Test: [0/47]	Time 0.347 (0.347)	Loss 0.2100 (0.2100)	Prec@1 95.312 (95.312)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.022 (0.032)	Loss 0.2009 (0.2790)	Prec@1 93.750 (90.253)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.011 (0.023)	Loss 0.3645 (0.2992)	Prec@1 89.062 (89.367)	Prec@5 100.000 (99.848)
Test: [26/40] Acc 89.567
epoch: 27
Train: [0/422]	Time 0.363 (0.363)	Loss 0.3199 (0.3199)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.057 (0.063)	Loss 0.2020 (0.2512)	Prec@1 92.188 (90.778)	Prec@5 100.000 (100.000)
Train: [100/422]	Time 0.055 (0.057)	Loss 0.3016 (0.2625)	Prec@1 92.188 (90.188)	Prec@5 100.000 (99.969)
Train: [150/422]	Time 0.067 (0.057)	Loss 0.1702 (0.2720)	Prec@1 93.750 (89.922)	Prec@5 100.000 (99.969)
Train: [200/422]	Time 0.064 (0.061)	Loss 0.2965 (0.2711)	Prec@1 87.500 (90.422)	Prec@5 100.000 (99.969)
Train: [250/422]	Time 0.064 (0.065)	Loss 0.3977 (0.2536)	Prec@1 89.062 (91.000)	Prec@5 100.000 (99.953)
Train: [300/422]	Time 0.065 (0.066)	Loss 0.1917 (0.2436)	Prec@1 92.188 (91.000)	Prec@5 100.000 (99.953)
Train: [350/422]	Time 0.062 (0.067)	Loss 0.3022 (0.2575)	Prec@1 89.062 (90.312)	Prec@5 100.000 (99.938)
Train: [400/422]	Time 0.064 (0.065)	Loss 0.2596 (0.2555)	Prec@1 89.062 (90.469)	Prec@5 100.000 (99.906)
Test: [0/47]	Time 0.349 (0.349)	Loss 0.2364 (0.2364)	Prec@1 89.062 (89.062)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.029)	Loss 0.2575 (0.2931)	Prec@1 89.062 (89.062)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.011 (0.021)	Loss 0.3766 (0.3074)	Prec@1 87.500 (88.758)	Prec@5 100.000 (99.809)
Test: [27/40] Acc 88.900
epoch: 28
Train: [0/422]	Time 0.369 (0.369)	Loss 0.1574 (0.1574)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.059 (0.064)	Loss 0.1419 (0.2523)	Prec@1 96.875 (90.809)	Prec@5 100.000 (100.000)
Train: [100/422]	Time 0.060 (0.058)	Loss 0.2334 (0.2514)	Prec@1 90.625 (90.828)	Prec@5 100.000 (99.969)
Train: [150/422]	Time 0.064 (0.060)	Loss 0.2761 (0.2521)	Prec@1 87.500 (90.875)	Prec@5 100.000 (99.938)
Train: [200/422]	Time 0.062 (0.065)	Loss 0.1969 (0.2584)	Prec@1 92.188 (90.672)	Prec@5 100.000 (99.938)
Train: [250/422]	Time 0.063 (0.064)	Loss 0.2604 (0.2560)	Prec@1 87.500 (90.422)	Prec@5 100.000 (99.906)
Train: [300/422]	Time 0.059 (0.061)	Loss 0.4197 (0.2517)	Prec@1 87.500 (90.531)	Prec@5 100.000 (99.906)
Train: [350/422]	Time 0.057 (0.059)	Loss 0.1574 (0.2507)	Prec@1 93.750 (90.844)	Prec@5 100.000 (99.922)
Train: [400/422]	Time 0.067 (0.059)	Loss 0.3669 (0.2456)	Prec@1 89.062 (90.969)	Prec@5 100.000 (99.906)
Test: [0/47]	Time 0.364 (0.364)	Loss 0.1836 (0.1836)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.017 (0.032)	Loss 0.1852 (0.2773)	Prec@1 96.875 (89.807)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.015 (0.024)	Loss 0.3082 (0.2937)	Prec@1 89.062 (88.910)	Prec@5 100.000 (99.848)
Test: [28/40] Acc 89.100
epoch: 29
Train: [0/422]	Time 0.408 (0.408)	Loss 0.2333 (0.2333)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.062 (0.069)	Loss 0.2345 (0.2507)	Prec@1 90.625 (90.319)	Prec@5 100.000 (99.969)
Train: [100/422]	Time 0.063 (0.063)	Loss 0.1816 (0.2405)	Prec@1 95.312 (91.125)	Prec@5 100.000 (99.984)
Train: [150/422]	Time 0.057 (0.064)	Loss 0.2251 (0.2365)	Prec@1 89.062 (91.531)	Prec@5 100.000 (99.969)
Train: [200/422]	Time 0.063 (0.062)	Loss 0.1828 (0.2440)	Prec@1 93.750 (90.969)	Prec@5 100.000 (99.938)
Train: [250/422]	Time 0.065 (0.063)	Loss 0.2850 (0.2492)	Prec@1 85.938 (90.906)	Prec@5 100.000 (99.922)
Train: [300/422]	Time 0.054 (0.065)	Loss 0.2682 (0.2496)	Prec@1 87.500 (90.875)	Prec@5 100.000 (99.922)
Train: [350/422]	Time 0.058 (0.060)	Loss 0.1594 (0.2396)	Prec@1 96.875 (91.031)	Prec@5 100.000 (99.938)
Train: [400/422]	Time 0.060 (0.058)	Loss 0.2752 (0.2534)	Prec@1 87.500 (90.297)	Prec@5 100.000 (99.969)
Test: [0/47]	Time 0.352 (0.352)	Loss 0.2190 (0.2190)	Prec@1 90.625 (90.625)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.012 (0.030)	Loss 0.2653 (0.2787)	Prec@1 87.500 (89.583)	Prec@5 100.000 (99.851)
Test: [40/47]	Time 0.012 (0.021)	Loss 0.3511 (0.3050)	Prec@1 90.625 (88.643)	Prec@5 100.000 (99.848)
Test: [29/40] Acc 88.967
epoch: 30
Train: [0/422]	Time 0.379 (0.379)	Loss 0.1344 (0.1344)	Prec@1 95.312 (95.312)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.064 (0.068)	Loss 0.2366 (0.2335)	Prec@1 87.500 (91.452)	Prec@5 100.000 (99.908)
Train: [100/422]	Time 0.065 (0.064)	Loss 0.3091 (0.2382)	Prec@1 89.062 (91.156)	Prec@5 100.000 (99.938)
Train: [150/422]	Time 0.071 (0.066)	Loss 0.3045 (0.2459)	Prec@1 87.500 (90.891)	Prec@5 100.000 (99.906)
Train: [200/422]	Time 0.073 (0.067)	Loss 0.2885 (0.2455)	Prec@1 92.188 (91.047)	Prec@5 100.000 (99.922)
Train: [250/422]	Time 0.071 (0.069)	Loss 0.1041 (0.2361)	Prec@1 95.312 (91.297)	Prec@5 100.000 (99.969)
Train: [300/422]	Time 0.075 (0.069)	Loss 0.3574 (0.2396)	Prec@1 84.375 (91.062)	Prec@5 100.000 (99.969)
Train: [350/422]	Time 0.063 (0.067)	Loss 0.1900 (0.2549)	Prec@1 92.188 (90.484)	Prec@5 100.000 (99.984)
Train: [400/422]	Time 0.074 (0.066)	Loss 0.3639 (0.2609)	Prec@1 89.062 (90.266)	Prec@5 100.000 (99.984)
Test: [0/47]	Time 0.323 (0.323)	Loss 0.1644 (0.1644)	Prec@1 92.188 (92.188)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.012 (0.027)	Loss 0.2325 (0.2621)	Prec@1 90.625 (90.402)	Prec@5 100.000 (99.702)
Test: [40/47]	Time 0.015 (0.020)	Loss 0.3648 (0.2818)	Prec@1 89.062 (89.787)	Prec@5 100.000 (99.771)
Test: [30/40] Acc 89.933
epoch: 31
Train: [0/422]	Time 0.368 (0.368)	Loss 0.1969 (0.1969)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.064 (0.065)	Loss 0.2895 (0.2387)	Prec@1 92.188 (91.391)	Prec@5 98.438 (99.969)
Train: [100/422]	Time 0.064 (0.062)	Loss 0.2902 (0.2357)	Prec@1 89.062 (91.422)	Prec@5 100.000 (99.969)
Train: [150/422]	Time 0.063 (0.064)	Loss 0.2723 (0.2375)	Prec@1 90.625 (91.344)	Prec@5 100.000 (99.953)
Train: [200/422]	Time 0.064 (0.064)	Loss 0.1454 (0.2339)	Prec@1 96.875 (91.266)	Prec@5 100.000 (99.938)
Train: [250/422]	Time 0.065 (0.065)	Loss 0.2676 (0.2387)	Prec@1 89.062 (90.766)	Prec@5 100.000 (99.922)
Train: [300/422]	Time 0.063 (0.065)	Loss 0.2653 (0.2499)	Prec@1 89.062 (90.812)	Prec@5 100.000 (99.922)
Train: [350/422]	Time 0.067 (0.065)	Loss 0.2167 (0.2411)	Prec@1 90.625 (91.359)	Prec@5 100.000 (99.969)
Train: [400/422]	Time 0.065 (0.066)	Loss 0.2778 (0.2505)	Prec@1 84.375 (90.531)	Prec@5 100.000 (99.984)
Test: [0/47]	Time 0.342 (0.342)	Loss 0.1643 (0.1643)	Prec@1 95.312 (95.312)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.030)	Loss 0.3064 (0.2760)	Prec@1 89.062 (90.030)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.013 (0.022)	Loss 0.2852 (0.2913)	Prec@1 92.188 (89.367)	Prec@5 100.000 (99.848)
Test: [31/40] Acc 89.467
epoch: 32
Train: [0/422]	Time 0.418 (0.418)	Loss 0.1551 (0.1551)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.063 (0.068)	Loss 0.1721 (0.2336)	Prec@1 92.188 (91.360)	Prec@5 100.000 (99.908)
Train: [100/422]	Time 0.064 (0.063)	Loss 0.1585 (0.2268)	Prec@1 92.188 (91.609)	Prec@5 100.000 (99.953)
Train: [150/422]	Time 0.064 (0.065)	Loss 0.2882 (0.2345)	Prec@1 85.938 (91.297)	Prec@5 100.000 (99.938)
Train: [200/422]	Time 0.070 (0.066)	Loss 0.1897 (0.2477)	Prec@1 90.625 (90.875)	Prec@5 100.000 (99.891)
Train: [250/422]	Time 0.067 (0.064)	Loss 0.2557 (0.2496)	Prec@1 90.625 (90.766)	Prec@5 100.000 (99.953)
Train: [300/422]	Time 0.069 (0.064)	Loss 0.5082 (0.2547)	Prec@1 87.500 (90.391)	Prec@5 98.438 (99.953)
Train: [350/422]	Time 0.072 (0.067)	Loss 0.1729 (0.2403)	Prec@1 92.188 (91.016)	Prec@5 100.000 (99.953)
Train: [400/422]	Time 0.096 (0.069)	Loss 0.1292 (0.2375)	Prec@1 96.875 (91.219)	Prec@5 100.000 (99.953)
Test: [0/47]	Time 0.338 (0.338)	Loss 0.1935 (0.1935)	Prec@1 89.062 (89.062)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.012 (0.029)	Loss 0.3246 (0.2836)	Prec@1 85.938 (90.179)	Prec@5 98.438 (99.628)
Test: [40/47]	Time 0.011 (0.021)	Loss 0.3145 (0.3039)	Prec@1 87.500 (89.787)	Prec@5 100.000 (99.771)
Test: [32/40] Acc 90.000
epoch: 33
Train: [0/422]	Time 0.372 (0.372)	Loss 0.1393 (0.1393)	Prec@1 96.875 (96.875)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.065 (0.073)	Loss 0.1951 (0.2130)	Prec@1 92.188 (92.463)	Prec@5 100.000 (99.969)
Train: [100/422]	Time 0.064 (0.066)	Loss 0.2159 (0.2266)	Prec@1 95.312 (91.891)	Prec@5 100.000 (99.953)
Train: [150/422]	Time 0.067 (0.065)	Loss 0.2960 (0.2385)	Prec@1 85.938 (91.078)	Prec@5 100.000 (99.938)
Train: [200/422]	Time 0.064 (0.064)	Loss 0.2116 (0.2334)	Prec@1 93.750 (91.000)	Prec@5 100.000 (99.953)
Train: [250/422]	Time 0.063 (0.064)	Loss 0.2115 (0.2326)	Prec@1 90.625 (91.297)	Prec@5 100.000 (99.938)
Train: [300/422]	Time 0.058 (0.063)	Loss 0.2749 (0.2443)	Prec@1 89.062 (90.812)	Prec@5 100.000 (99.953)
Train: [350/422]	Time 0.058 (0.060)	Loss 0.1184 (0.2500)	Prec@1 95.312 (90.734)	Prec@5 100.000 (99.953)
Train: [400/422]	Time 0.060 (0.059)	Loss 0.1829 (0.2351)	Prec@1 93.750 (91.516)	Prec@5 100.000 (99.922)
Test: [0/47]	Time 0.331 (0.331)	Loss 0.2406 (0.2406)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.014 (0.031)	Loss 0.3049 (0.2990)	Prec@1 92.188 (89.583)	Prec@5 100.000 (99.702)
Test: [40/47]	Time 0.015 (0.024)	Loss 0.3024 (0.3034)	Prec@1 90.625 (88.986)	Prec@5 100.000 (99.809)
Test: [33/40] Acc 89.300
epoch: 34
Train: [0/422]	Time 0.406 (0.406)	Loss 0.1529 (0.1529)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.064 (0.072)	Loss 0.1821 (0.2449)	Prec@1 93.750 (90.839)	Prec@5 98.438 (99.939)
Train: [100/422]	Time 0.064 (0.065)	Loss 0.2380 (0.2452)	Prec@1 89.062 (90.781)	Prec@5 100.000 (99.969)
Train: [150/422]	Time 0.064 (0.064)	Loss 0.2469 (0.2410)	Prec@1 89.062 (90.766)	Prec@5 100.000 (99.953)
Train: [200/422]	Time 0.064 (0.064)	Loss 0.1576 (0.2356)	Prec@1 92.188 (90.984)	Prec@5 100.000 (99.891)
Train: [250/422]	Time 0.068 (0.065)	Loss 0.1142 (0.2322)	Prec@1 95.312 (91.516)	Prec@5 100.000 (99.906)
Train: [300/422]	Time 0.062 (0.065)	Loss 0.2264 (0.2254)	Prec@1 93.750 (91.812)	Prec@5 100.000 (99.922)
Train: [350/422]	Time 0.068 (0.065)	Loss 0.2440 (0.2155)	Prec@1 92.188 (92.234)	Prec@5 100.000 (99.938)
Train: [400/422]	Time 0.063 (0.065)	Loss 0.2141 (0.2266)	Prec@1 85.938 (91.875)	Prec@5 100.000 (99.922)
Test: [0/47]	Time 0.345 (0.345)	Loss 0.2628 (0.2628)	Prec@1 89.062 (89.062)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.012 (0.030)	Loss 0.3405 (0.2942)	Prec@1 82.812 (88.839)	Prec@5 100.000 (99.628)
Test: [40/47]	Time 0.011 (0.021)	Loss 0.3352 (0.3115)	Prec@1 89.062 (88.643)	Prec@5 100.000 (99.733)
Test: [34/40] Acc 88.967
epoch: 35
Train: [0/422]	Time 0.366 (0.366)	Loss 0.1556 (0.1556)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.069 (0.072)	Loss 0.1959 (0.2369)	Prec@1 93.750 (91.391)	Prec@5 100.000 (99.969)
Train: [100/422]	Time 0.074 (0.067)	Loss 0.3191 (0.2341)	Prec@1 90.625 (91.469)	Prec@5 100.000 (99.953)
Train: [150/422]	Time 0.064 (0.068)	Loss 0.3377 (0.2216)	Prec@1 85.938 (91.562)	Prec@5 100.000 (99.906)
Train: [200/422]	Time 0.073 (0.068)	Loss 0.1870 (0.2282)	Prec@1 92.188 (91.328)	Prec@5 100.000 (99.891)
Train: [250/422]	Time 0.067 (0.067)	Loss 0.2634 (0.2410)	Prec@1 90.625 (91.094)	Prec@5 100.000 (99.953)
Train: [300/422]	Time 0.066 (0.067)	Loss 0.1568 (0.2393)	Prec@1 93.750 (91.016)	Prec@5 100.000 (100.000)
Train: [350/422]	Time 0.064 (0.066)	Loss 0.2456 (0.2290)	Prec@1 90.625 (91.453)	Prec@5 100.000 (99.938)
Train: [400/422]	Time 0.064 (0.065)	Loss 0.2512 (0.2351)	Prec@1 90.625 (91.781)	Prec@5 100.000 (99.922)
Test: [0/47]	Time 0.334 (0.334)	Loss 0.2194 (0.2194)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.013 (0.029)	Loss 0.2171 (0.2806)	Prec@1 92.188 (90.179)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.011 (0.021)	Loss 0.2764 (0.2960)	Prec@1 90.625 (89.405)	Prec@5 100.000 (99.848)
Test: [35/40] Acc 89.867
epoch: 36
Train: [0/422]	Time 0.372 (0.372)	Loss 0.2212 (0.2212)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.065 (0.071)	Loss 0.3004 (0.2210)	Prec@1 85.938 (91.820)	Prec@5 100.000 (100.000)
Train: [100/422]	Time 0.066 (0.065)	Loss 0.2479 (0.2202)	Prec@1 93.750 (91.656)	Prec@5 100.000 (99.984)
Train: [150/422]	Time 0.064 (0.065)	Loss 0.1872 (0.2294)	Prec@1 92.188 (91.188)	Prec@5 100.000 (99.953)
Train: [200/422]	Time 0.062 (0.065)	Loss 0.1323 (0.2330)	Prec@1 95.312 (91.219)	Prec@5 100.000 (99.953)
Train: [250/422]	Time 0.062 (0.064)	Loss 0.3047 (0.2366)	Prec@1 87.500 (91.172)	Prec@5 100.000 (99.953)
Train: [300/422]	Time 0.063 (0.064)	Loss 0.2762 (0.2325)	Prec@1 87.500 (91.219)	Prec@5 100.000 (99.969)
Train: [350/422]	Time 0.061 (0.064)	Loss 0.0929 (0.2169)	Prec@1 95.312 (91.844)	Prec@5 100.000 (99.984)
Train: [400/422]	Time 0.061 (0.062)	Loss 0.1041 (0.2250)	Prec@1 96.875 (91.734)	Prec@5 100.000 (99.953)
Test: [0/47]	Time 0.337 (0.337)	Loss 0.2331 (0.2331)	Prec@1 92.188 (92.188)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.013 (0.029)	Loss 0.2167 (0.2742)	Prec@1 89.062 (90.253)	Prec@5 100.000 (99.702)
Test: [40/47]	Time 0.011 (0.021)	Loss 0.4616 (0.2933)	Prec@1 87.500 (89.672)	Prec@5 100.000 (99.771)
Test: [36/40] Acc 89.933
epoch: 37
Train: [0/422]	Time 0.362 (0.362)	Loss 0.1740 (0.1740)	Prec@1 90.625 (90.625)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.065 (0.067)	Loss 0.1281 (0.1928)	Prec@1 96.875 (92.800)	Prec@5 100.000 (99.969)
Train: [100/422]	Time 0.076 (0.064)	Loss 0.2048 (0.2135)	Prec@1 93.750 (92.000)	Prec@5 100.000 (99.969)
Train: [150/422]	Time 0.065 (0.069)	Loss 0.2610 (0.2302)	Prec@1 92.188 (91.531)	Prec@5 100.000 (99.984)
Train: [200/422]	Time 0.063 (0.068)	Loss 0.1450 (0.2239)	Prec@1 92.188 (91.953)	Prec@5 100.000 (99.953)
Train: [250/422]	Time 0.064 (0.066)	Loss 0.3023 (0.2200)	Prec@1 89.062 (91.734)	Prec@5 100.000 (99.938)
Train: [300/422]	Time 0.059 (0.064)	Loss 0.2798 (0.2280)	Prec@1 87.500 (91.188)	Prec@5 100.000 (99.969)
Train: [350/422]	Time 0.055 (0.061)	Loss 0.2530 (0.2258)	Prec@1 85.938 (91.609)	Prec@5 100.000 (99.953)
Train: [400/422]	Time 0.065 (0.061)	Loss 0.2795 (0.2274)	Prec@1 89.062 (91.719)	Prec@5 100.000 (99.969)
Test: [0/47]	Time 0.345 (0.345)	Loss 0.2102 (0.2102)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.012 (0.030)	Loss 0.2939 (0.2918)	Prec@1 87.500 (90.253)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.012 (0.021)	Loss 0.3106 (0.3008)	Prec@1 90.625 (89.863)	Prec@5 100.000 (99.848)
Test: [37/40] Acc 90.167
epoch: 38
Train: [0/422]	Time 0.388 (0.388)	Loss 0.2029 (0.2029)	Prec@1 92.188 (92.188)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.066 (0.070)	Loss 0.1098 (0.2096)	Prec@1 98.438 (91.973)	Prec@5 100.000 (99.939)
Train: [100/422]	Time 0.064 (0.065)	Loss 0.2551 (0.2099)	Prec@1 90.625 (91.781)	Prec@5 100.000 (99.969)
Train: [150/422]	Time 0.064 (0.065)	Loss 0.2368 (0.2227)	Prec@1 90.625 (91.359)	Prec@5 100.000 (99.969)
Train: [200/422]	Time 0.064 (0.065)	Loss 0.1977 (0.2263)	Prec@1 93.750 (91.844)	Prec@5 100.000 (99.969)
Train: [250/422]	Time 0.078 (0.065)	Loss 0.4294 (0.2240)	Prec@1 85.938 (91.609)	Prec@5 100.000 (100.000)
Train: [300/422]	Time 0.069 (0.066)	Loss 0.1767 (0.2295)	Prec@1 92.188 (91.094)	Prec@5 100.000 (99.984)
Train: [350/422]	Time 0.067 (0.067)	Loss 0.2149 (0.2212)	Prec@1 93.750 (91.719)	Prec@5 100.000 (99.953)
Train: [400/422]	Time 0.062 (0.066)	Loss 0.2353 (0.2273)	Prec@1 93.750 (91.656)	Prec@5 100.000 (99.953)
Test: [0/47]	Time 0.350 (0.350)	Loss 0.1721 (0.1721)	Prec@1 95.312 (95.312)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.015 (0.031)	Loss 0.1796 (0.2813)	Prec@1 90.625 (90.476)	Prec@5 100.000 (99.628)
Test: [40/47]	Time 0.012 (0.022)	Loss 0.3089 (0.2934)	Prec@1 85.938 (89.444)	Prec@5 100.000 (99.771)
Test: [38/40] Acc 89.667
epoch: 39
Train: [0/422]	Time 0.366 (0.366)	Loss 0.1228 (0.1228)	Prec@1 96.875 (96.875)	Prec@5 100.000 (100.000)
Train: [50/422]	Time 0.064 (0.067)	Loss 0.2307 (0.2146)	Prec@1 90.625 (92.126)	Prec@5 100.000 (100.000)
Train: [100/422]	Time 0.078 (0.065)	Loss 0.1765 (0.2191)	Prec@1 93.750 (92.078)	Prec@5 100.000 (99.984)
Train: [150/422]	Time 0.063 (0.068)	Loss 0.1512 (0.2179)	Prec@1 92.188 (91.844)	Prec@5 100.000 (99.953)
Train: [200/422]	Time 0.065 (0.066)	Loss 0.1619 (0.2140)	Prec@1 95.312 (91.875)	Prec@5 100.000 (99.953)
Train: [250/422]	Time 0.073 (0.067)	Loss 0.2441 (0.2047)	Prec@1 93.750 (92.516)	Prec@5 100.000 (99.984)
Train: [300/422]	Time 0.071 (0.068)	Loss 0.1728 (0.2055)	Prec@1 93.750 (92.438)	Prec@5 100.000 (100.000)
Train: [350/422]	Time 0.072 (0.068)	Loss 0.2851 (0.2198)	Prec@1 92.188 (91.781)	Prec@5 100.000 (100.000)
Train: [400/422]	Time 0.076 (0.069)	Loss 0.1708 (0.2277)	Prec@1 92.188 (91.422)	Prec@5 100.000 (100.000)
Test: [0/47]	Time 0.332 (0.332)	Loss 0.1876 (0.1876)	Prec@1 93.750 (93.750)	Prec@5 98.438 (98.438)
Test: [20/47]	Time 0.012 (0.029)	Loss 0.2642 (0.2782)	Prec@1 87.500 (90.774)	Prec@5 100.000 (99.777)
Test: [40/47]	Time 0.011 (0.021)	Loss 0.2874 (0.2943)	Prec@1 92.188 (89.863)	Prec@5 100.000 (99.886)
Test: [39/40] Acc 90.133
best ACC: 90.16666666666667
